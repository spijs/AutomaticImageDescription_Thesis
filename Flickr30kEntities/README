/****************************************************************************

Flickr30k Entities Dataset

If you use our dataset please cite both the coreference and bounding box annotations [1] and the original Flickr30k dataset [2].

[1] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik, Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models, arXiv:1505.04870, 2015.

[2] Peter Young, Alice Lai, Micah Hodosh and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions, Transactions of the Association for Computational Linguistics, 2(Feb):67-78, 2014.

Note that the Flickr 30k Dataset includes images obtained from Flickr (https://www.flickr.com/). Use of the images must abide by the Flickr Terms of Use (http://www.flickr.com/help/terms/). We do not own the copyright of the images. They are solely provided for researchers and educators who wish to use the dataset for non-commercial research and/or educational purposes.


Acknowledgements:

This material is based upon work supported by the National Science Foundation under Grants No. 1053856, 1205627, 1405883, IIS-1228082, and CIF-1302438 as well as support from  Xerox UAC and the Sloan Foundation. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or any sponsor.

We thank the NVIDIA Corporation for the generous donation of the GPUs used for our experiments.

*****************************************************************************/

Version 1.0 - This dataset contains 244k coreference chains and 276k manually annotated bounding boxes for each of the 31,783 images and 158,915 English captions (five per image) in the original dataset.


Coreference Chains:

Each image in the dataset has a txt file in the "Sentences" folder.  Each line of this file contains a caption with annotated phrases blocked off with brackets.  Each annotation has the following form:

[/EN#<chain id>/<type 1>/<type 2>.../<type n> <word 1> <word 2> ... <word n>]

Phrases that belong to the same coreference chain share the same chain id.  Each phrase has one or more types associated with it, which correspond to the rough categories described in [1].  Phrases of the type "notvisual" have the null chain id of "0" and should be considered a set of singleton coreference chains since these phrases were not annotated.


Bounding Boxes or Scene/No Box:

Each image in the dataset has an xml file in the "Annotations" folder which follows a similar format to the PASCAL VOC datasets.  These files have object tags which either contain a bounding box defined by xmin,ymin,xmax,ymax tags or scene/no box binary flags contained in scene and nobndbox tags.

Each object tag also contains one or more name tags which contain the chain ids the object refers to.


Matlab Interface:

We have included Matlab code to parse our data files.  

To extract Coreference information use the following function call:

corefData = getSentenceData('<path_to_annotation_directory>/Sentences/<image id>.txt');

To extract the xml data use the following:

annotationData = getAnnotations('<path_to_annotation_directory>/Annotations/<image id>.xml');

Please see each function for details about the structures returned from each function.


Unrelated Captions:

We have a list of the captions in the dataset that do not relate to the images themselves in the UNRELATED_CAPTIONS file.  This list is likely incomplete.


Please direct any questions to bplumme2@illinois.edu