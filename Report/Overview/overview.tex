\documentclass{article}


% Title Page
\title{Thesis Progress Overview}
\author{Wout Vekemans \& Thijs Dieltjens}


\begin{document}
\maketitle
This report documents our progress so far. It has an overview of the papers we found most interesting. We also describe what we have been doing the last couple of weeks. 

We started from the implementation from \cite{Karpathy} and were able to run it on the Paris server. We tried to make a couple of extensions to the code, investigating possible improvements. For now we still use a RNN as language model, but in the future we might consider using a maximum entropy model as found in \cite{Fang2015}. We also looked at three recently published papers. The first looks at a new way of modelling a network (FSMN) \cite{Zhang2015}. The second one proposes a new dataset containing alignments between snippets of texts and corresponding Flickr30k images \cite{Plummer2015}. The last one is similar to \cite{Xu2015} but it adds scene factorization. \cite{Jin2015}

\paragraph{What we did so far}
\begin{enumerate}
	\item Understanding and running the code from Karpathy, both on our own machines and Paris. 
	\item Extracting the entities proposed in \cite{Plummer2015} to a useable format. Right now we don't see how this can easily be integrated with the system to improve it.
	\item Rough implementation of FSMN. This was mainly done to see if there was a time improvement compared to LSTM and RNN. The implementation is not perfect, but it gives us an estimate of execution time. It did not seem to improve much over RNN.
	\item Following \cite{Jin2015} we looked at the scene factorization. More concrete, we implemented LDA and ran it to extract topic distributions for each image in the Flickr30k training set. We also created a simple feed-forward neural network to extract a topic distribution from unseen images. This seems to work quite well. 
\end{enumerate}

\paragraph{What we will do in the near future}
\begin{enumerate}
	\item Use the topic distribution as extra input for the RNN and see how this affects the results.
	\item Further understanding and possible implementation of an attention based model.
\end{enumerate}



\bibliographystyle{apalike}
\bibliography{referenties}

\end{document}          
