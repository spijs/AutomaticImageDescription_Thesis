@article{Arge2015,
archivePrefix = {arXiv},
arxivId = {1409.1556v6},
author = {Arge, F O R L and Mage, Cale I},
eprint = {1409.1556v6},
file = {:G$\backslash$:/Dropbox/PDF/1409.1556v6.pdf:pdf},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2015}
}
@article{Bernardi,
author = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernardi et al. - Unknown - Automatic Image Description Systems A Survey.pdf:pdf},
number = {Cv},
title = {{Automatic Image Description Systems : A Survey}}
}
@article{Chen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.5654v1},
author = {Chen, Xinlei and Zitnick, C Lawrence},
eprint = {arXiv:1411.5654v1},
file = {:G$\backslash$:/thesis/Papers/1411.5654v1.pdf:pdf},
journal = {Proceedings of CoRR},
title = {{Learning a Recurrent Visual Representation for Image Caption Generation}},
year = {2014}
}
@article{Cho2015,
abstract = {Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.},
archivePrefix = {arXiv},
arxivId = {1507.01053},
author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
eprint = {1507.01053},
file = {:G$\backslash$:/Dropbox/PDF/1507.01053.pdf:pdf},
pages = {1--12},
title = {{Describing Multimedia Content using Attention-based Encoder--Decoder Networks}},
url = {http://arxiv.org/abs/1507.01053},
year = {2015}
}
@article{Devlin2015,
abstract = {Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.},
archivePrefix = {arXiv},
arxivId = {1505.01809},
author = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
eprint = {1505.01809},
file = {:G$\backslash$:/Dropbox/PDF/1505.01809.pdf:pdf},
number = {Me Lm},
title = {{Language Models for Image Captioning: The Quirks and What Works}},
url = {http://arxiv.org/abs/1505.01809},
year = {2015}
}
@article{Devlin2015a,
abstract = {We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the "consensus" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach.},
archivePrefix = {arXiv},
arxivId = {1505.04467},
author = {Devlin, Jacob and Gupta, Saurabh and Girshick, Ross and Mitchell, Margaret and Zitnick, C. Lawrence},
eprint = {1505.04467},
file = {:G$\backslash$:/Dropbox/PDF/1505.04467.pdf:pdf},
journal = {arXiv preprint},
pages = {1--6},
title = {{Exploring Nearest Neighbor Approaches for Image Captioning}},
url = {http://arxiv.org/abs/1505.04467},
year = {2015}
}
@article{Donahue2015,
abstract = {Visual recognition을 할 때 sequence를 인식 작업에 포함시키면 더 좋은 성능을 낼 것이다. 따라서 Long term Recurrent Convolution(LTRC)를 통하여 다양한 visual sequence 정보를 포함시켜 결과를 추론함.},
archivePrefix = {arXiv},
arxivId = {1411.4389},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor and Austin, U T and Lowell, Umass and Berkeley, U C},
eprint = {1411.4389},
file = {:G$\backslash$:/thesis/Papers/1411.4389v3.pdf:pdf},
journal = {Cvpr},
title = {{Long-term Recurrent Convolutional Networks for Visual Recognition and Description}},
year = {2015}
}
@article{Elliott2014,
author = {Elliott, Desmond and Keller, Frank},
file = {:G$\backslash$:/Dropbox/PDF/P14-2074.pdf:pdf},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
pages = {452--457},
title = {{Comparing Automatic Evaluation Measures for Image Description}},
url = {http://www.aclweb.org/anthology/P/P14/P14-2074},
year = {2014}
}
@article{Elliott2015,
abstract = {The Visual Dependency Representation (VDR) is an explicit model of the spa-tial relationships between objects in an im-age. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given descrip-tion using a state-of-the-art object detec-tor, and to use successful detections to pro-duce training data. The description of an unseen image is produced by first predict-ing its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-of-the-art multimodal deep neural network in images depicting actions.},
author = {Elliott, Desmond and Vries, Arjen P De},
file = {:G$\backslash$:/Dropbox/PDF/elliot2015.pdf:pdf},
journal = {Acl-2015},
pages = {42--52},
title = {{Describing Images using Inferred Visual Dependency Representations}},
year = {2015}
}
@article{Fei-Fei2007,
abstract = {What do we see when we glance at a natural scene and how does it change as the glance becomes longer? We asked naive subjects to report in a free-form format what they saw when looking at briefly presented real-life photographs. Our subjects received no specific information as to the content of each stimulus. Thus, our paradigm differs from previous studies where subjects were cued before a picture was presented and/or were probed with multiple-choice questions. In the first stage, 90 novel grayscale photographs were foveally shown to a group of 22 native-English-speaking subjects. The presentation time was chosen at random from a set of seven possible times (from 27 to 500 ms). A perceptual mask followed each photograph immediately. After each presentation, subjects reported what they had just seen as completely and truthfully as possible. In the second stage, another group of naive individuals was instructed to score each of the descriptions produced by the subjects in the first stage. Individual scores were assigned to more than a hundred different attributes. We show that within a single glance, much object- and scene-level information is perceived by human subjects. The richness of our perception, though, seems asymmetrical. Subjects tend to have a propensity toward perceiving natural scenes as being outdoor rather than indoor. The reporting of sensory- or feature-level information of a scene (such as shading and shape) consistently precedes the reporting of the semantic-level information. But once subjects recognize more semantic-level components of a scene, there is little evidence suggesting any bias toward either scene-level or object-level recognition.},
author = {Fei-Fei, Li and Iyer, Asha and Koch, Christof and Perona, Pietro},
doi = {10.1167/7.1.10},
file = {:G$\backslash$:/Dropbox/PDF/Fei-Fei et al. - 2007 - What do we perceive in a glance of a real-world scene.pdf:pdf},
isbn = {1534-7362},
issn = {1534-7362},
journal = {Journal of Vision},
keywords = {entry level,event recognition,indoor,natural scene,object categorization,object recognition,outdoor,perception,real-world scene,scene categorization,segmentation,sensory-level perception,subordinate,superordinate},
number = {1},
pages = {1--29},
pmid = {17461678},
title = {{What do we perceive in a glance of a real-world scene?}},
url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/7.1.10},
volume = {7},
year = {2007}
}
@article{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their pre- dictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thou- sands of novel labels never seen by the visual model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5650v3},
author = {Frome, Andrea and Corrado, Gs and Shlens, Jonathon},
eprint = {arXiv:1312.5650v3},
file = {:G$\backslash$:/Dropbox/PDF/Frome, Corrado, Shlens - 2013 - Devise A deep visual-semantic embedding model(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural {\ldots}},
pages = {1--11},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@article{Gilberto2015,
author = {Gilberto, Luis and Ortiz, Mateos and Wolff, Clemens and Lapata, Mirella},
file = {:G$\backslash$:/Dropbox/PDF/Gilberto et al. - 2015 - Learning to Interpret and Describe Abstract Scenes.pdf:pdf},
journal = {Naacl2015},
pages = {1505--1515},
title = {{Learning to Interpret and Describe Abstract Scenes}},
year = {2015}
}
@article{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final mAP of 48{\%} on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:G$\backslash$:/Dropbox/PDF/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr'14},
pages = {2--9},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@article{Goldberg2015,
archivePrefix = {arXiv},
arxivId = {1510.00726},
author = {Goldberg, Yoav},
eprint = {1510.00726},
file = {:G$\backslash$:/Dropbox/PDF/overzichtNNnlp.pdf:pdf},
pages = {1--76},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
year = {2015}
}
@article{Gong2014,
abstract = {This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of im- ages that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description.},
author = {Gong, Yunchao and Wang, Liwei and Hodosh, Micah and Hockenmaier, Julia},
file = {:G$\backslash$:/Dropbox/PDF/stackedAuxEmbedding.pdf:pdf},
journal = {Computer Vision–ECCV},
pages = {529--545},
title = {{Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections}},
year = {2014}
}
@article{Google,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep re-current architecture that combines recent advances in com-puter vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target de-scription sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descrip-tions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
author = {Google, Oriol Vinyals and Google, Alexander Toshev and Google, Samy Bengio and Google, Dumitru Erhan},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Google et al. - Unknown - Show and Tell A Neural Image Caption Generator(3).pdf:pdf},
title = {{Show and Tell: A Neural Image Caption Generator}}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v5},
file = {:G$\backslash$:/Dropbox/PDF/Graves - 2013 - Generating sequences with recurrent neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Guerrero2015,
abstract = {As humans, we regularly interpret images based on the relations between image regions. For example, a person riding object X, or a plank bridging two objects. Current methods provide limited support to search for images based on such relations. We present RAID, a relation-augmented image descriptor that supports queries based on inter-region relations. The key idea of our descriptor is to capture the spatial distribution of simple point-to-region relationships to describe more complex relationships between two image regions. We evaluate the proposed descriptor by querying into a large subset of the Microsoft COCO database and successfully extract nontrivial images demonstrating complex inter-region relations, which are easily missed or erroneously classified by existing methods.},
archivePrefix = {arXiv},
arxivId = {1510.01113},
author = {Guerrero, Paul and Mitra, Niloy J. and Wonka, Peter},
eprint = {1510.01113},
file = {:G$\backslash$:/Dropbox/PDF/1510.01113.pdf:pdf},
keywords = {a relation-augmented image de-,based on inter-segment relations,image descriptors,image retrieval,in this paper,query,relation-based,scriptor that supports queries,spatial relationships,we,we present raid as},
title = {{RAID: A Relation-Augmented Image Descriptor}},
url = {http://arxiv.org/abs/1510.01113},
year = {2015}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insucient, decaying error back ow. We brie y review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, ecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with arti cial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, arti cial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hochreiter, Schmidhuber - 1997 - Long Short-Term Memory(2).pdf:pdf},
number = {8},
pages = {1--32},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Hodosh2013,
abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated. © 2013 AI Access Foundation. All rights reserved.},
author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
doi = {10.1613/jair.3994},
file = {:G$\backslash$:/Dropbox/PDF/Hodosh, Young, Hockenmaier - 2013 - Framing image description as a ranking task Data, models and evaluation metrics(2).pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {853--899},
title = {{Framing image description as a ranking task: Data, models and evaluation metrics}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84883394520{\&}partnerID=tZOtx3y1},
volume = {47},
year = {2013}
}
@article{Ioffe2015,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:G$\backslash$:/Dropbox/PDF/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
journal = {Arxiv},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
eprint = {1408.5093},
file = {:G$\backslash$:/Dropbox/PDF/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://arxiv.org/abs/1408.5093},
year = {2014}
}
@article{Jin2015,
abstract = {Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of "abstract meaning", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1506.06272},
author = {Jin, Junqi and Fu, Kun and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
eprint = {1506.06272},
file = {:G$\backslash$:/Dropbox/PDF/1506.06272.pdf:pdf},
pages = {1--20},
title = {{Aligning where to see and what to tell: image caption with region-based attention and scene factorization}},
url = {http://arxiv.org/abs/1506.06272},
year = {2015}
}
@article{Karpathy,
abstract = {We present a model that generates natural language de-scriptions of images and their regions. Our approach lever-ages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between lan-guage and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architec-ture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in re-trieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions sig-nificantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
author = {Karpathy, Andrej and Fei-Fei, Li},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Image Des(2).pdf:pdf},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}}
}
@article{Karpathya,
abstract = {We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike pre-vious models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (ob-jects) and fragments of sentences (typed dependency tree relations) into a com-mon space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive exper-imental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.},
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Joulin, Fei-Fei - Unknown - Deep Fragment Embeddings for Bidirectional Image Sentence Mapping(3).pdf:pdf},
title = {{Deep Fragment Embeddings for Bidirectional Image Sentence Mapping}}
}
@article{Kiros2013,
abstract = {Abstract We introduce two multimodal neural language models : models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase ... $\backslash$n},
author = {Kiros, R and Zemel, R and Salakhutdinov, R},
file = {:G$\backslash$:/Dropbox/PDF/Kiros, Zemel, Salakhutdinov - 2013 - Multimodal Neural Language Models(3).pdf:pdf},
journal = {Proc NIPS Deep Learning {\ldots}},
keywords = {Image Tag Inference},
pages = {1--14},
title = {{Multimodal Neural Language Models}},
url = {http://www.cs.toronto.edu/{~}rkiros/papers/mnlm2014.pdf$\backslash$npapers3://publication/uuid/00AE85FB-98DC-4EB9-A6E8-A423C47C0B98},
year = {2013}
}
@article{Kiros2014,
abstract = {Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2539v1},
author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S},
eprint = {arXiv:1411.2539v1},
file = {:G$\backslash$:/thesis/Papers/1411.2539v1.pdf:pdf},
journal = {arXiv preprint arXiv:1411.2539},
pages = {1--13},
title = {{Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models}},
year = {2014}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
file = {:G$\backslash$:/Dropbox/PDF/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Le2014,
author = {Le, Quoc V},
file = {:G$\backslash$:/Dropbox/PDF/tutorial1.pdf:pdf},
pages = {1--18},
title = {{A Tutorial on Deep Learning Part 1: Nonlinear Classifiers and The Backpropagation Algorithm}},
year = {2014}
}
@article{Le2015,
author = {Le, Quoc V},
file = {:G$\backslash$:/Dropbox/PDF/tutorial2.pdf:pdf},
pages = {1--19},
title = {{A Tutorial on Deep Learning Part 2 : Autoencoders , Convolutional Neural Networks and Recurrent Neural Networks}},
year = {2015}
}
@article{Le,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the order-ing of the words and they also ignore semantics of the words. For example, " powerful, " " strong " and " Paris " are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algo-rithm that learns fixed-length feature representa-tions from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algo-rithm represents each document by a dense vec-tor which is trained to predict words in the doc-ument. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Para-graph Vectors outperform bag-of-words models as well as other techniques for text representa-tions. Finally, we achieve new state-of-the-art re-sults on several text classification and sentiment analysis tasks.},
author = {Le, Quoc and Mikolov, Tomas},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - Unknown - Distributed Representations of Sentences and Documents(3).pdf:pdf},
title = {{Distributed Representations of Sentences and Documents}}
}
@article{Lebret2015,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03671},
author = {Lebret, R{\'{e}}mi and Pinheiro, Pedro O. and Collobert, Ronan},
eprint = {1502.03671},
file = {:G$\backslash$:/Dropbox/PDF/1502.03671v2.pdf:pdf},
title = {{Phrase-based Image Captioning}},
url = {http://arxiv.org/abs/1502.03671},
year = {2015}
}
@article{Liang2015,
abstract = {각각의 convolution layer가 recurrent connection 되어 있는 recurrent CNN을 통해 object recognition 수행},
author = {Liang, Ming and Hu, Xiaolin},
file = {:G$\backslash$:/Dropbox/PDF/Liang15-cvpr.pdf:pdf},
journal = {Cvpr},
number = {Figure 1},
title = {{Recurrent Convolutional Neural Network for Object Recognition}},
year = {2015}
}
@article{Liu2015,
abstract = {Recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large image dataset can be used as a universal image descriptor, and that doing so leads to impressive performance for a variety of image classification tasks. Most of these studies adopt activations from a single DCNN layer, usually the fully-connected layer, as the image representation. In this paper, we proposed a novel way to extract image representations from two consecutive convolutional layers: one layer is utilized for local feature extraction and the other serves as guidance to pool the extracted features. By taking different viewpoints of convolutional layers, we further develop two schemes to realize this idea. The first one directly uses convolutional layers from a DCNN. The second one applies the pretrained CNN on densely sampled image regions and treats the fully-connected activations of each image region as convolutional feature activations. We then train another convolutional layer on top of that as the pooling-guidance convolutional layer. By applying our method to three popular visual classification tasks, we find our first scheme tends to perform better on the applications which need strong discrimination on subtle object patterns within small regions while the latter excels in the cases that require discrimination on category-level patterns. Overall, the proposed method achieves superior performance over existing ways of extracting image representations from a DCNN.},
archivePrefix = {arXiv},
arxivId = {1510.00921},
author = {Liu, Lingqiao and Shen, Chunhua and Hengel, Anton Van Den},
eprint = {1510.00921},
file = {:G$\backslash$:/Dropbox/PDF/1510.00921.pdf:pdf},
number = {2},
pages = {1--7},
title = {{Cross-convolutional-layer Pooling for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1510.00921},
year = {2015}
}
@article{Mao2015,
abstract = {In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/{\~{}}junhua.mao/projects/child{\_}learning.html},
archivePrefix = {arXiv},
arxivId = {1504.06692},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
eprint = {1504.06692},
file = {:G$\backslash$:/Dropbox/PDF/1504.06692.pdf:pdf},
title = {{Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images}},
url = {http://arxiv.org/abs/1504.06692},
year = {2015}
}
@article{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/{\~{}}junhua.mao/m-RNN.html .},
archivePrefix = {arXiv},
arxivId = {1412.6632},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
eprint = {1412.6632},
file = {:G$\backslash$:/Dropbox/PDF/1412.6632v5.pdf:pdf},
number = {2014},
pages = {1--17},
title = {{Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)}},
url = {http://arxiv.org/abs/1412.6632},
volume = {1090},
year = {2014}
}
@article{Mao2014a,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.1090v1},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
eprint = {arXiv:1410.1090v1},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks(2).pdf:pdf},
journal = {arXiv:1410.1090 [cs]},
pages = {1--9},
title = {{Explain Images with Multimodal Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1410.1090 http://www.arxiv.org/pdf/1410.1090.pdf},
year = {2014}
}
@article{Mikolov2010,
abstract = {基于RNN模型的语言模型，详细可参考作者的博士论文。周期神经网络。但是上下文，也没用取全部的，只取到了前5个。},
author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
file = {:G$\backslash$:/Dropbox/PDF/Mikolov et al. - 2010 - Recurrent Neural Network based Language Model(2).pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1045--1048},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space(3).pdf:pdf},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@article{Mikolov,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num-ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna-tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of " Canada " and " Air " cannot be easily combined to obtain " Air Canada " . Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {:C$\backslash$:/Users/Wout/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality(3).pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}}
}
@article{Fang2015,
abstract = {Image descriptor(caption)을 automatically generating 하는 방법 소개 visual detector, language models, multimodal similarity model을 이미지 dataset caption으로부터 학습 language model을 학습하기 위해 400,000 이상의 image descriptor를 사용},
archivePrefix = {arXiv},
arxivId = {1411.4952},
author = {Mitchell, Margaret and Doll, Piotr and Iandola, Forrest and Gao, Jianfeng and Zitnick, C Lawrence},
eprint = {1411.4952},
file = {:G$\backslash$:/thesis/Papers/1411.4952v3.pdf:pdf},
journal = {Cvpr},
title = {{From Captions to Visual Concepts and Back}},
year = {2015}
}
@article{Ordonez2011,
author = {Ordonez, V and Kulkarni, G and Berg, Tl},
file = {:G$\backslash$:/Dropbox/PDF/Ordonez, Kulkarni, Berg - 2011 - Im2text Describing images using 1 million captioned photographs(2).pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information {\ldots}},
pages = {1--9},
title = {{Im2text: Describing images using 1 million captioned photographs}},
url = {http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs},
year = {2011}
}
@article{Pan2015,
abstract = {The prediction of saliency areas in images has been traditionally addressed with hand crafted features based on neuroscience principles. This paper however addresses the problem with a completely data-driven approach by training a convolutional network. The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train a not very deep architecture which is both fast and accurate. The convolutional network in this paper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction with a superior performance in all considered metrics.},
archivePrefix = {arXiv},
arxivId = {1507.01422},
author = {Pan, Junting and Gir{\'{o}}-i-Nieto, Xavier},
eprint = {1507.01422},
file = {:G$\backslash$:/Dropbox/PDF/1507.01422.pdf:pdf},
pages = {1--6},
title = {{End-to-end Convolutional Network for Saliency Prediction}},
url = {http://arxiv.org/abs/1507.01422},
year = {2015}
}
@article{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wj},
doi = {10.3115/1073083.1073135},
file = {:G$\backslash$:/Dropbox/PDF/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation(3).pdf:pdf},
isbn = {1-55860-883-4},
issn = {00134686},
journal = {{\ldots} of the 40Th Annual Meeting on {\ldots}},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@article{Pascanu2013,
abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.},
archivePrefix = {arXiv},
arxivId = {1312.6026},
author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1312.6026},
file = {:G$\backslash$:/Dropbox/PDF/1312.6026v5.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6026},
pages = {1--10},
title = {{How to Construct Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1312.6026},
year = {2013}
}
@article{Plummer2015,
abstract = {The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging.},
archivePrefix = {arXiv},
arxivId = {1505.04870},
author = {Plummer, Bryan a. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
eprint = {1505.04870},
file = {:G$\backslash$:/Dropbox/PDF/1505.04870.pdf:pdf},
title = {{Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}},
url = {http://arxiv.org/abs/1505.04870},
year = {2015}
}
@article{Schuster1997,
abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
author = {Schuster, M. and Paliwal, K. K},
doi = {10.1109/78.650093},
file = {:G$\backslash$:/Dropbox/PDF/Schuster, Paliwal - 1997 - Bidirectional recurrent neural networks(2).pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
number = {11},
pages = {2673--2681},
title = {{Bidirectional recurrent neural networks}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=650093},
volume = {45},
year = {1997}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:G$\backslash$:/Dropbox/PDF/Sermanet et al. - 2013 - OverFeat Integrated Recognition , Localization and Detection using Convolutional Networks.pdf:pdf},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Socher2011,
abstract = {Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1{\%}). The features from the image parse tree outperform Gist descriptors for scene classification by 4{\%}.},
author = {Socher, R and Lin, Cc},
doi = {10.1007/978-3-540-87479-9},
file = {:G$\backslash$:/Dropbox/PDF/Socher, Lin - 2011 - Parsing natural scenes and natural language with recursive neural networks(2).pdf:pdf},
isbn = {9781450306195},
issn = {<null>},
journal = {Proceedings of the {\ldots}},
pages = {129--136},
title = {{Parsing natural scenes and natural language with recursive neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/ICML2011Socher{\_}125.pdf},
year = {2011}
}
@article{Socher2010,
abstract = {We propose a semi-supervised model which segments and annotates images using very few labeled images and a large unaligned text corpus to relate image regions to text labels. Given photos of a sports event, all that is necessary to provide a pixel-level labeling of objects and background is a set of newspaper articles about this sport and one to five labeled images. Our model is motivated by the observation that words in text corpora share certain context and feature similarities with visual objects. We describe images using visual words, a new region-based representation. The proposed model is based on kernelized canonical correlation analysis which finds a mapping between visual and textual words by projecting them into a latent meaning space. Kernels are derived from context and adjective features inside the respective visual and textual domains. We apply our method to a challenging dataset and rely on articles of the New York Times for textual features. Our model outperforms the state-of-the-art in annotation. In segmentation it compares favorably with other methods that use significantly more labeled training data.},
author = {Socher, Richard and Fei-Fei, Li},
doi = {10.1109/CVPR.2010.5540112},
file = {:G$\backslash$:/Dropbox/PDF/Socher, Fei-Fei - 2010 - Connecting modalities Semi-supervised segmentation and annotation of images using unaligned text corpora.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {966--973},
pmid = {5540112},
title = {{Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora}},
year = {2010}
}
@article{Socher2014,
abstract = {Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.},
author = {Socher, Richard and Karpathy, Andrej and Le, Quoc V and Manning, Christopher D and Ng, Andrew Y},
file = {:G$\backslash$:/Dropbox/PDF/Socher et al. - 2014 - Grounded Compositional Semantics for Finding and Describing Images with Sentences.pdf:pdf},
issn = {2307-387X},
journal = {Tacl},
number = {April},
pages = {207--218},
title = {{Grounded Compositional Semantics for Finding and Describing Images with Sentences}},
url = {http://nlp.stanford.edu/{~}socherr/SocherLeManningNg{\_}nipsDeepWorkshop2013.pdf},
volume = {2},
year = {2014}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
eprint = {1502.03044},
file = {:G$\backslash$:/Dropbox/PDF/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention(2).pdf:pdf},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@article{Yao2012,
abstract = {In this paper we propose an approach to holistic scene understanding that reasons jointly about regions, location, class and spatial extent of objects, presence of a class in the image, as well as the scene type. Learning and inference in our model are efficient as we reason at the segment level, and introduce auxiliary variables that allow us to decom- pose the inherent high-order potentials into pairwise poten- tials between a fewvariables with small number of states (at most the number of classes). Inference is done via a conver- gent message-passing algorithm, which, unlike graph-cuts inference, has no submodularity restrictions and does not require potential specific moves. We believe this is very im- portant, as it allows us to encode our ideas and prior knowl- edge about the problem without the need to change the in- ference engine every time we introduce a newpotential. Our approach outperforms the state-of-the-art on the MSRC-21 benchmark, while being much faster. Importantly, our holis- tic model is able to improve performance in all tasks.},
author = {Yao, Jian and Fidler, Sanja and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6247739},
file = {:G$\backslash$:/Dropbox/PDF/Yao, Fidler, Urtasun - 2012 - Describing the scene as a whole Joint object detection, scene classification and semantic segmentation.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
pages = {702--709},
title = {{Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation}},
year = {2012}
}
@article{Zhang2005,
abstract = {Abstract A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola -Jones detector cascade and training it with a new variant of ... $\backslash$n},
author = {Zhang, C and Platt, J C and Viola, P a},
file = {:G$\backslash$:/Dropbox/PDF/NIPS2005{\_}590.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Neural Information Processing Systems},
number = {10},
pages = {1769--1775},
title = {{Multiple instance boosting for object detection}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231211001299$\backslash$npapers3://publication/doi/10.1016/j.neucom.2011.02.011},
volume = {74},
year = {2005}
}
@article{Zhang2015,
abstract = {We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.},
archivePrefix = {arXiv},
arxivId = {1510.02693},
author = {Zhang, ShiLiang and Jiang, Hui and Wei, Si and Dai, LiRong},
eprint = {1510.02693},
file = {:G$\backslash$:/Dropbox/PDF/1510.02693v1.pdf:pdf},
pages = {3--6},
title = {{Feedforward Sequential Memory Neural Networks without Recurrent Feedback}},
url = {http://arxiv.org/abs/1510.02693},
year = {2015}
}
@article{Zhao2015,
author = {Zhao, Rui and Li, Hongsheng and Wang, Xiaogang},
file = {:G$\backslash$:/Dropbox/PDF/Zhao, Li, Wang - 2015 - Saliency Detection by Multi-Context Deep Learning.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr},
pages = {1265--1274},
title = {{Saliency Detection by Multi-Context Deep Learning}},
year = {2015}
}
