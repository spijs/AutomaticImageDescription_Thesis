@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Krizhevsky2012a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks(2).pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Rampf2015,
abstract = {Very. Indeed, it is shown here that in a flat, cold dark matter (CDM) dominated Universe with positive cosmological constant ({\$}\backslashLambda{\$}), modelled in terms of a Newtonian and collisionless fluid, particle trajectories are analytical in time (representable by a convergent Taylor series) until at least a finite time after decoupling. The time variable used for this statement is the cosmic scale factor, i.e., the "{\$}a{\$}-time", and not the cosmic time. For this, a Lagrangian-coordinates formulation of the Euler-Poisson equations is employed, originally used by Cauchy for 3-D incompressible flow. Temporal analyticity for {\$}\backslashLambda{\$}CDM is found to be a consequence of novel explicit all-order recursion relations for the {\$}a{\$}-time Taylor coefficients of the Lagrangian displacement field, from which we derive the convergence of the {\$}a{\$}-time Taylor series. A lower bound for the {\$}a{\$}-time where analyticity is guaranteed and shell-crossing is ruled out is obtained, whose value depends only on {\$}\backslashLambda{\$} and on the initial spatial smoothness of the density field. The largest time interval is achieved when {\$}\backslashLambda{\$} vanishes, i.e., for an Einstein-de Sitter universe. Analyticity holds also if, instead of the {\$}a{\$}-time, one uses the linear structure growth {\$}D{\$}-time, but no simple recursion relations are then obtained. The analyticity result also holds when a curvature term is included in the Friedmann equation for the background, but inclusion of a radiation term arising from the primordial era spoils analyticity.},
archivePrefix = {arXiv},
arxivId = {1504.0032},
author = {Rampf, Cornelius and Villone, Barbara and Frisch, Uriel},
eprint = {1504.0032},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/coco{\_}caption{\_}generation.pdf:pdf},
pages = {16},
title = {{Microsoft COCO Captions: Data Collection and Evaluation Server}},
url = {http://arxiv.org/abs/1504.0032},
year = {2015}
}
@article{Chen2014,
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.5654v1},
author = {Chen, Xinlei and Zitnick, C Lawrence},
eprint = {arXiv:1411.5654v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Zitnick - 2014 - Learning a Recurrent Visual Representation for Image Caption Generation.pdf:pdf},
journal = {Proceedings of CoRR},
title = {{Learning a Recurrent Visual Representation for Image Caption Generation}},
year = {2014}
}
@article{Karpathy2014,
abstract = {We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.},
archivePrefix = {arXiv},
arxivId = {1406.5679},
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
eprint = {1406.5679},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Joulin, Fei-Fei - 2014 - Deep Fragment Embeddings for Bidirectional Image Sentence Mapping.pdf:pdf},
pages = {1--9},
title = {{Deep Fragment Embeddings for Bidirectional Image Sentence Mapping}},
url = {http://arxiv.org/abs/1406.5679},
year = {2014}
}
@article{Mao2015,
abstract = {In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/{\~{}}junhua.mao/projects/child{\_}learning.html},
archivePrefix = {arXiv},
arxivId = {1504.06692},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
eprint = {1504.06692},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2015 - Learning like a Child Fast Novel Visual Concept Learning from Sentence Descriptions of Images.pdf:pdf},
title = {{Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images}},
url = {http://arxiv.org/abs/1504.06692},
year = {2015}
}
@article{Kiros2013,
abstract = {Abstract We introduce two multimodal neural language models : models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase ... $\backslash$n},
author = {Kiros, R and Zemel, R and Salakhutdinov, R},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2013 - Multimodal Neural Language Models.pdf:pdf},
journal = {Proc NIPS Deep Learning {\ldots}},
keywords = {Image Tag Inference},
pages = {1--14},
title = {{Multimodal Neural Language Models}},
url = {http://www.cs.toronto.edu/{~}rkiros/papers/mnlm2014.pdf$\backslash$npapers3://publication/uuid/00AE85FB-98DC-4EB9-A6E8-A423C47C0B98},
year = {2013}
}
@article{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/ICCV.2011.6126456},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1409.4842v1.pdf:pdf},
isbn = {9781467369640},
issn = {1550-5499},
journal = {arXiv preprint arXiv:1409.4842},
pages = {1--12},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842v1},
year = {2014}
}
@article{Jia2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5093v1},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor and Eecs, U C Berkeley},
eprint = {arXiv:1408.5093v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
isbn = {9781450330633},
journal = {ACM Conference on Multimedia},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
title = {{Caffe : Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
eprint = {1502.03044},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@article{Karpathy2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1503.08909v2},
author = {Karpathy, a. and Fei-Fei, Li},
eprint = {1503.08909v2},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Image Des.pdf:pdf},
journal = {Cvpr2015},
title = {{Deep Visual-Semantic Alignments for Generating Image Des}},
year = {2015}
}
@article{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their pre- dictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thou- sands of novel labels never seen by the visual model.},
author = {Frome, Andrea and Corrado, Gs and Shlens, Jonathon},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frome, Corrado, Shlens - 2013 - Devise A deep visual-semantic embedding model.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural {\ldots}},
pages = {1--11},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sermanet et al. - 2013 - OverFeat Integrated Recognition , Localization and Detection using Convolutional Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6229},
pages = {1--15},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
eprint = {1405.0312},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1405.0312v3.pdf:pdf},
title = {{Microsoft COCO: Common Objects in Context}},
url = {http://arxiv.org/abs/1405.0312},
year = {2014}
}
@article{Goldberg2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1510.00726v1},
author = {Goldberg, Yoav},
eprint = {arXiv:1510.00726v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldberg - 2015 - A Primer on Neural Network Models for Natural Language Processing.pdf:pdf},
pages = {1--75},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
year = {2015}
}
@article{Johnson2015,
abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
archivePrefix = {arXiv},
arxivId = {1511.07571},
author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
eprint = {1511.07571},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1511.07571v1.pdf:pdf},
title = {{DenseCap: Fully Convolutional Localization Networks for Dense Captioning}},
url = {http://arxiv.org/abs/1511.07571},
year = {2015}
}
@article{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final mAP of 48{\%} on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
annote = {Gebruikt Caffe voor implemenatie},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr'14},
pages = {2--9},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@article{Pan2015,
abstract = {The prediction of saliency areas in images has been traditionally addressed with hand crafted features based on neuroscience principles. This paper however addresses the problem with a completely data-driven approach by training a convolutional network. The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train a not very deep architecture which is both fast and accurate. The convolutional network in this paper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction with a superior performance in all considered metrics.},
archivePrefix = {arXiv},
arxivId = {1507.01422},
author = {Pan, Junting and Gir{\'{o}}-i-Nieto, Xavier},
eprint = {1507.01422},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Gir{\'{o}}-i-Nieto - 2015 - End-to-end Convolutional Network for Saliency Prediction.pdf:pdf},
pages = {1--6},
title = {{End-to-end Convolutional Network for Saliency Prediction}},
url = {http://arxiv.org/abs/1507.01422},
year = {2015}
}
@article{Le2015,
author = {Le, Quoc V},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le - 2015 - A Tutorial on Deep Learning Part 2 Autoencoders , Convolutional Neural Networks and Recurrent Neural Networks.pdf:pdf},
pages = {1--19},
title = {{A Tutorial on Deep Learning Part 2 : Autoencoders , Convolutional Neural Networks and Recurrent Neural Networks}},
year = {2015}
}
@article{Zhao2015,
author = {Zhao, Rui and Li, Hongsheng and Wang, Xiaogang},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Li, Wang - 2015 - Saliency Detection by Multi-Context Deep Learning.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr 2015},
pages = {1265--1274},
title = {{Saliency Detection by Multi-Context Deep Learning}},
year = {2015}
}
@article{Pascanu2013,
abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.},
archivePrefix = {arXiv},
arxivId = {1312.6026},
author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1312.6026},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu et al. - 2013 - How to Construct Deep Recurrent Neural Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6026},
pages = {1--10},
title = {{How to Construct Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1312.6026},
year = {2013}
}
@article{Jin2015,
abstract = {Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of "abstract meaning", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1506.06272},
author = {Jin, Junqi and Fu, Kun and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
eprint = {1506.06272},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2015 - Aligning where to see and what to tell image caption with region-based attention and scene factorization.pdf:pdf},
pages = {1--20},
title = {{Aligning where to see and what to tell: image caption with region-based attention and scene factorization}},
url = {http://arxiv.org/abs/1506.06272},
year = {2015}
}
@article{Devlin2015a,
abstract = {We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the "consensus" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach.},
archivePrefix = {arXiv},
arxivId = {1505.04467},
author = {Devlin, Jacob and Gupta, Saurabh and Girshick, Ross and Mitchell, Margaret and Zitnick, C. Lawrence},
eprint = {1505.04467},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2015 - Exploring Nearest Neighbor Approaches for Image Captioning.pdf:pdf},
journal = {arXiv preprint},
pages = {1--6},
title = {{Exploring Nearest Neighbor Approaches for Image Captioning}},
url = {http://arxiv.org/abs/1505.04467},
year = {2015}
}
@article{Mao2014a,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.1090v1},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
eprint = {arXiv:1410.1090v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
journal = {arXiv:1410.1090 [cs]},
pages = {1--9},
title = {{Explain Images with Multimodal Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1410.1090 http://www.arxiv.org/pdf/1410.1090.pdf},
year = {2014}
}
@article{Le2014,
author = {Le, Quoc V},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le - 2014 - A Tutorial on Deep Learning Part 1 Nonlinear Classifiers and The Backpropagation Algorithm.pdf:pdf},
pages = {1--18},
title = {{A Tutorial on Deep Learning Part 1: Nonlinear Classifiers and The Backpropagation Algorithm}},
year = {2014}
}
@article{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/{\~{}}junhua.mao/m-RNN.html .},
archivePrefix = {arXiv},
arxivId = {1412.6632},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
eprint = {1412.6632},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:pdf},
number = {2014},
pages = {1--17},
title = {{Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)}},
url = {http://arxiv.org/abs/1412.6632},
volume = {1090},
year = {2014}
}
@article{Kiros2014,
abstract = {Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2539v1},
author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S},
eprint = {arXiv:1411.2539v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Salakhutdinov, Zemel - 2014 - Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.pdf:pdf},
journal = {arXiv preprint arXiv:1411.2539},
pages = {1--13},
title = {{Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models}},
year = {2014}
}
@article{Arge2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arge, Mage - 2015 - V d c n l -s i r.pdf:pdf},
title = {{Very Deep Convolutional Networks For Large-Scale Image Recognition}},
year = {2015}
}
@article{Liu2015,
abstract = {Recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large image dataset can be used as a universal image descriptor, and that doing so leads to impressive performance for a variety of image classification tasks. Most of these studies adopt activations from a single DCNN layer, usually the fully-connected layer, as the image representation. In this paper, we proposed a novel way to extract image representations from two consecutive convolutional layers: one layer is utilized for local feature extraction and the other serves as guidance to pool the extracted features. By taking different viewpoints of convolutional layers, we further develop two schemes to realize this idea. The first one directly uses convolutional layers from a DCNN. The second one applies the pretrained CNN on densely sampled image regions and treats the fully-connected activations of each image region as convolutional feature activations. We then train another convolutional layer on top of that as the pooling-guidance convolutional layer. By applying our method to three popular visual classification tasks, we find our first scheme tends to perform better on the applications which need strong discrimination on subtle object patterns within small regions while the latter excels in the cases that require discrimination on category-level patterns. Overall, the proposed method achieves superior performance over existing ways of extracting image representations from a DCNN.},
archivePrefix = {arXiv},
arxivId = {1510.00921},
author = {Liu, Lingqiao and Shen, Chunhua and Hengel, Anton Van Den},
eprint = {1510.00921},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Shen, Hengel - 2015 - Cross-convolutional-layer Pooling for Generic Visual Recognition.pdf:pdf},
number = {2},
pages = {1--7},
title = {{Cross-convolutional-layer Pooling for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1510.00921},
year = {2015}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/nature14539.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539$\backslash$n10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Liang2015,
abstract = {각각의 convolution layer가 recurrent connection 되어 있는 recurrent CNN을 통해 object recognition 수행},
author = {Liang, Ming and Hu, Xiaolin},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Hu - 2015 - Recurrent Convolutional Neural Network for Object Recognition.pdf:pdf},
journal = {Cvpr},
number = {Figure 1},
title = {{Recurrent Convolutional Neural Network for Object Recognition}},
year = {2015}
}
@article{Donahue2015,
abstract = {Visual recognition을 할 때 sequence를 인식 작업에 포함시키면 더 좋은 성능을 낼 것이다. 따라서 Long term Recurrent Convolution(LTRC)를 통하여 다양한 visual sequence 정보를 포함시켜 결과를 추론함.},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {1411.4389},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor and Austin, U T and Lowell, Umass and Berkeley, U C},
eprint = {1411.4389},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue et al. - 2015 - Long-term Recurrent Convolutional Networks for Visual Recognition and Description.pdf:pdf},
journal = {Cvpr},
title = {{Long-term Recurrent Convolutional Networks for Visual Recognition and Description}},
year = {2015}
}
@article{Mitchell2015,
abstract = {Image descriptor(caption)을 automatically generating 하는 방법 소개 visual detector, language models, multimodal similarity model을 이미지 dataset caption으로부터 학습 language model을 학습하기 위해 400,000 이상의 image descriptor를 사용},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {1411.4952},
author = {Mitchell, Margaret and Doll, Piotr and Iandola, Forrest and Gao, Jianfeng and Zitnick, C Lawrence},
eprint = {1411.4952},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell et al. - 2015 - From Captions to Visual Concepts and Back.pdf:pdf},
journal = {Cvpr},
title = {{From Captions to Visual Concepts and Back}},
year = {2015}
}
@article{Socher2014,
abstract = {Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.},
author = {Socher, Richard and Karpathy, Andrej and Le, Quoc V and Manning, Christopher D and Ng, Andrew Y},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher et al. - 2014 - Grounded Compositional Semantics for Finding and Describing Images with Sentences.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
number = {April},
pages = {207--218},
title = {{Grounded Compositional Semantics for Finding and Describing Images with Sentences}},
url = {http://nlp.stanford.edu/{~}socherr/SocherLeManningNg{\_}nipsDeepWorkshop2013.pdf},
volume = {2},
year = {2014}
}
@article{Cho2015,
abstract = {Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.},
archivePrefix = {arXiv},
arxivId = {1507.01053},
author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
eprint = {1507.01053},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Courville, Bengio - 2015 - Describing Multimedia Content using Attention-based Encoder--Decoder Networks.pdf:pdf},
pages = {1--12},
title = {{Describing Multimedia Content using Attention-based Encoder--Decoder Networks}},
url = {http://arxiv.org/abs/1507.01053},
year = {2015}
}
@article{Ciresan2012,
abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.2745v1},
author = {Ciresan, D},
doi = {10.1109/CVPR.2012.6248110},
eprint = {arXiv:1202.2745v1},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/IDSIA-04-12.pdf:pdf},
isbn = {9781467312288},
issn = {1063-6919},
journal = {Computer Vision and {\ldots}},
keywords = {Benchmark testing,Computer architecture,Error analysis,Graphics processing unit,Neurons,Training},
number = {February},
pages = {3642--3649},
pmid = {18225950},
title = {{Multi-column Deep Neural Networks for Image Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6248110},
year = {2012}
}
@article{Gong2014,
abstract = {This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of im- ages that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description.},
author = {Gong, Yunchao and Wang, Liwei and Hodosh, Micah and Hockenmaier, Julia},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gong et al. - 2014 - Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections.pdf:pdf},
journal = {Computer Vision–ECCV},
pages = {529--545},
title = {{Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections}},
year = {2014}
}
