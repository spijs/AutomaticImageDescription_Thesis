@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/pascanu grad clipping.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{Weenink2003,
author = {Weenink, David},
doi = {10.4135/9781412961288.n37},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/weenink{\_}cca.pdf:pdf},
isbn = {9781412961271},
journal = {Proceedings of the Institute of Phonetic Sciences of the University of Amsterdam},
pages = {81--99},
title = {{Canonical correlation analysis}},
volume = {25},
year = {2003}
}
@article{Huang2005,
abstract = {The problem of parameter estimation from Rician distributed data (e.g., magnitude magnetic resonance images) is addressed. The properties of conventional estimation methods are discussed and compared to maximum-likelihood (ML) estimation which is known to yield optimal results asymptotically. In contrast to previously proposed methods, ML estimation is demonstrated to be unbiased for high signal-to-noise ratio (SNR) and to yield physical relevant results for low SNR.},
author = {Huang, Jonathan},
doi = {10.1.1.93.2881},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/dirichlet.pdf:pdf},
issn = {02780062},
journal = {Distribution},
number = {2},
pages = {1--9},
pmid = {9735899},
title = {{Maximum Likelihood Estimation of Dirichlet Distribution Parameters}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.2881{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {40},
year = {2005}
}
@article{Bengio2013,
abstract = {After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.0901v2},
author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
doi = {10.1109/ICASSP.2013.6639349},
eprint = {arXiv:1212.0901v2},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/advances in training nerual networks.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Recurrent networks,deep learning,long-term dependencies,representation learning},
pages = {8624--8628},
title = {{Advances in optimizing recurrent networks}},
year = {2013}
}
@article{Duchi2011,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/adagrad.pdf:pdf},
journal = {The Journal of Machine Learning Research (JMLR)},
pages = {2121--2159},
title = {{Adaptive subgradient methods for online learning and stochastic optimization}},
volume = {12},
year = {2011}
}
@article{Srivastava2013,
abstract = {Deep neural nets with a huge number of parameters are very powerful machine learning systems. How- ever, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from a neural network during training. This prevents the units from co-adapting too much. Dropping units creates thinned networks during training. The number of possible thinned networks is exponential in the number of units in the network. At test time all possible thinned networks are com- bined using an approximate model averaging procedure. Dropout training followed by this approximate model combination significantly reduces overfitting and gives major improvements over other regulariza- tion methods. In this work, we describe models that improve the performance of neural networks using dropout, often obtaining state-of-the-art results on benchmark datasets.},
author = {Srivastava, N},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/dropout.pdf:pdf},
journal = {Thesis},
title = {{Improving neural networks with dropout}},
url = {http://www.cs.toronto.edu/{~}nitish/msc{\_}thesis.pdf},
year = {2013}
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/adadelta.pdf:pdf},
isbn = {1212.5701},
journal = {arXiv},
pages = {6},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
doi = {10.1.1.208.6449},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/glorot11a.pdf:pdf},
issn = {15324435},
journal = {Aistats},
pages = {315--323},
title = {{Deep Sparse Rectifier Neural Networks}},
volume = {15},
year = {2011}
}
@article{Zinman2010,
abstract = {Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors--and specifically those of the brain--in decisions to trust, the biological influences should naturally include those related to gender. To show empirically that online trust is associated with activity changes in certain brain areas, the authors used functional magnetic resonance imaging (fMRI). In a laboratory experiment, they captured the brain activity of 10 female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. They found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, they found that women activated more brain areas than did men.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Zinman, A and Donath, J and Zhu, Yanchun and Zhang, Wenping Wei and Yu, Changhai and Zhou, J and Zhao, Fang and Huang, Yongzhen Yalou and Wang, Liang and Tan, Tieniu and Zhang, Tong and Zhang, Liang and Yang, Jie Jing and Chu, Wei and Tseng, Belle L and Zhang, Dongsong Dakun and Yan, Zhijun and Jiang, Hansi and Kim, Taeha and Zhai, Zhongwu and Liu, Bing and Xu, Hua and Jia, Peifa and YouTube and Yoo, Kyung-Hyan and Gretzel, Ulrike and Ying, Xiaowei and Wu, Xintao and {Barbar $\backslash$'a}, Daniel and Yang, Kun Kuo-Shu and Yang, Chao and Zhang, Jie Jialong Jun and Gu, Guofei and Xuan, Liu and Pengzhu, Zhang and Xu, Chang and Zhang, Jie Jialong Jun and Chang, Kuiyu Kevin Chen-Chuan and Long, Chong and Xiao, Bo and Benbasat, Izak and Wu, Shelly Xiaonan and Banzhaf, Wolfgang and Wu, Guangyu and Greene, Derek and Smyth, Barry and Cunningham, P 'adraig and Whissell, John S. and Clarke, Charles L. A. and Welch, Michael J and Schonfeld, Uri and He, Dan and Cho, Junghoo and Wang, Xiaolong Xinxi and Wang, Ye and Wei, Furu and Liu, Xiaohua and Zhou, Ming and Zhang, Ming Michael and Wang, De and Irani, Danesh and Pu, Calton and Wang, Chong and Blei, David M and Li, Fei-Fei and Wang, Alex Hai and Virdhagriswaran, Sankar and Dakin, Gordon and Vijayalekshmi, S and Rabara, S A and van Marle, D and Utgoff, Paul E and Ukkonen, Esko and Tsui, Anne S and Tsai, Cheng Hao and Lin, Chenghua Chin Y Chih-Jen Jen Chieh Yen and Lin, Chenghua Chin Y Chih-Jen Jen Chieh Yen and Toneguzzi, M and Tiantian, Qin and Burgoon, Judee K and Blair, J P and Nunamaker, J F and Tian, Tian and Zhu, Jun and Xia, Fen and Zhuang, Xin and Zhang, Tong and Thomas, J and Biros, D and Tan, Enhua and Guo, Lei and Chen, Songqing and Zhang, Xiaodong and Zhao, Yihong and Swaminathan, Ashwin and Cattelan, Renan G and Wexler, Ydo and Mathew, Cherian V and Kirovski, Darko and Sureka, Ashish and Suhara, Yoshihiko and Toda, Hiroyuki and Nishioka, Shuichi and Susaki, Seiji and Stone-Gross, Brett and Stevens, Ryan and Zarras, Apostolis and Kemmerer, Richard and Kruegel, Chris and Vigna, Giovanni and Steyvers, Mark and Griffiths, Tom and Spirin, Nikita and Han, Jiawei and Soucy, Pascal and Mineau, Guy W and Sood, Sara Owsley and Churchill, Elizabeth F and Antin, Judd and Song, Long and Zhang, Wenping Wei and Lau, RYK and Liao, SSY and Kwok, RCW and Singhal, Amit and Choi, John and Hindle, Donald and Lewis, David D and Pereira, Fernando and Singh, Monika and Bansal, Divya and Sofat, Sanjeev and Short, John and Christie, Bruce and Williams, Ederyn and Shoaib, M. and Farooq, M. and Shapiro, Joseph M and Lamont, Gary B and Peterson, Gilbert L and Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan and Cotter, Andrew and Sebastiani, Fabrizio and Ricerche, Consiglio Nazionale Delle and Sculley, David and Salton, Gerard and McGill, Michael J and Sahin, Y and Duman, E and Rosenblatt, Frank and Rockmann, Kevin W and Northcraft, Gregory B and Robinson, Sandra and Robertson, Stephen E and Walker, Steve and Beaulieu, M M and Gatford, Mike and Payne, Alison and Riedl, Ren{\~{A}}© and Hubert, Marco and Kenning, Peter and Resnick, Paul and Zeckhauser, Richard and Swanson, John and Lockwood, Kate and Rayana, Shebuti and Akoglu, Leman and Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D and Po-Ching, Lin and Po-Min, Huang and Perez, D G and Lavalle, M M and Pavlou, Paul A and Dimoka, Angelika and Pandit, Shashank and Chau, Duen Horng and Wang, Samuel Shengjie and Faloutsos, Christos and Ott, Myle and Choi, Yejin and Cardie, Claire and Hancock, Jeffrey T and Ostaszewski, Marek and Seredynski, Franciszek and Bouvry, Pascal and O'Callaghan, Derek and Harrigan, Martin and Carthy, Joe and Cunningham, P 'adraig and {Nhien An Le}, Khac and Markos, S and Kechadi, M T and Nevo, Dorit and Benbasat, Izak and Wand, Yair and Najada, Hamzah Al and Zhu, Xingquan and Myers, Eugene W and Musau, Felix and Wang, Guojun and Abdullahi, Muhammad Bashir and Mukherjee, Subhabrata and Malu, Akshat and A.R., Balamurali and Bhattacharyya, Pushpak and Mukherjee, Arjun and Liu, Bing and Glance, Natalie and Kumar, Abhinav and Liu, Bing and Wang, Junhui and Hsu, Meichun and Castellanos, Malu and Ghosh, Riddhiman and Moghaddam, Samaneh and Ester, Martin and Mitchell, Tom M and Miller, Zachary and Dickinson, Brian and Deitrick, William and Hu, Wei and Wang, Alex Hai and {Miguel Helft} and Metwally, Ahmed and Agrawal, Divyakant and {El Abbadi}, Amr and Abbadi, Amr El and Mesleh, Abdelwadood Moh'd A and Mercer, Lindsay C and Mehrabian, Albert and Russell, James A and McAllister, Daniel J and Mason, Jennifer and Martinez-Romo, Juan and Araujo, Lourdes and Martinez, Christophe and Tony, Giraud-Carrier and Markines, Benjamin and Cattuto, Ciro and Menczer, Filippo and Maranzato, Rafael and Pereira, Adriano M and Neubert, Marden and do Lago, Alair Pereira and Neubert, Marden and Pereira, Adriano M and do Lago, Alair Pereira and Manaskasemsak, Bundit and Jiarpakdee, Jirayus and Rungsawang, Arnon and Luca, Michael and Zervas, Georgios and Liu, Tao and Liu, Ou and Ma, Jian and Poon, Pak-Lok and Zhang, Jie Jialong Jun and Liu, Jingjing and Cao, Yunbo and Lin, Chenghua Chin Y Chih-Jen Jen Chieh Yen and Huang, Yongzhen Yalou and Zhou, Ming and Liu, Bing and Liu, Lin and Tsykin, Anna and Goodall, Gregory J and Green, Jeffrey E and Zhu, Min and Kim, Chang Hee and Li, Jiuyong and Lin, Yu-Ru and Sundaram, Hari and Chi, Yun and Tatemura, Junichi and Tseng, Belle L and Lin, Liu and Kun, Jia and Lin, Chenghua Chin Y Chih-Jen Jen Chieh Yen and Weng, Ruby C and Keerthi, S Sathiya and He, Yulan and Lim, Ee-Peng and Nguyen, Viet-An and Jindal, Nitin and Liu, Bing and Lauw, Hady Wirawan and Li-Jen, Kao and Yo-Ping, Huang and Li, Ze Zhiguo and Shen, Haiying and Grant, Joseph Edward and Li, Yi and Long, PhilipM and Li, Wenbo and Sun, Le and Feng, Yuanyong and Zhang, Dongsong Dakun and Li, Rui and Wang, Samuel Shengjie and Deng, Hongbo and Wang, Rui and Chang, Kuiyu Kevin Chen-Chuan and Leon-Suematsu, Yutaka I and Inui, Kentaro and Kurohashi, Sadao and Kidawara, Yutaka and Lee, Kyumin and Caverlee, James and Webb, Steve and Cheng, Zhiyuan and Sui, Daniel Z and Klos, Tomas B and Alkemade, Floortje and Kim, Soo-Min and Pantel, Patrick and Chklovski, Tim and Pennacchiotti, Marco and Kietzmann, Jan H and Hermkens, Kristopher and McCarthy, Ian P and Silvestre, Bruno S and Kaplan, Andreas M and Haenlein, Michael and Kaiyong, Deng and Ru, Zhang and Dongfang, Zhang and WenFeng, Jiang and Xinxin, Niu and Hong, Guo and Johnson, Paul E and Grazioli, Stefano and Jamal, Karim and Berryman, R G and Johns, Gary and Joachims, Thorsten and Jindal, Nitin and Liu, Bing and Lim, Ee-Peng and Ji, Li and Lu, Liu and Jie, Xu and {James Poon}, Teng Fatt and Jagatic, Tom N and Johnson, Nathaniel A and Jakobsson, Markus and Menczer, Filippo and Iyer, Divya and Mohanpurkar, Arti and Janardhan, Sneha and Rathod, Dhanashree and Sardeshmukh, Amruta and IC3 and Hofmeyr, Steven Andrew and Heydari, Atefeh and ali Tavakoli, Mohammad and Salim, Naomie and Heydari, Zahra and Hesse-Biber, Sharlene and Heinrich, Christina U and Borkenau, Peter and Harris, Lloyd C and Harold and Nguyen and Nexgate and Harary, F and Halliday, Michael A and Matthiessen, Christian M and Hahn, Jungpil and Wang, Tawei and Gupta, S and Johari, R and Gupta, Rajeev and Gupta, Himanshu and Mohania, Mukesh and Guha, R and Kumar, Ravi and Raghavan, Prabhakar and Tomkins, Andrew and Grier, Chris and Thomas, Kurt and Paxson, Vern and Zhang, Ming Michael and Gregg, Dawn G and Scott, Judy E and George, J F and Marett, K and Tilley, P and Geng, Xin and Smith-Miles, Kate and Ge, Liang and Gao, Jing and Li, Xiaoyi and Zhang, Aidong and Gayo-Avello, Daniel and Gabbur, Prasad and Pankanti, Sharath and Fan, Quanfu and Trinh, Hoang and Fu, Xianghua and Yang, Kun Kuo-Shu and Huang, Joshua Zhexue and Cui, Laizhong and Fran{\c{c}}ois, Rousseau and Michalis, Vazirgiannis and Forman, George and Fisher, Douglas H and Felt, Adrienne and Evans, David and Fayazi, Amir and Lee, Kyumin and Caverlee, James and Squicciarini, Anna and Fawcett, Tom and Provost, Foster and Fairclough, Norman and Excell, David and Ensing and David and Dunham, Margaret H and Ming, Data and Duda, Richard O and Hart, Peter E and Stork, David G and Duan, Zhenhai and Gopalan, Kartik and Yuan, Xin and Djamasbi, Soussan and Loiacono, Eleanor T and Dellarocas, Chrysanthos and Dave, Vacha and Guha, Saikat and Zhang, Yin and Danescu-Niculescu-Mizil, Cristian and Kossinets, Gueorgi and Kleinberg, Jon and Lee, Lillian and Dai, Hanbo and Zhu, Feida and Lim, Ee-Peng and Pang, HweeHwa and Daft, R L and Lengel, R H and Dae-Ha, Park and Eun-Ae, Cho and Byung-Won, On and Cui, K and Zhou, B and Jia, Y and Liang, Z and Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram and Cortes, Corinna and Vapnik, Vladimir and Congfu, Xu and Baojun, Su and Yunbiao, Cheng and Weike, Pan and Chu, Victor W. and Wong, Raymond K. K. and Chen, Fang and Chi, Chi-Hung and Chin, Wynne and Marcolin, Barbara and Newsted, Peter and Chen, Yu-Ren and Chen, Hsin-Hsi Hsinchun and Chang, Remco and Ghoniem, Mohammad and Kosara, Robert and Ribarsky, William and Yang, Jie Jing and Suma, Evan and Ziemkiewicz, Caroline and Kern, Daniel and Sudjianto, Agus and Chang, I Chiu and Hwang, Hsin-Ginn and Hung, Ming-Chien and Lin, Ming-Hui and Yen, David C and Chandramouli, R and Chandola, Varun and Banerjee, Arindam and Kumar, Vipin and Cenfetelli, Ronald T and Benbasat, Izak and Al-Natour, Sameh and Casella, George and George, Edward I and Cappella, Joseph N and Greene, John O and Burgoon, Judee K and Buller, David B and Ebesu, Amy and Rockwell, Patricia and Burgoon, Judee K and Brown, Garrett and Howe, Travis and Ihbe, Micheal and Prakash, Atul and Borders, Kevin and Brody, Samuel and Elhadad, Noemie and Boyle, R J and Boyd, D and Heer, J and Boshmaf, Yazan and Muslukhov, Ildar and Beznosov, Konstantin and Ripeanu, Matei and Borgatti, Stephen P and Mehra, Ajay and Brass, Daniel J and Labianca, Giuseppe and Bollen, Johan and Mao, Huina and Zeng, Xiaojun and Blei, David M and Ng, Andrew Y and Jordan, Michael I and Blanco, Roi and Lioma, Christina and Bishop, Christopher M and B{\'{\i}}r{\'{o}}, Istv{\'{a}}n and Szab{\'{o}}, J{\'{a}}cint and Bencz{\'{u}}r, Andr{\'{a}}s A and {Benson Edwin Raj}, S and Portia, A A and icio Benevenuto, Fabr ' and Rodrigues, Tiago and ilio Almeida, Virg ' and Almeida, Jussara and Gon$\backslash$ccalves, Marcos and Veloso, A and Almeida, Jussara and Goncalves, M and ilio Almeida, Virg ' and Balfe, Shane and Paterson, Kenneth G and Ba, Sulin and Pavlou, Paul A and Au, Norman and Ngai, Eric W T and Cheng, T C Edwin and Aphinyanaphongs, Yindalon and Fu, Lawrence D and Li, Ze Zhiguo and Peskin, Eric R and Efstathiadis, Efstratios and Aliferis, Constantin F and Statnikov, Alexander and Amati, Gianni and {Van Rijsbergen}, Cornelis Joost and Alsaleh, Mansour and Alarifi, Abdulrahman and Al-Salman, Abdul Malik and Alfayez, Mohammed and Almuhaysin, Abdulmajeed and Al-Harbi, S and Almuhareb, A and Al-Thubaity, A and Khorsheed, M S and Al-Rajeh, A and Al-Furiah, Saleh and Al-Braheem, Lamia and Alexandrov, Mikhail and Gelbukh, Alexander F and Lozovoi, George and Ahmed, Shabbir and Mithun, Farzana and Ahmed, Faraz and Abulaish, Muhammad and Abbasi, Ahmed and Zhang, Zhu and Zimbra, David and Chen, Hsin-Hsi Hsinchun and {Nunamaker Jr}, Jay F},
doi = {10.1016/s0364-0213(01)00040-4},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/gibbssamplingbook.pdf:pdf},
isbn = {1532-4435},
issn = {1386-4564},
journal = {MIS Quarterly},
keywords = {0454,0723,0984,1220,1494,1516,1581 7837,1781 4068 7406 7375 825,2022,2100,2340,2382 7401,2400,2420,2500,2586,2593 677,2598 4068 7406 7375 825,2667,2671 7694,2750,3020,3040,3116 2022,3660,3892 3869 6601 8049 1516,4063 1573,4067 4068 7406 7375 825,4300,454112,4550 8550 1070 5831,4605 1583,4901,4952,5250,5497 7375 825,6213 1516,6242 5831,6613 7375 825,708 5760,7300,7419 5760,7906,8249 6298 7375 825,8390,8444 5760,9130,9190,ATT,Accuracy,Adaptable,Adulthood (18 yrs {\&} older),Aftereffect model,Algorithms,Anomaly detection,Applied,Applied sciences,Astroturf political campaigns,Asymmetric and Private Information (D820),Auctions,Auditory Perception,Automated social engineering,BM25,BNC,BUSINESS ethics,Bayes methods,Bayesian network classifier,Bayesian network classifiers,Behavioral Research,Behavioral sciences,Benchmark testing,Biological Sciences,Bose--Einstein statistics,Botnets,Browsers,Business and Economics,COMMUNITY STRUCTURE,COMPLEX NETWORKS,CONDITIONED response,CONNECTIVITY,CONSUMER education,CONSUMER fraud,CONSUMER protection,Celebrities,Chinese microblog,Classification,Classification algorithms,Clustering,Cognitions,Cognitive Processes,Collective mood,Communication,Communication Systems,Communication and the arts,Communities,Community Networks,Computer Softwa,Computer mediation,Computer science,Computer security,Content spam,Contracts and Reputat,Cooperation,Credibility,Customer relations,Data analysis,Data mining,Data sampling,Data stream,Deception,Decision trees,Detection,Distributed detection,Document clustering,Dyads,Dynamic topic analysis,E-business,EBAY,ELECTRONIC commerce,EMV,Electronic commerce,Electronic mail systems,Emotions,Empirical Study,Employee Attitudes,Employee Interaction,Employees,Employers,Environmental Effects,Environmental Stress,Evolution,Experimental/theoretical,Experimental/theoretical treatment,FALSE advertising,Facebook,Fake reviews,Fake user accounts,Familiarity,Feature extraction,Feature vectors,Feature weighting,Feedback,Female,Filtering,First,Fraud,General Psychology,Genetic algorithms,Germany,Graph centrality,Graph theory,Group {\&} Interpersonal Processes,HIERARCHICAL DIRICHLET PROCESSES,HIGH technology,Heuristic,Hierarchical Dirichlet Process,Human,Humans,INFORMATION resources,INFORMATION resources management,INFORMATION technology,INTERNET advertising,INTERNET marketing,INTERNET users,Image,Immunological,Inference,Information Systems,Information and Internet Services,Information retrieval,Internet,Interpersonal Communication,Interpersonal Influences,Interpersonal Relations,Interpersonal communication,J4.8 algorithm,J48,Katz score,Laplace,Law,LinkedIn,Logistics,Lying,MARKETING,METABOLIC NETWORKS,MODEL,MTIO,Machine Learning,Machine learning,Machine learning algorithms,Male,Malicious users,Malware,Management,Management Personnel,Managers,Mass Media,Mass Media Communications,Models,Monte Carlo simulation,More,Motivation,Natural language processing,Negative selection,New employees,Non-malicious users,Non-parametric topic model,Northern America,ORGANIZATION,Occupational psychology,Okapi BM25,Online Social Networks (OSNs),Online privacy,Online social networks,Ontologies,Ontology,Opinion spam,Organizational Behavior,Organizational behavior,PSYCHOLOGY,Phishing websites,Poisson,Prestige,Probabilistic logic,Problem Solving,Professional Personnel,Professional relationships,Promoter,Proximity,Psychological Theory,Psychological aspects,Psychology: Professional {\&} Research,Public relations,Quantitative Study,RESEARCH,Random variables,Reciprocity,Regression analysis,Reputation,Reputation management,Reputations,Research and Development: Ge,Retail and Wholesale Trade,Retailing,Retailing industry,Review spam,Review spammer detection,SCALE-FREE NETWORKS,SERVICE industries -- Marketing,SMALL-WORLD NETWORKS,SNS features,SVM,Sales,Sales {\&} selling,Second,Semantics,Sentiment analysis,Sentiment tracking,Servers,Service level agreements,Sina Weibo,Social Media,Social Perception,Social Perception {\&} Cognition,Social Sciences,Social Sciences: trends,Social Support,Social aspects,Social conditions {\&} trends,Social media,Social network,Social network analysis,Social network security,Social network service,Social network services,Social networking sites,Social networks,Social research,Social sciences,Social trends {\&} culture,Socialbots,Spam,Spam campaign identification,Spam detection,Spam detection techniques,Spam profile identification,Spamdexing,Spamming,Statistical natural language processing,Stochastic gradient descent,Stock market,Studies,Support vector machines,Survey,Sybil account,Sybil accounts detection,TEXT,TSD,Technological Change,Technology,Telecommunication,Telecommunications systems {\&} Internet communicatio,Text mining,Theory,Topic sentiment analysis,Training,Transactional Relationships,Trust,Trust (Social Behavior),Trust Scale,Truth bias,Twitter,Twitter API,Twitter spam,U 2000,U.S.,URL rate,US,USpam,United States,Unsolicited electronic mail,User Centered Design,User Generated Content,User-generated content,Vectors,Virtual worlds,Visual Perception,WEB,WEB sites,Web 2.0,Web spam,Web spamming prevention techniques,Weibo spam detection method,Willingness to pay,YouTube,[[mkup]]Deception[[mkup]],accounting fraud,ad fraud,ad networks,adaptive deception,adaptive learning paths,affect {\&} cognitions in development of interpersona,affective based trust,amazon,annotations,anomaly detection,ant colony optimization,application,application program interfaces,approximate set similarity,article,artificial neural network,auction,automated Turing tests,automated deception detection,bayesian,bayesian parameter estimation,belief networks,belief propagation,binomial law,bipartite cores,blogs,book,bookitem,card not present,classification,classification method,click spam detection,cliques enumeration,cmc,coalition fraud attacks,coevolutionary algorithms,cognitive based trust,communication technology,computational model,computer network security,consistency of visible {\&} audible characteristics,content-based spam detection frameworks,cooperative behavior,credibility,credit card,crowdsourced manipulation,crowdsourcing,crowdsourcing site,data mining,data sampling,dblp,deceptive review,deep belief network,defection,detecting deception,detection,discrepancy-arousal hypothesis,distrust,document length normalization,e-Commerce (L810),e-business,e-commerce Web sites,e-mail,e-markets,eBay Inc,ecology,effective features,electronic markets,eliteness,email,emerging outbreak monitoring,emnlp2007,environmental psychology,evaluation,expertise location,expressive behavio,facebook,fake identities,fake product reviews,fake reviews,fake web review,feature extraction,file-import-08-07-28,financial fraud,framework,fraud,fraud detection,fraud evidence,fraud prevention,fuzzy logic,graph theory,graphical model,heterogenous networks,iSRD,identity,idf,imbalanced data distribution,imbalanced data distributions,information,information retrieval,information systems,interaction rate,internet advertising,invasive software,large margin classifiers,latent dirichlet allocation,lda,learning (artificial intelligence),learning methods,least-squares,logistic regression,long-surviving Twitter spam accounts,machine learning algorithm,machine learning classifiers,malware,markov random fields,media richness,mercer,meta-memory,metadata,microblog,microblogging social network,microblogging spam detection,multi-instance learning,multiple motives of deception as strategic communi,naive Bayes,on-line intrusion analysis,online advertising,online communities,online customer service,online fraud,online learning,online markets,online shopping process,ontologies (artificial intelligence),opinion mining,opinion spam,opinion spam detection,outlier detection,parisian approach,pattern classification,pattern discovery,penalty distance,perceptrons,plagiarism,plda,post,privacy,probabilistic models,product reviews,promoter,pvnets,randomness,real data experiments,regression,regularity,reputation management,reputation mechanisms,reputation systems,resource,response,retail,retail data processing,retreival,review analysis,review helpfulness,review site,review spam,review spammer,review utility,reviewer behavior,reward distance,scalable algorithms,security,self-nonself space pattern construction,self-similarity,semi-supervised learning,sentiment,sentiment analysis,service quality,shilling,signaling,similarity-sensitive sampling,sir,smoothing,social honeypots,social influence,social media,social media spam,social network security,social networking (online),social networking Websites,social networking services,social networks,social spam,social spam discovery,social-networks,social-spam,spam,spam detection,spam message detection,spam review detection,spammer,spammer identification,spamming behavior,spectrum,splog detection,stepwise,succession law,supervised classification methods,support vector machine,support vector machines,supporting services,supporting services functionality,sweethearting,tag,tag similarity,teiresias,temporal dynamics,term frequency normalization,term weighting,text categorization,text classification,topic model,topic modeling,topology,transactive memory,trust,trust management,trust propagation,trusted computing,twitter,unexpected patterns,unsolicited e-mail,user accounts,user centric ontology,user-generated content,video,video promotion,video response,video spam,web 2.0,web forum,web of trust,web security,web spam detection,wide scale proliferation},
number = {3},
pages = {993--1022},
pmid = {21362469},
title = {{Latent dirichlet allocation}},
url = {http://www.sciencedirect.com/science/article/pii/S0140366413001047$\backslash$nhttp://ceas.cc/2004/167.pdf$\backslash$nhttp://doi.acm.org/10.1145/1806338.1806450$\backslash$nhttp://eprints.soton.ac.uk/272254/$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7033160$\backslash$nhttp://},
volume = {3},
year = {2010}
}
@article{Hockenmaier2014,
author = {Hockenmaier, Julia},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/CVPR14TutorialHockenmaier2.pdf:pdf},
journal = {Cvpr2014},
title = {{Describing Images in Natural Language Part II CVPR tutorial}},
year = {2014}
}
@article{,
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/beiaardcantus G/E-Ticket{\_}48787.pdf:pdf},
title = {吀 䤀 䌀 䬀 䔀 吀 䈀 䔀 䤀 䄀 䄀 刀 䐀 䌀 䄀 一 吀 唀 匀 吀 䤀 䌀 䬀 䔀 吀 䈀 䔀 䤀 䄀 䄀 刀 䐀 䌀 䄀 一 吀 唀 匀},
year = {2016}
}
@article{Miller1990,
abstract = {WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets. © 1990 Oxford University Press.},
author = {Miller, George A. and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine J.},
doi = {10.1093/ijl/3.4.235},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/wordnet.pdf:pdf},
isbn = {0950384614774577},
issn = {09503846},
journal = {International Journal of Lexicography},
number = {4},
pages = {235--244},
pmid = {15102489},
title = {{Introduction to wordnet: An on-line lexical database}},
volume = {3},
year = {1990}
}
@article{Kataoka,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.07627v1},
author = {Kataoka, Hirokatsu and Iwata, Kenji and Satoh, Yutaka},
eprint = {arXiv:1509.07627v1},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/cnncompare.pdf:pdf},
pages = {4--8},
title = {{Recognition and Detection}}
}
@article{Gupta2012,
author = {Gupta, Ankush and Verma, Yashaswi and Jawahar, C. V.},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/5039-22368-1-PB.pdf:pdf},
isbn = {9781577355687},
journal = {Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI-12)},
keywords = {Knowledge-Based Information Systems (Main Track)},
number = {July},
pages = {606--612},
title = {{Choosing Linguistics over Vision to Describe Images}},
year = {2012}
}
@article{Mason2014,
author = {Mason, Rebecca and Charniak, Eugene},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/364{\_}Paper.pdf:pdf},
isbn = {9781937284732},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
pages = {592--598},
title = {{Nonparametric Method for Data-driven Image Captioning}},
url = {http://www.aclweb.org/anthology/P/P14/P14-2097},
year = {2014}
}
@article{Mitchell2012,
abstract = {This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator ﬁlters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.},
author = {Mitchell, Margaret and Dodge, Jesse and Goyal, Amit and Yamaguchi, Kota and Stratos, Karl and Mensch, Alyssa and Berg, Alex and Han, Xufeng and Berg, Tamara and Health, Oregon},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/EACL12.pdf:pdf},
isbn = {978-1-937284-19-0},
journal = {Eacl},
pages = {747--756},
title = {{Midge: Generating Image Descriptions From Computer Vision Detections}},
year = {2012}
}
@article{Li2011,
abstract = {Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effec- tive approach to automatically compose im- age descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or re- trieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the de- scription – making for more human-like anno- tations than previous approaches.},
author = {Li, Siming and Kulkarni, Girish and Berg, TL and Berg, AC and Choi, Yejin},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/ngram{\_}desc.pdf:pdf},
isbn = {978-1-932432-92-3},
journal = {Conference on Computational Natural Language Learning. Association for Computational Linguistics},
number = {June},
pages = {220--228},
title = {{Composing simple image descriptions using web-scale n-grams}},
url = {http://dl.acm.org/citation.cfm?id=2018962},
year = {2011}
}
@article{Patterson2014,
abstract = {In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the 'SUN attribute database' on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes. [ABSTRACT FROM AUTHOR]},
author = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
doi = {10.1007/s11263-013-0695-z},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/SUN{\_}Attribute{\_}Database-Patterson{\_}et{\_}al (1).pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Attributes,Crowdsourcing,Image captioning,Scene parsing,Scene understanding},
number = {1-2},
pages = {59--81},
title = {{The SUN attribute database: Beyond categories for deeper scene understanding}},
volume = {108},
year = {2014}
}
@article{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/ICCV.2011.6126456},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1409.4842v1.pdf:pdf},
isbn = {9781467369640},
issn = {1550-5499},
journal = {arXiv preprint arXiv:1409.4842},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842v1},
year = {2014}
}
@article{Kuznetsova2012,
abstract = {We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web. More speciﬁcally, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our ﬁnal system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines.},
author = {Kuznetsova, Polina and Ordonez, Vicente and Berg, Ac},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/acl{\_}12.pdf:pdf},
isbn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {359--368},
title = {{Collective generation of natural image descriptions}},
url = {http://dl.acm.org/citation.cfm?id=2390575},
volume = {1},
year = {2012}
}
@article{Ciresan2012,
abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.2745v1},
author = {Ciresan, D},
doi = {10.1109/CVPR.2012.6248110},
eprint = {arXiv:1202.2745v1},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/IDSIA-04-12.pdf:pdf},
isbn = {9781467312288},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition (CVPR)},
keywords = {Benchmark testing,Computer architecture,Error analysis,Graphics processing unit,Neurons,Training},
pages = {3642--3649},
pmid = {18225950},
title = {{Multi-column Deep Neural Networks for Image Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6248110},
year = {2012}
}
@article{Elliott2013,
author = {Elliott, Desmond and Keller, Frank},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/D13-1128.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP'13)},
number = {October},
pages = {1292--1302},
title = {{Image Description using Visual Dependency Representations}},
year = {2013}
}
@article{Zhou2015,
abstract = {Face recognition performance improves rapidly with the recent deep learning technique developing and underlying large training dataset accumulating. In this paper, we re- port our observations on how big data impacts the recog- nition performance. According to these observations, we build our Megvii Face Recognition System, which achieves 99.50{\%} accuracy on the LFW benchmark, outperforming the previous state-of-the-art. Furthermore, we report the performance in a real-world security certification scenario. There still exists a clear gap between machine recognition and human performance. We summarize our experiments and present three challenges lying ahead in recent face recognition. And we indicate several possible solutions to- wards these challenges. We hope our work will stimulate the community’s discussion of the difference between research benchmark and real-world applications.},
archivePrefix = {arXiv},
arxivId = {1501.0469},
author = {Zhou, Erjin and Cao, Zhimin and Yin, Qi},
doi = {10.1103/PhysRevD.91.045023},
eprint = {1501.0469},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1501.04690v1.pdf:pdf},
journal = {arXiv preprint arXiv:1501.04690},
title = {{Naive-Deep face Recognition: Touching the Limit of LFW Benchmark or Not?}},
year = {2015}
}
@article{Rampf2015,
abstract = {Very. Indeed, it is shown here that in a flat, cold dark matter (CDM) dominated Universe with positive cosmological constant ({\$}\backslashLambda{\$}), modelled in terms of a Newtonian and collisionless fluid, particle trajectories are analytical in time (representable by a convergent Taylor series) until at least a finite time after decoupling. The time variable used for this statement is the cosmic scale factor, i.e., the "{\$}a{\$}-time", and not the cosmic time. For this, a Lagrangian-coordinates formulation of the Euler-Poisson equations is employed, originally used by Cauchy for 3-D incompressible flow. Temporal analyticity for {\$}\backslashLambda{\$}CDM is found to be a consequence of novel explicit all-order recursion relations for the {\$}a{\$}-time Taylor coefficients of the Lagrangian displacement field, from which we derive the convergence of the {\$}a{\$}-time Taylor series. A lower bound for the {\$}a{\$}-time where analyticity is guaranteed and shell-crossing is ruled out is obtained, whose value depends only on {\$}\backslashLambda{\$} and on the initial spatial smoothness of the density field. The largest time interval is achieved when {\$}\backslashLambda{\$} vanishes, i.e., for an Einstein-de Sitter universe. Analyticity holds also if, instead of the {\$}a{\$}-time, one uses the linear structure growth {\$}D{\$}-time, but no simple recursion relations are then obtained. The analyticity result also holds when a curvature term is included in the Friedmann equation for the background, but inclusion of a radiation term arising from the primordial era spoils analyticity.},
archivePrefix = {arXiv},
arxivId = {1504.0032},
author = {Rampf, Cornelius and Villone, Barbara and Frisch, Uriel},
eprint = {1504.0032},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/coco{\_}caption{\_}generation.pdf:pdf},
pages = {16},
title = {{Microsoft COCO Captions: Data Collection and Evaluation Server}},
url = {http://arxiv.org/abs/1504.0032},
year = {2015}
}
@article{Koehn2006,
abstract = {We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.},
author = {Koehn, Philipp and Shen, Wade and Federico, Marcello and Bertoldi, Nicola and Callison-Burch, Chris and Cowan, Brooke and Dyer, Chris and Hoang, Hieu and Bojar, Ondrej and Zens, Richard and Constantin, Alexandra and Herbst, Evan and Moran, Christine},
doi = {10.3115/1557769.1557821},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/acl2007-moses.pdf:pdf},
journal = {Proceedings of ACL},
number = {June},
pages = {177--180},
title = {{Open Source Toolkit for Statistical Machine Translation}},
year = {2006}
}
@article{Busactie2016,
author = {Busactie, Sziget},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/SZIGET/order45600904.pdf:pdf},
isbn = {5155077415228},
pages = {10--17},
title = {{Sziget Festival 2016}},
year = {2016}
}
@article{Denkowski2007a,
author = {Denkowski, Michael and Lavie, Alon},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/meteor-wmt11.pdf:pdf},
title = {{Meteor 1 . 3 : Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems}},
year = {2007}
}
@article{Bilenko2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01538v1},
author = {Bilenko, Natalia Y and Gallant, Jack L},
eprint = {arXiv:1503.01538v1},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/PyRCCA.pdf:pdf},
pages = {1--13},
title = {{Pyrcca : regularized kernel canonical correlation analysis in Python and its applications to neuroimaging .}},
year = {2015}
}
@article{Chen2014a,
author = {Chen, Boxing and Cherry, Colin and Canada, Council},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/W14-3346.pdf:pdf},
number = {2},
pages = {362--367},
title = {{A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU Boxing Chen and Colin Cherry}},
year = {2014}
}
@article{Ranzato2008,
abstract = {Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have cer- tain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the repre- sentation. We describe a novel and efficient algorithm to learn sparse represen- tations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order},
author = {Ranzato, Marc'aurelio and Boureau, Y-lan and Cun, Yann L.},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/3363-sparse-feature-learning-for-deep-belief-networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
number = {Mcmc},
pages = {1185--1192},
title = {{Sparse Feature Learning for Deep Belief Networks}},
url = {http://papers.nips.cc/paper/3363-sparse-feature-learning-for-deep-belief-networks},
year = {2008}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/nature14539.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539$\backslash$n10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Young2014,
abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.},
author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/TACLDenotationGraph.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
number = {April},
pages = {67--78},
title = {{From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions}},
url = {http://nlp.cs.illinois.edu/HockenmaierGroup/Papers/DenotationGraph.pdf},
volume = {2},
year = {2014}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
eprint = {1405.0312},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1405.0312v3.pdf:pdf},
journal = {Computer Vision {\{}-{\}} ECCV 2014},
title = {{Microsoft COCO: Common Objects in Context}},
url = {http://arxiv.org/abs/1405.0312},
year = {2014}
}
@article{Blei2012,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/BleiNgJordan2003.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.cs.princeton.edu/{~}blei/lda-c/$\backslash$npapers2://publication/doi/10.1162/jmlr.2003.3.4-5.993$\backslash$npapers2://publication/uuid/4001D0D9-4F9C-4D8F-AE49-46ED6A224F4A$\backslash$npapers2://publication/uuid/7D10D5DA-B421-4D94-A3ED-028107B7F9B6$\backslash$nhttp://www.crossref.org/jmlr},
volume = {3},
year = {2003}
}
@article{LeCun2010,
abstract = {Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or "features")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.},
author = {LeCun, Yann and Kavukcuoglu, Koray and Farabet, Cl{\'{e}}ment},
doi = {10.1109/ISCAS.2010.5537907},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/Convolutional networks and applications in vision{\_}0.pdf:pdf},
isbn = {9781424453085},
issn = {02714302},
journal = {ISCAS 2010 - 2010 IEEE International Symposium on Circuits and Systems: Nano-Bio Circuit Fabrics and Systems},
pages = {253--256},
title = {{Convolutional networks and applications in vision}},
year = {2010}
}
@article{Johnson2015,
abstract = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
archivePrefix = {arXiv},
arxivId = {1511.07571},
author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
eprint = {1511.07571},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1511.07571v1.pdf:pdf},
title = {{DenseCap: Fully Convolutional Localization Networks for Dense Captioning}},
url = {http://arxiv.org/abs/1511.07571},
year = {2015}
}
@article{Fernando2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.04942v1},
author = {Jia, Xu and Gavves, Efstratios and Fernando, Basura and Tuytelaars, Tinne},
eprint = {arXiv:1509.04942v1},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1509.04942v1.pdf:pdf},
journal = {arXiv preprint arXiv:1509.04942},
title = {{Guiding Long-Short Term Memory for Image Caption Generation}},
year = {2015}
}
@article{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
doi = {10.1.1.145.8736},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/07.rah.rec.nips.pdf:pdf},
isbn = {160560352X},
journal = {Advances in neural information {\ldots}},
number = {1},
pages = {1--8},
title = {{Random features for large-scale kernel machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}833.pdf},
year = {2007}
}
@article{Girshick2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08083v2},
author = {Girshick, Ross},
eprint = {arXiv:1504.08083v2},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1504.08083v2.pdf:pdf},
journal = {arXiv},
title = {{Fast R-CNN}},
year = {2015}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster{\_}rcnn.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
eprint = {1506.01497},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/1506.01497.pdf:pdf},
pages = {1--10},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
year = {2015}
}
@article{Farhadi2010,
abstract = {Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.},
author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
doi = {10.1007/978-3-642-15561-1{\_}2},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/chp{\%}3A10.1007{\%}2F978-3-642-15561-1{\_}2.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
journal = {Computer Vision ECCV 2010.},
pages = {15--29},
title = {{Every picture tells a story: Generating sentences from images}},
year = {2010}
}
@article{Yang2011,
abstract = {We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi- gaword corpus to obtain their estimates; to- gether with probabilities of co-located nouns, scenes and prepositions. We use these esti- mates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image de- tections as the emissions. Experimental re- sults show that our strategy of combining vi- sion and language produces readable and de- scriptive sentences compared to naive strate- gies that use vision alone.},
author = {Yang, Yezhou and Teo, Ching Lik and Daume, Hal and Aloimonos, Yiannis},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/sengen{\_}emnlp2011{\_}final.pdf:pdf},
isbn = {1937284115},
journal = {Proceedings of EMNLP},
pages = {444--454},
title = {{Corpus-Guided Sentence Generation of Natural Images}},
year = {2011}
}
@article{Felzenszwalb2008,
author = {Felzenszwalb, Pedro and McAllester, David and Ramanan, Deva},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/DPMCVPR98.pdf:pdf},
isbn = {9781424422432},
journal = {Computer Vision and Pattern Recognition},
title = {{A Discriminatively Trained, Multiscaled, Deformable Part Model}},
year = {2008}
}
@article{Russakovsky2014,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/art{\%}3A10.1007{\%}2Fs11263-015-0816-y.pdf:pdf},
isbn = {0920-5691},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {benchmark,dataset,large-scale,object detection,object recognition},
number = {3},
pages = {211--252},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://arxiv.org/abs/1409.0575},
volume = {115},
year = {2014}
}
@article{Oliva2006,
abstract = {Humans can recognize the gist of a novel image in a single glance, independent of its complexity. How is this remarkable feat accomplished? On the basis of behavioral and computational evidence, this paper describes a formal approach to the representation and the mechanism of scene gist understanding, based on scene-centered, rather than object-centered primitives. We show that the structure of a scene image can be estimated by the mean of global image features, providing a statistical summary of the spatial layout properties (Spatial Envelope representation) of the scene. Global features are based on configurations of spatial scales and are estimated without invoking segmentation or grouping operations. The scene-centered approach is not an alternative to local image analysis but would serve as a feed-forward and parallel pathway of visual processing, able to quickly constrain local feature analysis and enhance object recognition in cluttered natural scenes. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Oliva, Aude and Torralba, Antonio},
doi = {10.1016/S0079-6123(06)55002-2},
file = {:C$\backslash$:/Users/spijs/Dropbox/PDF/OlivaTorralbaPBR2006.pdf:pdf},
isbn = {1617258865},
issn = {00796123},
journal = {Progress in Brain Research},
keywords = {gist,global image feature,natural image,scene recognition,spatial envelope,spatial frequency},
pages = {23--36},
pmid = {17027377},
title = {{Chapter 2 Building the gist of a scene: the role of global image features in recognition}},
volume = {155 B},
year = {2006}
}
@article{Le2015,
author = {Le, Quoc V},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le - 2015 - A Tutorial on Deep Learning Part 2 Autoencoders , Convolutional Neural Networks and Recurrent Neural Networks.pdf:pdf},
pages = {1--19},
title = {{A Tutorial on Deep Learning Part 2 : Autoencoders , Convolutional Neural Networks and Recurrent Neural Networks}},
year = {2015}
}
@article{Ou2007,
author = {Ou, Guobin and Murphey, Yi Lu},
doi = {10.1016/j.patcog.2006.04.041},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ou, Murphey - 2007 - Multi-class pattern classification using neural networks.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {machine learning,multi-class classification,neural networks,pattern recognition},
number = {1},
pages = {4--18},
title = {{Multi-class pattern classification using neural networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320306002081},
volume = {40},
year = {2007}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Le2014,
author = {Le, Quoc V},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le - 2014 - A Tutorial on Deep Learning Part 1 Nonlinear Classifiers and The Backpropagation Algorithm.pdf:pdf},
pages = {1--18},
title = {{A Tutorial on Deep Learning Part 1: Nonlinear Classifiers and The Backpropagation Algorithm}},
year = {2014}
}
@article{Gong2014,
abstract = {This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of im- ages that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description.},
author = {Gong, Yunchao and Wang, Liwei and Hodosh, Micah and Hockenmaier, Julia},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gong et al. - 2014 - Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections.pdf:pdf},
journal = {Computer Vision - ECCV},
pages = {529--545},
title = {{Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections}},
year = {2014}
}
@article{Mao2015,
abstract = {In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/{\~{}}junhua.mao/projects/child{\_}learning.html},
archivePrefix = {arXiv},
arxivId = {1504.06692},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
eprint = {1504.06692},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2015 - Learning like a Child Fast Novel Visual Concept Learning from Sentence Descriptions of Images.pdf:pdf},
title = {{Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images}},
url = {http://arxiv.org/abs/1504.06692},
year = {2015}
}
@article{Zhang,
archivePrefix = {arXiv},
arxivId = {arXiv:1510.02693v1},
author = {Zhang, Shiliang and Jiang, Hui and Wei, Si and Dai, Lirong},
eprint = {arXiv:1510.02693v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Feedforward Sequential Memory Neural Networks without Recurrent Feedback.pdf:pdf},
title = {{Feedforward Sequential Memory Neural Networks without Recurrent Feedback}},
year = {2015}
}
@article{Mao2014,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/{\~{}}junhua.mao/m-RNN.html .},
archivePrefix = {arXiv},
arxivId = {1412.6632},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
eprint = {1412.6632},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).pdf:pdf},
journal = {arXiv preprint arXiv:1412.6632},
title = {{Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)}},
url = {http://arxiv.org/abs/1412.6632},
year = {2014}
}
@article{Elliott2014,
author = {Elliott, Desmond and Keller, Frank},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elliott, Keller - 2014 - Comparing Automatic Evaluation Measures for Image Description.pdf:pdf},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
pages = {452--457},
title = {{Comparing Automatic Evaluation Measures for Image Description}},
url = {http://www.aclweb.org/anthology/P/P14/P14-2074},
year = {2014}
}
@article{Bernardi,
annote = {PRINTED},
author = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernardi et al. - Unknown - Automatic Image Description Systems A Survey.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
number = {55},
pages = {409--442},
title = {{Automatic Image Description Systems : A Survey}},
year = {2016}
}
@article{Goldberg2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1510.00726v1},
author = {Goldberg, Yoav},
eprint = {arXiv:1510.00726v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldberg - 2015 - A Primer on Neural Network Models for Natural Language Processing.pdf:pdf},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
year = {2015}
}
@article{Chen2014,
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.5654v1},
author = {Chen, Xinlei and Zitnick, C Lawrence},
eprint = {arXiv:1411.5654v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Zitnick - 2014 - Learning a Recurrent Visual Representation for Image Caption Generation.pdf:pdf},
journal = {Proceedings of CoRR},
title = {{Learning a Recurrent Visual Representation for Image Caption Generation}},
year = {2014}
}
@article{Liang2015,
abstract = {각각의 convolution layer가 recurrent connection 되어 있는 recurrent CNN을 통해 object recognition 수행},
author = {Liang, Ming and Hu, Xiaolin},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Hu - 2015 - Recurrent Convolutional Neural Network for Object Recognition.pdf:pdf},
journal = {Cvpr},
number = {Figure 1},
title = {{Recurrent Convolutional Neural Network for Object Recognition}},
year = {2015}
}
@article{Mitchell2015,
abstract = {Image descriptor(caption)을 automatically generating 하는 방법 소개 visual detector, language models, multimodal similarity model을 이미지 dataset caption으로부터 학습 language model을 학습하기 위해 400,000 이상의 image descriptor를 사용},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {1411.4952},
author = {Mitchell, Margaret and Doll, Piotr and Iandola, Forrest and Gao, Jianfeng and Zitnick, C Lawrence},
eprint = {1411.4952},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell et al. - 2015 - From Captions to Visual Concepts and Back.pdf:pdf},
journal = {Cvpr},
title = {{From Captions to Visual Concepts and Back}},
year = {2015}
}
@article{Devlin2015,
abstract = {Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.},
archivePrefix = {arXiv},
arxivId = {1505.01809},
author = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
eprint = {1505.01809},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2015 - Language Models for Image Captioning The Quirks and What Works.pdf:pdf},
number = {Me Lm},
title = {{Language Models for Image Captioning: The Quirks and What Works}},
url = {http://arxiv.org/abs/1505.01809},
year = {2015}
}
@article{Kiros2014,
abstract = {Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2539v1},
author = {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S},
eprint = {arXiv:1411.2539v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Salakhutdinov, Zemel - 2014 - Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.pdf:pdf},
journal = {arXiv preprint arXiv:1411.2539},
title = {{Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models}},
year = {2014}
}
@article{Zhang2005,
abstract = {Abstract A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola -Jones detector cascade and training it with a new variant of ... $\backslash$n},
author = {Zhang, C and Platt, J C and Viola, P a},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Platt, Viola - 2005 - Multiple instance boosting for object detection.pdf:pdf},
isbn = {1049-5258},
issn = {10495258},
journal = {Neural Information Processing Systems},
number = {10},
pages = {1769--1775},
title = {{Multiple instance boosting for object detection}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231211001299$\backslash$npapers3://publication/doi/10.1016/j.neucom.2011.02.011},
volume = {74},
year = {2005}
}
@article{Devlin2015a,
abstract = {We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the "consensus" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach.},
archivePrefix = {arXiv},
arxivId = {1505.04467},
author = {Devlin, Jacob and Gupta, Saurabh and Girshick, Ross and Mitchell, Margaret and Zitnick, C. Lawrence},
eprint = {1505.04467},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2015 - Exploring Nearest Neighbor Approaches for Image Captioning.pdf:pdf},
journal = {arXiv preprint},
title = {{Exploring Nearest Neighbor Approaches for Image Captioning}},
url = {http://arxiv.org/abs/1505.04467},
year = {2015}
}
@article{Guerrero2015,
abstract = {As humans, we regularly interpret images based on the relations between image regions. For example, a person riding object X, or a plank bridging two objects. Current methods provide limited support to search for images based on such relations. We present RAID, a relation-augmented image descriptor that supports queries based on inter-region relations. The key idea of our descriptor is to capture the spatial distribution of simple point-to-region relationships to describe more complex relationships between two image regions. We evaluate the proposed descriptor by querying into a large subset of the Microsoft COCO database and successfully extract nontrivial images demonstrating complex inter-region relations, which are easily missed or erroneously classified by existing methods.},
archivePrefix = {arXiv},
arxivId = {1510.01113},
author = {Guerrero, Paul and Mitra, Niloy J. and Wonka, Peter},
eprint = {1510.01113},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guerrero, Mitra, Wonka - 2015 - RAID A Relation-Augmented Image Descriptor.pdf:pdf},
keywords = {a relation-augmented image de-,based on inter-segment relations,image descriptors,image retrieval,in this paper,query,relation-based,scriptor that supports queries,spatial relationships,we,we present raid as},
title = {{RAID: A Relation-Augmented Image Descriptor}},
url = {http://arxiv.org/abs/1510.01113},
year = {2015}
}
@article{Pan2015,
abstract = {The prediction of saliency areas in images has been traditionally addressed with hand crafted features based on neuroscience principles. This paper however addresses the problem with a completely data-driven approach by training a convolutional network. The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train a not very deep architecture which is both fast and accurate. The convolutional network in this paper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction with a superior performance in all considered metrics.},
archivePrefix = {arXiv},
arxivId = {1507.01422},
author = {Pan, Junting and Gir{\'{o}}-i-Nieto, Xavier},
eprint = {1507.01422},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Gir{\'{o}}-i-Nieto - 2015 - End-to-end Convolutional Network for Saliency Prediction.pdf:pdf},
pages = {1--6},
title = {{End-to-end Convolutional Network for Saliency Prediction}},
url = {http://arxiv.org/abs/1507.01422},
year = {2015}
}
@article{Donahue2015,
abstract = {Visual recognition을 할 때 sequence를 인식 작업에 포함시키면 더 좋은 성능을 낼 것이다. 따라서 Long term Recurrent Convolution(LTRC)를 통하여 다양한 visual sequence 정보를 포함시켜 결과를 추론함.},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {1411.4389},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor and Austin, U T and Lowell, Umass and Berkeley, U C},
eprint = {1411.4389},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue et al. - 2015 - Long-term Recurrent Convolutional Networks for Visual Recognition and Description.pdf:pdf},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Long-term Recurrent Convolutional Networks for Visual Recognition and Description}},
year = {2015}
}
@article{Cho2015,
abstract = {Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.},
archivePrefix = {arXiv},
arxivId = {1507.01053},
author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
eprint = {1507.01053},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Courville, Bengio - 2015 - Describing Multimedia Content using Attention-based Encoder--Decoder Networks.pdf:pdf},
pages = {1--12},
title = {{Describing Multimedia Content using Attention-based Encoder--Decoder Networks}},
url = {http://arxiv.org/abs/1507.01053},
year = {2015}
}
@article{Jin2015,
abstract = {Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of "abstract meaning", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1506.06272},
author = {Jin, Junqi and Fu, Kun and Cui, Runpeng and Sha, Fei and Zhang, Changshui},
eprint = {1506.06272},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2015 - Aligning where to see and what to tell image caption with region-based attention and scene factorization.pdf:pdf},
journal = {arXiv preprint arXiv:1506.06272},
title = {{Aligning where to see and what to tell: image caption with region-based attention and scene factorization}},
url = {http://arxiv.org/abs/1506.06272},
year = {2015}
}
@article{Plummer2015,
abstract = {The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging.},
archivePrefix = {arXiv},
arxivId = {1505.04870},
author = {Plummer, Bryan a. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
eprint = {1505.04870},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plummer et al. - 2015 - Flickr30k Entities Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.pdf:pdf},
title = {{Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}},
url = {http://arxiv.org/abs/1505.04870},
year = {2015}
}
@article{Liu2015,
abstract = {Recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large image dataset can be used as a universal image descriptor, and that doing so leads to impressive performance for a variety of image classification tasks. Most of these studies adopt activations from a single DCNN layer, usually the fully-connected layer, as the image representation. In this paper, we proposed a novel way to extract image representations from two consecutive convolutional layers: one layer is utilized for local feature extraction and the other serves as guidance to pool the extracted features. By taking different viewpoints of convolutional layers, we further develop two schemes to realize this idea. The first one directly uses convolutional layers from a DCNN. The second one applies the pretrained CNN on densely sampled image regions and treats the fully-connected activations of each image region as convolutional feature activations. We then train another convolutional layer on top of that as the pooling-guidance convolutional layer. By applying our method to three popular visual classification tasks, we find our first scheme tends to perform better on the applications which need strong discrimination on subtle object patterns within small regions while the latter excels in the cases that require discrimination on category-level patterns. Overall, the proposed method achieves superior performance over existing ways of extracting image representations from a DCNN.},
archivePrefix = {arXiv},
arxivId = {1510.00921},
author = {Liu, Lingqiao and Shen, Chunhua and Hengel, Anton Van Den},
eprint = {1510.00921},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Shen, Hengel - 2015 - Cross-convolutional-layer Pooling for Generic Visual Recognition.pdf:pdf},
number = {2},
pages = {1--7},
title = {{Cross-convolutional-layer Pooling for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1510.00921},
year = {2015}
}
@article{Elliott2015,
abstract = {The Visual Dependency Representation (VDR) is an explicit model of the spa-tial relationships between objects in an im-age. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given descrip-tion using a state-of-the-art object detec-tor, and to use successful detections to pro-duce training data. The description of an unseen image is produced by first predict-ing its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-of-the-art multimodal deep neural network in images depicting actions.},
author = {Elliott, Desmond and Vries, Arjen P De},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elliott, Vries - 2015 - Describing Images using Inferred Visual Dependency Representations.pdf:pdf},
journal = {Acl-2015},
pages = {42--52},
title = {{Describing Images using Inferred Visual Dependency Representations}},
year = {2015}
}
@article{Gilberto2015,
author = {Gilberto, Luis and Ortiz, Mateos and Wolff, Clemens and Lapata, Mirella},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gilberto et al. - 2015 - Learning to Interpret and Describe Abstract Scenes.pdf:pdf},
journal = {Naacl2015},
pages = {1505--1515},
title = {{Learning to Interpret and Describe Abstract Scenes}},
year = {2015}
}
@article{Pascanu2013,
abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.},
archivePrefix = {arXiv},
arxivId = {1312.6026},
author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1312.6026},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pascanu et al. - 2013 - How to Construct Deep Recurrent Neural Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6026},
pages = {1--10},
title = {{How to Construct Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1312.6026},
year = {2013}
}
@article{Lebret2013,
abstract = {Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some well-known embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.},
archivePrefix = {arXiv},
arxivId = {1312.5542},
author = {Lebret, R{\'{e}}mi and Collobert, Ronan},
eprint = {1312.5542},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lebret, Collobert - 2013 - Word Emdeddings through Hellinger PCA.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5542},
title = {{Word Emdeddings through Hellinger PCA}},
url = {http://arxiv.org/abs/1312.5542},
year = {2013}
}
@article{Socher2010,
abstract = {We propose a semi-supervised model which segments and annotates images using very few labeled images and a large unaligned text corpus to relate image regions to text labels. Given photos of a sports event, all that is necessary to provide a pixel-level labeling of objects and background is a set of newspaper articles about this sport and one to five labeled images. Our model is motivated by the observation that words in text corpora share certain context and feature similarities with visual objects. We describe images using visual words, a new region-based representation. The proposed model is based on kernelized canonical correlation analysis which finds a mapping between visual and textual words by projecting them into a latent meaning space. Kernels are derived from context and adjective features inside the respective visual and textual domains. We apply our method to a challenging dataset and rely on articles of the New York Times for textual features. Our model outperforms the state-of-the-art in annotation. In segmentation it compares favorably with other methods that use significantly more labeled training data.},
author = {Socher, Richard and Fei-Fei, Li},
doi = {10.1109/CVPR.2010.5540112},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Fei-Fei - 2010 - Connecting modalities Semi-supervised segmentation and annotation of images using unaligned text corpora.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {966--973},
pmid = {5540112},
title = {{Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora}},
year = {2010}
}
@article{Hodosh2013,
abstract = {The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated. © 2013 AI Access Foundation. All rights reserved.},
author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
doi = {10.1613/jair.3994},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodosh, Young, Hockenmaier - 2013 - Framing image description as a ranking task Data, models and evaluation metrics.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {853--899},
title = {{Framing image description as a ranking task: Data, models and evaluation metrics}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84883394520{\&}partnerID=tZOtx3y1},
volume = {47},
year = {2013}
}
@article{Fei-Fei2007,
abstract = {What do we see when we glance at a natural scene and how does it change as the glance becomes longer? We asked naive subjects to report in a free-form format what they saw when looking at briefly presented real-life photographs. Our subjects received no specific information as to the content of each stimulus. Thus, our paradigm differs from previous studies where subjects were cued before a picture was presented and/or were probed with multiple-choice questions. In the first stage, 90 novel grayscale photographs were foveally shown to a group of 22 native-English-speaking subjects. The presentation time was chosen at random from a set of seven possible times (from 27 to 500 ms). A perceptual mask followed each photograph immediately. After each presentation, subjects reported what they had just seen as completely and truthfully as possible. In the second stage, another group of naive individuals was instructed to score each of the descriptions produced by the subjects in the first stage. Individual scores were assigned to more than a hundred different attributes. We show that within a single glance, much object- and scene-level information is perceived by human subjects. The richness of our perception, though, seems asymmetrical. Subjects tend to have a propensity toward perceiving natural scenes as being outdoor rather than indoor. The reporting of sensory- or feature-level information of a scene (such as shading and shape) consistently precedes the reporting of the semantic-level information. But once subjects recognize more semantic-level components of a scene, there is little evidence suggesting any bias toward either scene-level or object-level recognition.},
author = {Fei-Fei, Li and Iyer, Asha and Koch, Christof and Perona, Pietro},
doi = {10.1167/7.1.10},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fei-Fei et al. - 2007 - What do we perceive in a glance of a real-world scene.pdf:pdf},
isbn = {1534-7362},
issn = {1534-7362},
journal = {Journal of Vision},
keywords = {entry level,event recognition,indoor,natural scene,object categorization,object recognition,outdoor,perception,real-world scene,scene categorization,segmentation,sensory-level perception,subordinate,superordinate},
number = {1},
pages = {1--29},
pmid = {17461678},
title = {{What do we perceive in a glance of a real-world scene?}},
url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/7.1.10},
volume = {7},
year = {2007}
}
@article{Zhao2015,
author = {Zhao, Rui and Li, Hongsheng and Wang, Xiaogang},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Li, Wang - 2015 - Saliency Detection by Multi-Context Deep Learning.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr 2015},
pages = {1265--1274},
title = {{Saliency Detection by Multi-Context Deep Learning}},
year = {2015}
}
@article{Arge2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arge, Mage - 2015 - V d c n l -s i r.pdf:pdf},
journal = {arXiv preprint arXiv:1409.1556},
title = {{Very Deep Convolutional Networks For Large-Scale Image Recognition}},
year = {2014}
}
@article{Lebret2015,
abstract = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03671},
author = {Lebret, R{\'{e}}mi and Pinheiro, Pedro O. and Collobert, Ronan},
eprint = {1502.03671},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lebret, Pinheiro, Collobert - 2015 - Phrase-based Image Captioning.pdf:pdf},
journal = {arXiv preprint arXiv:1502.03671},
title = {{Phrase-based Image Captioning}},
url = {http://arxiv.org/abs/1502.03671},
year = {2015}
}
@article{Kiros2013,
abstract = {Abstract We introduce two multimodal neural language models : models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase ... $\backslash$n},
author = {Kiros, R and Zemel, R and Salakhutdinov, R},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2013 - Multimodal Neural Language Models.pdf:pdf},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
keywords = {Image Tag Inference},
pages = {595--603},
title = {{Multimodal Neural Language Models}},
url = {http://www.cs.toronto.edu/{~}rkiros/papers/mnlm2014.pdf$\backslash$npapers3://publication/uuid/00AE85FB-98DC-4EB9-A6E8-A423C47C0B98},
year = {2014}
}
@article{Krizhevsky2012a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks(2).pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
annote = {PRINTED},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
eprint = {1502.03044},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
journal = {arXiv preprint arXiv:1502.03044},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v5},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves - 2013 - Generating sequences with recurrent neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Papineni2001,
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
doi = {10.3115/1073083.1073135},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Papineni et al. - 2001 - BLEU a Method for Automatic Evaluation of Machine Translation.pdf:pdf},
issn = {00134686},
journal = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL '02},
number = {July},
pages = {311--318},
title = {{BLEU: a Method for Automatic Evaluation of Machine Translation}},
url = {http://dl.acm.org/citation.cfm?id=1073083.1073135},
year = {2001}
}
@article{Socher2014,
abstract = {Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.},
author = {Socher, Richard and Karpathy, Andrej and Le, Quoc V and Manning, Christopher D and Ng, Andrew Y},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher et al. - 2014 - Grounded Compositional Semantics for Finding and Describing Images with Sentences.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
number = {April},
pages = {207--218},
title = {{Grounded Compositional Semantics for Finding and Describing Images with Sentences}},
url = {http://nlp.stanford.edu/{~}socherr/SocherLeManningNg{\_}nipsDeepWorkshop2013.pdf},
volume = {2},
year = {2014}
}
@article{Socher2011,
abstract = {Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1{\%}). The features from the image parse tree outperform Gist descriptors for scene classification by 4{\%}.},
author = {Socher, R and Lin, Cc},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Socher, Lin - 2011 - Parsing natural scenes and natural language with recursive neural networks.pdf:pdf},
isbn = {9781450306195},
issn = {<null>},
journal = {Proceedings of the {\ldots}},
pages = {129--136},
title = {{Parsing natural scenes and natural language with recursive neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/ICML2011Socher{\_}125.pdf},
year = {2011}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sermanet et al. - 2013 - OverFeat Integrated Recognition , Localization and Detection using Convolutional Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6229},
pages = {1--15},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Mao2014a,
abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.1090v1},
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L.},
eprint = {arXiv:1410.1090v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain Images with Multimodal Recurrent Neural Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1410.1090},
title = {{Explain Images with Multimodal Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1410.1090 http://www.arxiv.org/pdf/1410.1090.pdf},
year = {2014}
}
@article{Frome2013,
abstract = {Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their pre- dictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as seman- tic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recogni- tion challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18{\%} across thou- sands of novel labels never seen by the visual model.},
author = {Frome, Andrea and Corrado, Gs and Shlens, Jonathon},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frome, Corrado, Shlens - 2013 - Devise A deep visual-semantic embedding model.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural {\ldots}},
pages = {1--11},
title = {{Devise: A deep visual-semantic embedding model}},
url = {http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model},
year = {2013}
}
@article{Mikolov2010,
abstract = {基于RNN模型的语言模型，详细可参考作者的博士论文。周期神经网络。但是上下文，也没用取全部的，只取到了前5个。},
author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2010 - Recurrent Neural Network based Language Model.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1045--1048},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@article{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final mAP of 48{\%} on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
annote = {Gebruikt Caffe voor implemenatie},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr'14},
pages = {2--9},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@inproceedings{Yao2012,
abstract = {In this paper we propose an approach to holistic scene understanding that reasons jointly about regions, location, class and spatial extent of objects, presence of a class in the image, as well as the scene type. Learning and inference in our model are efficient as we reason at the segment level, and introduce auxiliary variables that allow us to decom- pose the inherent high-order potentials into pairwise poten- tials between a fewvariables with small number of states (at most the number of classes). Inference is done via a conver- gent message-passing algorithm, which, unlike graph-cuts inference, has no submodularity restrictions and does not require potential specific moves. We believe this is very im- portant, as it allows us to encode our ideas and prior knowl- edge about the problem without the need to change the in- ference engine every time we introduce a newpotential. Our approach outperforms the state-of-the-art on the MSRC-21 benchmark, while being much faster. Importantly, our holis- tic model is able to improve performance in all tasks.},
author = {Yao, Jian and Fidler, Sanja and Urtasun, Raquel},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247739},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao, Fidler, Urtasun - 2012 - Describing the scene as a whole Joint object detection, scene classification and semantic segmentation.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
pages = {702--709},
title = {{Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation}},
year = {2012}
}
@article{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wj},
doi = {10.3115/1073083.1073135},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation.pdf:pdf},
issn = {00134686},
journal = {{\ldots} of the 40Th Annual Meeting on {\ldots}},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@article{Schuster1997,
abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported View full abstract},
author = {Schuster, Mike and Paliwal, Kuldip K},
doi = {10.1109/78.650093},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schuster, Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:pdf},
issn = {1053-587X},
journal = {Signal Processing, IEEE Transactions on},
number = {11},
pages = {2673--2681},
title = {{Bidirectional recurrent neural networks}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=650093{\&}matchBoolean=true{\&}rowsPerPage=30{\&}searchField=Search{\_}All{\&}queryText=(p{\_}Title:"Bidirectional+Recurrent+Neural+Networks")$\backslash$npapers2://publication/doi/10.1109/78.650093},
volume = {45},
year = {1997}
}
@article{Ordonez2011,
abstract = {We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset – performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely chal- lenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods in- corporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective perfor- mance measure for image captioning.},
author = {Ordonez, V and Kulkarni, G and Berg, Tl},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ordonez, Kulkarni, Berg - 2011 - Im2text Describing images using 1 million captioned photographs.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems},
pages = {1143--1151},
title = {{Im2text: Describing images using 1 million captioned photographs}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2011{\_}0671.pdf},
year = {2011}
}
@article{SeppHochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sepp Hochreiter - 1997 - Long short-term memory.pdf:pdf},
journal = {Neural computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long short-term memory}},
volume = {9},
year = {1997}
}
@article{Le2014a,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Qv and Mikolov, Tomas},
eprint = {1405.4053},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
journal = {International Conference on Machine Learning - ICML 2014},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {3111--3119},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Jia2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1408.5093v1},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor and Eecs, U C Berkeley},
eprint = {arXiv:1408.5093v1},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:pdf},
isbn = {9781450330633},
journal = {ACM Conference on Multimedia},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
title = {{Caffe : Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@article{Google,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep re-current architecture that combines recent advances in com-puter vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target de-scription sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descrip-tions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4555v2},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
eprint = {arXiv:1411.4555v2},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Google et al. - Unknown - Show and Tell A Neural Image Caption Generator.pdf:pdf},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3156--3164},
title = {{Show and Tell: A Neural Image Caption Generator}},
year = {2015}
}
@article{Karpathy2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1503.08909v2},
author = {Karpathy, A. and Fei-Fei, Li},
eprint = {1503.08909v2},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Image Des.pdf:pdf},
journal = {Cvpr2015},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
year = {2015}
}
@article{Karpathy2014,
abstract = {We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.},
archivePrefix = {arXiv},
arxivId = {1406.5679},
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
eprint = {1406.5679},
file = {:C$\backslash$:/Users/spijs/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Joulin, Fei-Fei - 2014 - Deep Fragment Embeddings for Bidirectional Image Sentence Mapping.pdf:pdf},
journal = {Advances in neural information processing systems},
pages = {1889--1897},
title = {{Deep Fragment Embeddings for Bidirectional Image Sentence Mapping}},
url = {http://arxiv.org/abs/1406.5679},
year = {2014}
}
