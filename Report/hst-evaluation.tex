\chapter{Evaluatie}
\label{hoofdstuk:evaluatie}
Dit hoofdstuk behandelt enkele gebruikte methodes om de ontwikkelde systemen te evalueren. Twee van deze methodes vinden hun oorsprong in de computervertaling. Ze zijn ontwikkeld om automatisch de kwaliteit van vertalingen te meten. Het doel van deze evaluatiealgoritmes is om zo goed als mogelijk menselijke evaluatiescores te benaderen. De laatste vorm van evaluatie kijkt naar statistieken van de gegenereerde zinnen.


\section{Automatische evaluatie}
Om verschillende systemen te kunnen vergelijken, moet er een manier zijn om de gegenereerde zinnen te evalueren. Elke zin moet aan twee belangrijke voorwaarden voldoen. Enerzijds moet de inhoud van de zin overeenkomen met de inhoud van de afbeelding. Anderzijds moet de zin een grammaticaal correcte structuur hebben en voldoende vloeiend zijn.
Ideaal gezien beoordelen meerdere mensen elke zin om tot een betrouwbare evaluatie van beide eigenschappen te komen. Deze beoordeling kan bijvoorbeeld door een score te geven op de kwaliteit van elke zin. Een andere optie is om proefpersonen te laten oordelen of een automatisch gegenereerde zin beter, gelijkaardig of slechter is dan de overeenkomstige referentiezin. Het nadeel van elk van deze methodes is de nood aan meerdere proefpersonen die voor elke gebruikte methode of instelling van de netwerken, alle duizend zinnen van de validatieverzameling of de testverzameling moet beoordelen. Het is duidelijk dat dit een kostelijke operatie is. Daarvoor bieden automatische evaluatiealgoritmes een oplossing. Het nadeel van deze methodes is dat ze niet perfect overeenkomen met de menselijke evaluatie.

In wat volgt bespreken we eerst twee algoritmes uit de computervertaling, namelijk BLEU en METEOR. \todo{BLEU en meteor} Deze algoritmes zijn bruikbaar omdat de gegenereerde zinnen kunnen worden beschouwd als ``vertalingen'' van de afbeeldingen. Het is aangetoond dat de twee methodes correleren met menselijke evaluaties van gegenereerde zinnen. Van de gebruikte methodes heeft METEOR de hoogste correlatie. \todo{referentie elliot en keller}
Geen van beide algoritmes is echter perfect voor het meten van de kwaliteit.
Om die reden bekijken we ook nog enkele statistieken van de gegenereerde zinnen om hieruit andere interessante fenomenen af te leiden.
Als laatste onderdeel volgt een bespreking van een methode die de laatste twee jaar in onbruik is geraakt binnen dit onderzoeksdomein. Hierbij geeft een rangschikking van de afbeeldingen of zinnen een numerieke score.

\section{BLEU}
Kort gesteld berekent het BLEU-algoritme scores van computervertalingen op basis van overeenkomsten met de referentiezinnen. Deze overeenkomsten kunnen gaan over woorden of over aaneensluitingen van woorden. Verschillende vormen van BLEU kunnen worden gebruikt naargelang het aantal gebruikte n-grams \todo{deze zin is vaag, ma ik weet ni wa ge bedoelt dus ik kan het ni verbeteren}. Het gebruik van BLEU heeft echter wel enkele nadelen.

\subsection{Algoritme}
Om de n-gram BLEU-score van een zin te berekenen bepaalt het algoritme eerst de \textit{modified n-gram precision}. Dit is een vorm van precisie die rekening houdt met het aantal keer dat elk woord in de referentiezinnen voorkomt. Op deze manier krijgen zinnen als \texttt{the the the the the} een lage score omdat \texttt{the} nooit vijfmaal voorkomt in een referentiezin. 

De berekening van \textit{modified n-gram precision} gebeurt door eerst het maximum te nemen van het aantal keer dat een specifieke woordsequentie voorkomt in elke referentiezin. Vervolgens telt het algoritme het aantal voorkomens van deze sequentie in de gegenereerde zin. Het minimum van deze twee getallen ($Count_ {clip}$) wordt dan voor elk woord in de vertaalde zin opgeteld en gedeeld door het totaal aantal woorden in de gegenereerde zin.

\begin{equation}
p_{modified}(s) =
\frac{\sum\limits_{ngram \in s} Count_{clip}(ngram)}{\sum\limits_{ngram' \in s} Count(ngram')}
\label{formule:ngramprecision}
\end{equation}
Vanuit deze formule volgt een score voor een volledige corpus van gegenereerde zinnen als volgt:
\begin{equation}
p_{n} =
\frac{\sum\limits_{C \in \{Candidates\} } \sum\limits_{ngram \in C} Count_{clip}(ngram)}{\sum\limits_{C' \in \{Candidates\} } \sum\limits_{ngram' \in C'} Count(ngram')}
\label{formule:corpus_modified}
\end{equation}

Vanuit deze scores voor $i=1$ tot en met $n$ is het mogelijk om een n-gram BLEU-score te bepalen. Dit gebeurt door het gemiddelde logaritme te nemen met uniforme gewichten, wat overeenkomt met het geometrisch gemiddelde van de modified n-gram precisions. \todo{hier zou ik ook nog een formule bij zetten}

Deze score dwingt echter niet de juiste lengte van de zin af. Om die reden bepaalt de paper een extra multiplicatieve factor, namelijk de Brevity Penalty ($BP$). Voor elke gegenereerde zin bepaalt het algoritme de referentiezin met de dichtstbijzijnde lengte. De lengte daarvan noemt de paper de ``beste match lengte''. Vervolgens worden zowel de lengtes van de gegenereerde zinnen als de beste match lengte opgeteld tot respectievelijk $c$ en $r$. Berekenen van de Brevity Penalty kan dan met volgende formule:
\begin{equation}BP=
 \begin{cases}
1 & if c > r \\
e^{1-r/c} & else
\end{cases}
\end{equation}

De uiteindelijke BLEU-N score is dan gelijk aan:
\begin{equation}
BLEU = BP\cdot exp(\sum\limits_{n=1}^N w_nlog(p_n))
\end{equation}
Hierbij is $w_n$ gelijk aan $1/N$ wanneer het uniforme geometrisch gemiddelde wordt genomen.

\subsection{Nadelen en gebruik}
De BLEU-score is de meest gebruikte gebruikte evaluatiemethode in de literatuur. De meeste papers die afbeeldingsbeschrijvingen genereren bespreken BLEU-1 tot BLEU-4 scores. De literatuur lijkt het echter niet eens over het gebruik van de Brevity Penalty. Sommige papers vermelden expliciet dat ze het niet gebruiken, maar andere volgen de paper van Papinemi volledig. Door dit verschil in evaluatie is de vergelijking van verschillende systemen niet altijd eerlijk. Wanneer de gegenereerde zinnen lang genoeg zijn komen de scores wel overeen.
In onze experimenten stellen we de Brevity Penalty steeds gelijk aan 1.

Naast deze problemen zijn er verschillende implementaties met kleine onderlinge verschillen. In onze experimenten gebruiken we de \texttt{multi-BLEU.pl}\footnote{\url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}} code uit de Moses decoder\todo{cite acl2007-moses}.

In \todo{cite elliott} toont Elliot et al. aan dat BLEU slechts een matige correlatie heeft met menselijke beoordeling.
Hiervoor zijn meerdere verklaringen mogelijk. Zo kijkt BLEU enkel naar exacte woordmatches maar houdt het geen rekening mee met semantisch gelijkaardige woorden. Zo geeft bijvoorbeeld het gebruik van een synoniem geen hogere score terwijl dit bij menselijke evaluaties geen probleem is. Ook werkwoorden die semantische informatie bevatten die in de referentiezin als substantief voorkomen geven problemen. Voorbeelden van zulke situaties waarbij zinnen slecht scoren volgens BLEU, maar inhoudelijk toch hetzelfde weergeven:
\\

Reference 1: \texttt{Two boys are on their bikes.}

Candiate 1: \texttt{Two boys are on their bicycles.}

Reference 2: \texttt{A man is skiing down a hill.}

Candidate 2: \texttt{A man is going down a hill on his skis}
\\


METEOR is een evaluatiealgoritme met als doel het vermijden van dergelijke foute evaluaties.

\section{METEOR}
Elliott et al. \todo{cite} toonden aan dat zeker unigram BLEU (B1) slechts een zwakke correlatie heeft met menselijke evaluatie. Hogere orde BLEU-scores hebben slechts een matige correlatie. METEOR is een metriek die specifiek ontworpen is om de tekortkomingen van BLEU te verbeteren. Van de door Elliott onderzochte evaluatiemechanismen, correleert METEOR het beste met menselijke evaluatie.

\subsection{Algoritme}
Meteor \todo{cite meteor!} evalueert vertaalde zinnen door ze te aligneren met de referenties en hiervan per zin een score te berekenen. Bij het berekenen van de score wordt net zoals bij BLEU gekeken naar de precisie. Daarnaast heeft in contrast met BLEU ook de recall een invloed. De paper bevat een implementatie in de vorm van een JAR-bestand, zodat er geen twijfel bestaat over het gebruik van het algoritme.

Concreet probeert het algoritme twee zinnen te aligneren met behulp van vier matchers. Als eerste is er een match wanneer de woordvormen exact dezelfde zijn. Als tweede is er een match wanneer met een Snowball Stemmer (\todo{cite porter 2001} porter) ``gestemde'' woorden gelijk zijn. Een derde matcher kijkt naar overeenkomsten in de WordNet synoniemenlijst van elk woord. \todo{cite miller en felbaum} Als laatste vormen frases een match wanneer ze in zogenoemde frasetabellen \todo{mss voorbeeldje van ne frasetabel dabei} voorkomen.
Elke matcher heeft een bepaald gewicht. In de experimenten gebruiken we de standaardgewichten 0.85, 0.2 ,0.6 en 0.75.

Uiteindelijk worden alle matches gegeneraliseerd tot frase matches met een bepaalde startpositie en eindlengte. Een van de doelen van de Meteor score is om zoveel mogelijk woorden af te dekken in de twee zinnen. Daarnaast moet het aantal \textit{chunks} minimaal zijn. Denkowski et al. defini\"eren een chunk als aaneengesloten en identiek geordende matches tussen de twee zinnen. De uiteindelijke METEOR score bestaat dan uit de F-score vermenigvuldigd met een penalisatiefactor op basis van het aantal chunks.


\begin{equation}
Score = (1 - Pen)*F_{mean}
\end{equation} 
\begin{equation}
Pen = \gamma (\frac{ch}{m})^\theta 
\end{equation}
\begin{equation}
F_{mean} = \frac{PR}{\alpha P + (1- \alpha)R}
\end{equation}
\todo{is gamma een functie? of een constante? zelfde voor alfa}
Hierbij zijn $P$ en $R$ respectievelijk de gewogen precisie en recall van de gealigneerde unigrams ($m$) tussen kandidaat- en referentiezin.

Wanneer er meerdere referenties zijn, bepaalt het maximum van de individuele scores voor elke referentie de score van de vertaling.

\subsection{Gebruik en nadelen}
Zoals al aangehaald toonden Elliott en Keller in 2014 al aan dat van de bestudeerde evaluatiemethodes METEOR de hoogste correlatie heeft met menselijke beoordelingen voor afbeeldingsbeschrijvingen. Om deze reden rapporteren wij ook de resultaten met dit algoritme. In de literatuur blijven BLEU-scores echter de meest gerapporteerde resultaten.
Ondanks dat het het meest performante algoritme is, vertoont METEOR nog steeds slechts matige correlatie met menselijke beoordelingen. Verder onderzoek naar betere automatische evaluatie kan dus nuttig zijn. Daarnaast vereist het tabellen en synoniemenlijsten die niet voor elke taal beschikbaar zijn. Voor het Engels is deze informatie echter wel beschikbaar. 


\section{Extra informatie uit de gegenereerde zinnen}
Naast de automatische algoritmes die aan elk model een duidelijke score geven, bevatten de gegenereerde zinnen van bijvoorbeeld de validatieverzameling nog andere interessante statistieken. Deze statistieken hebben niet altijd een rechtstreeks verband met de kwaliteit van de zinnen. Ze geven wel informatie die nuttig kan zijn om het gebruikte model te analyseren en te verbeteren.

Een eerste vorm van informatie ligt in de verdeling van de lengtes van de zinnen. Ons systeem biedt de mogelijkheid om voor elke aanwezige lengte in de bestudeerde verzameling het aantal zinnen te bepalen. Hierdoor is het onder andere mogelijk om uitschieters vast te stellen. Ook maakt het de detectie van vermoedelijk foutieve zinsconstructies mogelijk. Het aantal zinnen van lengte twee, drie of bijvoorbeeld meer dan twintig geeft een goede indicatie van de neiging om inhoudsloze zinnen te genereren. Daarnaast berekent het systeem ook de gemiddelde lengte en vergelijkt het deze met de gemiddelde lengte van de referentiezinnen. Dit maakt duidelijk of het model een voorkeur heeft voor bijvoorbeeld korte zinnen.

De gebruikte woordenschat en bijbehorende woordfrequenties bieden een tweede bron van informatie. Het aantal unieke woorden geeft een idee van hoe gevarieerd het woordgebruik is van het model. Als dit aantal aan de lage kant is, is de kans groter dat het model veel dezelfde uitdrukkingen en bij uitbreiding zinnen genereert. Daarnaast print de evaluatie ook een rangschikking van de gebruikte woorden op basis van hun aantal voorkomens. Dit geeft dus informatie over de voorkeur voor bepaalde woorden. De vergelijking van deze voorkeur met deze van de referentiezinnen kan dan leiden tot het ontdekken van bepaalde eigenschappen van het model.

\section{Afbeelding-zin rangschikking}
Enkele oudere werken in de literatuur over automatische afbeeldingsbeschrijving gebruiken nog een andere vorm van evaluatie. Hierbij worden alle beschikbare beschrijvingen ge\"evalueerd door het geleerde model voor elke afbeelding. Op basis van deze evaluatie vormt men een rangschikking van de zinnen bij elke foto. Vervolgens vormt de positie $r$ van de eerste correcte zin de basis van de uiteindelijke score. \todo{referereer naar voorbeelden?} De eenvoudige maar veelgebruikte metriek $recall @ k$ wordt gebruikt voor evaluatie. Hierbij vormt het percentage van de afbeeldingen waarbij de correcte zin bij de eerste $k$ zinnen zit de uiteindelijke score. Ook de mediaan van de gevonden posities ($med\: r$)wordt dikwijls vermeld. Hetzelfde principe werkt ook in de omgekeerde richting. Hierbij gaat men van een zin via het model terug naar een rangschikking van afbeeldingen (image-retrieval). Deze laatste manier van evalueren toont aan hoe het gebruikte model kan worden gebruikt om gekende afbeeldingen te zoeken op basis van nieuwe queries. 
Volgens \todo{referentie naar vinyals} is de transformatie van generatie naar rangschikking echter niet gerechtvaardigd als evaluatiemethode. Naarmate afbeeldingen en daarbij ook het woordenboek complexer worden, groeit het aantal mogelijke zinnen exponentieel. Hierdoor daalt de likelihood van de voorgedefin\"ieerde zinnen, tenzij het aantal van deze zinnen ook exponentieel stijgt. Dit is geen realistische veronderstelling en maakt de evaluatie computationeel onhaalbaar. Omwille van deze redenering verdwijnt $recall @ k$ in de meer recente papers en gebruiken wij deze evaluatiemethode ook niet. Tabel \ref{table:recall} geeft een voorbeeld van de afbeelding-zin rangschikking in beide richtingen. Dit zijn de resultaten zoals weergegeven in de paper van Vinyals. Opvallend is hier dat de gevonden scores zeker niet veel beter zijn dan de referentiescores, terwijl zijn model wel veel beter scoort op de automatische evaluatiemethodes (BLEU-N). Van deze laatste methodes is bovendien de correlatie met menselijke evaluatie wel aangetoond.


\begin{table}
	\todo[inline]{Alle cites in tabel fixen!}
	\centering
	\begin{small}
		\setlength{\tabcolsep}{3pt}
		\begin{tabular}{|c|ccc|ccc|}
			\hline
			\multirow{2}{*}{Approach} & \multicolumn{3}{c|}{Image Annotation} & \multicolumn{3}{c|}{Image Search} \\
			& R@1 & R@10 & Med $r$ &  R@1 & R@10 & Med $r$ \\
			\hline
			\hline
			DeFrag~\cite{karpathy2014deep} & 16 & 55 & 8             &    10 & 45 & 13  \\
			m-RNN~\cite{baidu2014}         &  18 & 51 & 10               &  13 & 42 & 16\\
			MNLM~\cite{kiros2014}        &  \textbf{23}   & \textbf{63} & \textbf{5}        &  \textbf{17} & \textbf{57} & \textbf{8}   \\
			\hline
			NIC                            &  17 & 56  & 7               &    \textbf{17} & \textbf{57} & \textbf{7} \\
			\hline
		\end{tabular}
	\end{small}
	\caption{Recall@k and mediaan van de rang op Flickr30k van NIC.\label{tab:recall@1030k}}
	\label{table:recall}
\end{table}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "masterproef"
%%% End:
