\chapter{Evaluatie}
\label{hoofdstuk:evaluatie}
Om verschillende systemen te vergelijken, moet er een manier zijn om de gegenereerde zinnen te evalueren. Elke zin moet aan twee belangrijke voorwaarden voldoen. Enerzijds moet de inhoud van de zin overeenkomen met de inhoud van de afbeelding. Anderzijds moet de zin een grammaticaal correcte structuur hebben en voldoende vloeiend zijn.
Dit hoofdstuk behandelt enkele gebruikte methodes om de ontwikkelde systemen te evalueren. Twee van deze methodes vinden hun oorsprong in de computervertaling. Ze zijn ontwikkeld om automatisch de kwaliteit van vertalingen te meten. Het doel van deze evaluatiealgoritmes is om zo goed als mogelijk menselijke evaluatiescores te benaderen. De tweede sectie bekijkt enkele nuttige statistieken over de gegenereerde zinnen die peilen naar bepaalde eigenschappen zoals originaliteit. De laatste sectie behandelt een evaluatiemethode die de laatste jaren in onbruik is geraakt binnen dit onderzoeksdomein. Deze vorm van evaluatie kijkt naar het ophalen van afbeeldingen op basis van zinnen en omgekeerd. Hieruit volgt steeds een rangschikking, die leidt tot een numerieke score.


\section{Automatische evaluatie}
Ideaal gezien beoordelen meerdere mensen elke zin om tot een betrouwbare evaluatie van de zinskwaliteit te komen. Deze beoordeling kan bijvoorbeeld door een score te geven op de kwaliteit van elke zin. Een andere optie is om proefpersonen te laten oordelen of een automatisch gegenereerde zin beter, gelijkaardig of slechter is dan de overeenkomstige referentiezin. Het nadeel van elk van deze methodes is de nood aan meerdere proefpersonen die voor elke gebruikte methode of instelling van het model, alle duizend zinnen van de validatieverzameling of de testverzameling moet beoordelen. Het is duidelijk dat dit een kostelijke operatie is. Daarvoor bieden automatische evaluatiealgoritmes een oplossing. Het nadeel van deze methodes is dat ze niet perfect overeenkomen met de menselijke evaluatie.

De rest van deze sectie bevat een bespreking van twee algoritmes uit de computervertaling BLEU\cite{Papineni2001} en Meteor\cite{Denkowski2007a}. Deze algoritmes zijn bruikbaar omdat ze de gegenereerde zinnen beschouwen als ``vertalingen'' van de afbeeldingen. Het is aangetoond dat de twee methodes in zekere mate correleren met menselijke evaluaties van de gegenereerde zinnen. Van de gebruikte methodes heeft Meteor de hoogste correlatie\cite{Elliott2014}.

\subsection{BLEU}
Kort gesteld berekent het BLEU-algoritme scores van computervertalingen op basis van overeenkomsten met de referentiezinnen. Deze overeenkomsten bestaan uit gemeenschappelijke woorden of opeenvolgingen van gemeenschappelijke woorden. Verschillende vormen van BLEU kunnen worden gebruikt afhankelijk van het aantal gebruikte woorden in een opeenvolging. Een opeenvolging van $n$ woorden krijgt de naam \emph{n-gram}. Het redelijk eenvoudige algoritme van BLEU heeft echter wel enkele nadelen.

\subsubsection{Algoritme}
Om de n-gram BLEU-score van een zin te berekenen bepaalt het algoritme eerst de \textit{modified n-gram precision} of gewijzigde n-gram-precisie. Hierbij volgt de precisie uit gemeenschappelijke n-grams. N-gram-precisie houdt bovendien rekening met het aantal keer dat elk woord in de referentiezinnen voorkomt. Op deze manier krijgen zinnen als \texttt{the the the the the} een lage score omdat \texttt{the} nooit vijfmaal voorkomt in een referentiezin. 

De berekening van de gewijzigde n-gram-precisie neemt eerst het maximum van het aantal keer dat een specifieke n-gram voorkomt in elke referentiezin. Vervolgens telt het algoritme het aantal voorkomens ($Count(ngram)$) van deze sequentie in de gegenereerde zin $s$. Het minimum van deze twee getallen ($Count_ {clip}$) wordt dan voor elk woord in de vertaalde zin opgeteld en gedeeld door de som van het totaal aantal n-grams in de gegenereerde zin. Deze score is afhankelijk van de waarde van $n$.

\begin{equation}
p_{modified}(s) =
\frac{\sum\limits_{ngram \in s} Count_{clip}(ngram)}{\sum\limits_{ngram' \in s} Count(ngram')}
\label{formule:ngramprecision}
\end{equation}
Vanuit deze formule volgt een score voor een volledige corpus van gegenereerde zinnen als volgt:
\begin{equation}
p_{n} =
\frac{\sum\limits_{C \in \{Candidates\} } \sum\limits_{ngram \in C} Count_{clip}(ngram)}{\sum\limits_{C' \in \{Candidates\} } \sum\limits_{ngram' \in C'} Count(ngram')}
\label{formule:corpus_modified}
\end{equation}
Hierin is $Candidates$ de verzameling van alle gegenereerde zinnen.

Vanuit deze scores voor $n=1$ tot en met $N$ is het mogelijk om een N-gram BLEU-score te bepalen. Dit gebeurt door het gemiddelde logaritme te nemen met uniforme gewichten $w_n$, wat overeenkomt met het geometrisch gemiddelde van de gewijzigde n-gram-precisies.
\begin{equation}
BLEU = exp(\sum\limits_{n=1}^N w_nlog(p_n))
\end{equation}
Deze score dwingt echter niet de juiste lengte van de zin af. Om die reden bepaalt Papineni een extra multiplicatieve factor, namelijk de \texttt{Brevity Penalty} ($BP$). Voor elke gegenereerde zin bepaalt het algoritme de referentiezin met de dichtstbijzijnde lengte. De lengte daarvan noemt de paper de ``beste match lengte''. Vervolgens telt hij zowel de lengtes van de gegenereerde zinnen als de beste match lengte op tot respectievelijk $c$ en $r$. De onderstaande formule berekent de Brevity Penalty.
\begin{equation}BP=
 \begin{cases}
1 & if c > r \\
e^{1-r/c} & else
\end{cases}
\end{equation}

De uiteindelijke BLEU-N score is dan gelijk aan:
\begin{equation}
BLEU = BP\cdot exp(\sum\limits_{n=1}^N w_nlog(p_n))
\end{equation}
Hierbij is $w_n$ gelijk aan $1/N$ wanneer het uniforme geometrisch gemiddelde wordt genomen.

\subsubsection{Nadelen en gebruik}
De BLEU-score is de meest gebruikte gebruikte evaluatiemethode in de literatuur. De meeste papers die afbeeldingsbeschrijvingen genereren, bespreken BLEU-1 tot BLEU-4 scores. De literatuur lijkt het echter niet eens over het gebruik van de Brevity Penalty. Sommige papers vermelden expliciet dat ze het niet gebruiken, maar andere volgen de paper van Papinemi volledig. Door dit verschil in evaluatie is de vergelijking van verschillende systemen niet altijd eerlijk. Wanneer de gegenereerde zinnen lang genoeg zijn komen de scores wel overeen.
In onze experimenten stellen we de Brevity Penalty steeds gelijk aan 1.

Naast deze problemen bestaan er verschillende implementaties met kleine onderlinge verschillen. De meeste van deze verschillen vinden in hun oorsprong in het al dan niet toevoegen van normalisatie. Hierdoor zijn de concrete scores dus afhankelijk van de gebruikte implementatie. In onze experimenten gebruiken we de \texttt{multi-BLEU.pl}\footnote{\url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}} code uit de Moses decoder\cite{Koehn2006}, omdat Karpathy deze ook voorziet in zijn code.

Elliot et al.\cite{Elliott2014} tonen aan dat BLEU-3 en BLEU-4 slechts een matige correlatie heeft met menselijke beoordeling. De correlatie van BLEU-1 en BLEU-2 is nog minder.
Hiervoor zijn meerdere verklaringen mogelijk. Zo kijkt BLEU enkel naar exacte woordovereenkomsten, maar houdt het geen rekening met semantisch gelijkaardige woorden. Het gebruik van een synoniem in plaats van een overeenkomstig woord bijvoorbeeld geeft geen hogere score, terwijl dit bij menselijke evaluaties wel zo is. Daarnaast krijgen woorden die weinig semantische waarde bevatten een evengrote score als woorden die veel meer informatie over de afbeelding geven. Concreet geeft dus een woord als \texttt{a} evenveel extra score als een woord als \texttt{ski}. Ook werkwoorden die semantische informatie bevatten die in de referentiezin als substantief voorkomen geven problemen. Hieronder volgen voorbeelden van situaties waarbij zinnen slecht scoren volgens BLEU, maar inhoudelijk toch hetzelfde weergeven.
\\

Referentie 1: \texttt{Two boys are on their bikes.}

Kandidaat 1: \texttt{Two boys are on their bicycles.}

Referentie 2: \texttt{A man is skiing down a hill.}

Kandidaat 2: \texttt{A man is going down a hill on his skis}
\\

Meteor is een evaluatiealgoritme met als doel het vermijden van dergelijke foute evaluaties.

\subsection{Meteor}
Elliott et al.\cite{Elliott2014} tonen aan dat zeker unigram BLEU (BLEU-1) slechts een zwakke correlatie heeft met menselijke evaluatie. Hogere orde BLEU-scores hebben slechts een matige correlatie. Meteor is een metriek die specifiek ontworpen is om de tekortkomingen van BLEU te verbeteren. Meteor correleert het beste met menselijke evaluatie van de door Elliott onderzochte evaluatiemechanismen.

\subsubsection{Algoritme}
Meteor\cite{Denkowski2007a} evalueert vertaalde zinnen door ze te aligneren met de referenties en van deze alignering per zin een score te berekenen. Bij het berekenen van de score wordt net zoals bij BLEU gekeken naar de precisie. Daarnaast heeft in contrast met BLEU ook de recall een invloed.\todo{wat met recall?} De paper bevat een implementatie in de vorm van een JAR-bestand, zodat er geen twijfel bestaat over het gebruik van het algoritme.

Concreet probeert het algoritme twee zinnen te aligneren met behulp van vier ``matchers''. Als eerste is er een match wanneer twee woordvormen exact dezelfde zijn. Als tweede is er een match wanneer met een Snowball Stemmer\cite{porter2001snowball} ``gestemde'' woorden gelijk zijn. Een derde matcher kijkt naar overeenkomsten in de WordNet synoniemenlijst van elk woord\cite{Miller1990}. Als laatste vormen frases of woordsequenties een match wanneer ze in zogenoemde parafrasetabellen voorkomen. Een parafrasetabel bevat paren van frases en overeenkomstige parafrases. Parafrases zijn woordsequenties die dezelfde betekenis hebben dan de overeenkomstige frase, maar anders geformuleerd zijn.
Elke matcher heeft een bepaald gewicht. De experimenten gebruiken de standaardgewichten van Meteor 0.85, 0.2 ,0.6 en 0.75.

Uiteindelijk worden alle matches gegeneraliseerd tot frase matches met een bepaalde startpositie en eindlengte. Een van de doelen van de Meteor score is om zoveel mogelijk woorden af te dekken in de twee zinnen. Daarnaast moet het aantal \textit{chunks} minimaal zijn. Denkowski et al. defini\"eren een chunk als aaneengesloten en identiek geordende matches tussen de twee zinnen. De uiteindelijke Meteor score bestaat uit de F-score $F_{mean}$ vermenigvuldigd met een penalisatiefactor $Pen$ op basis van het aantal chunks.


\begin{equation}
Score = (1 - Pen)*F_{mean}
\end{equation} 
\begin{equation}
Pen = \gamma (\frac{ch}{m})^\beta 
\end{equation}
\begin{equation}
F_{mean} = \frac{PR}{\alpha P + (1- \alpha)R}
\end{equation}
Hierbij zijn $P$ en $R$ respectievelijk de gewogen precisie en recall van de gealigneerde unigrams ($m$) tussen kandidaat- en referentiezin. $m$ is het gemiddeld aantal gematchte woorden. $ch$ is het aantal chunks. $\alpha$,$\beta$ en $\gamma$ zijn vooraf getrainde parameters.

Wanneer er meerdere referenties zijn in het corpus, bepaalt het maximum van de individuele scores van elke referentie de score van de vertaling.

\subsubsection{Gebruik en nadelen}
Zoals al aangehaald toonden Elliott en Keller in 2014 aan dat van de bestudeerde evaluatiemethodes Meteor de hoogste correlatie heeft met menselijke beoordelingen voor afbeeldingsbeschrijvingen. Om deze reden rapporteren wij ook de resultaten met dit algoritme. In de literatuur blijven BLEU-scores echter de meest gerapporteerde resultaten.

Ondanks dat het het meest performante algoritme is, vertoont Meteor nog steeds slechts matige correlatie met menselijke beoordelingen. Verder onderzoek naar betere automatische evaluatie kan dus nuttig zijn. Daarnaast vereist het tabellen en synoniemenlijsten die niet voor elke taal beschikbaar zijn. Voor het Engels is deze informatie wel beschikbaar. 


\section{Extra informatie uit de gegenereerde zinnen}
Naast de automatische algoritmes die aan elk model een duidelijke score geven, bevatten de gegenereerde zinnen nog andere interessante statistieken. Deze statistieken hebben niet altijd een rechtstreeks verband met de kwaliteit van de zinnen. Ze geven wel informatie die nuttig kan zijn om het gebruikte model te analyseren en te verbeteren.

Een eerste vorm van informatie ligt in de verdeling van de lengtes van de zinnen. Ons systeem biedt de mogelijkheid om voor elke aanwezige lengte in de bestudeerde verzameling het aantal zinnen te bepalen. Hierdoor is het onder andere mogelijk om uitschieters vast te stellen. Ook maakt het de detectie van vermoedelijk foutieve zinsconstructies mogelijk. Het aantal zinnen van lengte twee, drie of bijvoorbeeld meer dan twintig geeft een goede indicatie van de neiging om inhoudsloze zinnen te genereren. Daarnaast berekent het systeem ook de gemiddelde lengte en vergelijkt het deze met de gemiddelde lengte van de referentiezinnen. Dit maakt duidelijk of het model een voorkeur heeft voor bijvoorbeeld korte zinnen.

De gebruikte woordenschat en bijbehorende woordfrequenties bieden een tweede bron van informatie. Het aantal unieke woorden geeft een idee van hoe gevarieerd het woordgebruik is van het model. Als dit aantal aan de lage kant is, is de kans groter dat het model veel dezelfde uitdrukkingen en bij uitbreiding zinnen genereert. Daarnaast zal het niet in staat zijn om uitzonderlijke foto's correct te beschrijven. Het aantal voorkomens van de gebruikte woorden geeft informatie over de voorkeur voor bepaalde woorden. De vergelijking van deze voorkeur met deze van de referentiezinnen leidt tot de ontdekking van bepaalde eigenschappen van het model.

Een derde optie kijkt hoeveel unieke zinnen het systeem genereert. De implementatie biedt de mogelijkheid om de gegenereerde zinnen voor de testverzameling te vergelijken met die in de trainingsverzameling. Dit geeft een beeld van de mate waarin het model nieuwe zinnen genereert of gekende zinnen teruggeeft. Daarnaast is er de mogelijkheid om het aantal volledig unieke zinnen te berekenen. Dit zijn zinnen die niet in de trainingsverzameling voorkomen of al gegenereerd zijn. Zo is het mogelijk om een beeld te vormen van hoe creatief het systeem is in het genereren van zinnen. 

\section{Afbeelding-zin rangschikking}
Enkele oudere werken in de literatuur over automatische afbeeldingsbeschrijving gebruiken nog een andere vorm van evaluatie. Hodosh introduceerde deze vorm van evolueren voor dit type van problemen\cite{Hodosh2013}. Hij defini\"eert twee types van evaluatie op basis van het opzoeken van een gezochte zin of afbeelding. Enerzijds met als startpunt een afbeelding, waarbij beschrijvende zinnen worden gezocht (sentence retrieval). Anderzijds zoekt hij afbeeldingen op basis van een zin (image retrieval). Voor afbeeldingen moet een systeem een rangschikking van de zinnen bij elke foto produceren. Vervolgens vormt de positie $r$ van de eerste correcte zin de basis van de score. De eenvoudige maar veelgebruikte metriek $recall @ k$ wordt gebruikt voor evaluatie. Hierbij vormt het percentage van de afbeeldingen waarbij de correcte zin bij de eerste $k$ zinnen zit de uiteindelijke score. Ook de mediaan van de gevonden posities ($med\: r$)wordt dikwijls vermeld. Hetzelfde principe werkt ook in de omgekeerde richting. Hierbij gaat het model op basis van een zin naar een rangschikking van de afbeeldingen. Deze laatste manier van evalueren toont aan hoe het gebruikte model kan worden gebruikt om afbeeldingen te zoeken op basis van nieuwe queries. 

Volgens Vinyals et al.\cite{Google} is de transformatie van generatie naar rangschikking echter geen gerechtvaardigde evaluatiemethode. Naarmate afbeeldingen en daarbij ook het woordenboek complexer worden, groeit het aantal mogelijke zinnen exponentieel. Hierdoor daalt de waarschijnlijkheid van de voorgedefin\"ieerde zinnen, tenzij het aantal van deze zinnen ook exponentieel stijgt. Dit is geen realistische veronderstelling en maakt de evaluatie computationeel onhaalbaar. Omwille van deze redenering verdwijnt $recall @ k$ in de meer recente papers en gebruiken wij deze evaluatiemethode ook niet. Tabel \ref{table:recall} geeft een voorbeeld van de afbeelding-zin rangschikking in beide richtingen. Dit zijn de resultaten zoals weergegeven in de paper van Vinyals. Opvallend is hier dat de gevonden scores zeker niet veel beter zijn dan de referentiescores, terwijl het model wel veel beter scoort op de automatische evaluatiemethodes (BLEU-N). Van deze laatste methodes is bovendien de correlatie met menselijke evaluatie wel aangetoond.


\begin{table}
	\centering
	\begin{small}
		\setlength{\tabcolsep}{3pt}
		\begin{tabular}{|c|ccc|ccc|}
			\hline
			\multirow{2}{*}{Approach} & \multicolumn{3}{c|}{Image Annotation} & \multicolumn{3}{c|}{Image Search} \\
			& R@1 & R@10 & Med $r$ &  R@1 & R@10 & Med $r$ \\
			\hline
			\hline
			DeFrag~\cite{Karpathy2014} & 16 & 55 & 8             &    10 & 45 & 13  \\
			m-RNN~\cite{Mao2014}         &  18 & 51 & 10               &  13 & 42 & 16\\
			MNLM~\cite{Kiros2014}        &  \textbf{23}   & \textbf{63} & \textbf{5}        &  \textbf{17} & \textbf{57} & \textbf{8}   \\
			\hline
			NIC~\cite{Google}                           &  17 & 56  & 7               &    \textbf{17} & \textbf{57} & \textbf{7} \\
			\hline
		\end{tabular}
	\end{small}
	\caption{Recall@k and mediaan van de rang op Flickr30k uit de NIC paper\cite{google}.\label{tab:recall@1030k}}
	\label{table:recall}
\end{table}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "masterproef"
%%% End:
