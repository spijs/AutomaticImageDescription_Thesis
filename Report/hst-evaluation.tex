\chapter{Evaluatie}
\label{hoofdstuk:evaluatie}
Om verschillende systemen te vergelijken moet er een manier zijn om de gegenereerde zinnen te evalueren. Elke zin moet aan twee belangrijke voorwaarden voldoen: enerzijds moet de inhoud van de zin overeenkomen met de inhoud van de afbeelding, anderzijds moet de zin een grammaticaal correcte structuur hebben en voldoende vloeiend zijn.

Dit hoofdstuk behandelt enkele gebruikte methodes om de ontwikkelde systemen te evalueren. Een eerste sectie behandelt BLEU en Meteor, twee methodes die hun oorsprong vinden in de computervertaling. Ze zijn ontwikkeld om automatisch de kwaliteit van vertalingen te meten. Het doel van deze evaluatiealgoritmes is om zo goed als mogelijk menselijke evaluatiescores te benaderen. De tweede sectie bekijkt enkele nuttige statistieken over de gegenereerde zinnen die peilen naar bepaalde eigenschappen zoals originaliteit. De laatste sectie behandelt een evaluatiemethode die de laatste jaren in onbruik is geraakt binnen dit onderzoeksdomein. Deze vorm van evaluatie kijkt naar het ophalen van afbeeldingen op basis van zinnen en omgekeerd. Hieruit volgt steeds een rangschikking, die leidt tot een numerieke score.


\section{Automatische evaluatie}
Ideaal gezien beoordelen meerdere mensen elke zin om tot een betrouwbare evaluatie van de kwaliteit van de zinnen te komen. Deze beoordeling kan bijvoorbeeld door een score te geven op de kwaliteit van elke zin. Een andere optie is om proefpersonen te laten oordelen of een automatisch gegenereerde zin beter, gelijkaardig of slechter is dan de overeenkomstige referentiezin. Het nadeel van elk van deze methodes is de nood aan meerdere proefpersonen die voor elke gebruikte methode of instelling van het model, alle duizend zinnen van de validatieverzameling of de testverzameling moeten beoordelen. Het is duidelijk dat dit een kostelijke operatie is. Automatische evaluatiealgoritmes bieden hiervoor een oplossing. Het nadeel van deze methodes is dat ze niet perfect overeenkomen met de menselijke evaluatie.

De rest van deze sectie bevat een bespreking van twee algoritmes uit de computervertaling: BLEU~\cite{Papineni2001} en Meteor~\cite{Denkowski2007a}. Deze algoritmes zijn bruikbaar voor evaluatie van afbeeldingsbeschrijving omdat ze de gegenereerde zinnen beschouwen als vertalingen van de afbeeldingen. Het is aangetoond dat de twee methodes in zekere mate correleren met menselijke evaluaties van de gegenereerde zinnen. Van de gebruikte methodes heeft Meteor de hoogste correlatie~\cite{Elliott2014}.

\subsection{BLEU}
Kort gesteld berekent het BLEU-algoritme scores van computervertalingen op basis van overeenkomsten met de referentiezinnen. Deze overeenkomsten bestaan uit gemeenschappelijke woorden of gemeenschappelijke opeenvolgingen van woorden. Verschillende vormen van BLEU kunnen worden gebruikt afhankelijk van het aantal gebruikte woorden in een opeenvolging. Een opeenvolging van $n$ woorden krijgt de naam \emph{$n$-gram}. Het redelijk eenvoudige algoritme van BLEU heeft wel enkele nadelen.

\subsubsection{Algoritme}
Om de $n$-gram BLEU-score van een zin te berekenen bepaalt het algoritme eerst de \textit{modified $n$-gram precision} of gewijzigde $n$-gram-precisie. Hierbij volgt de precisie uit het aantal gemeenschappelijke $n$-grams. $N$-gram-precisie houdt bovendien rekening met het aantal keer dat elk $n$-gram in de referentiezinnen voorkomt. Op deze manier hebben zinnen als \texttt{the the the the the} een lage \emph{modified unigram precision} omdat het unigram \texttt{the} nooit vijfmaal voorkomt in een referentiezin. 

De berekening van de gewijzigde $n$-gram-precisie neemt eerst het maximum van het aantal keer dat een specifiek $n$-gram voorkomt in elke referentiezin. Vervolgens telt het algoritme het aantal keer dat deze sequentie voorkomt in de gegenereerde zin $s$ ($Count(ngram)$). Het minimum van deze twee getallen ($Count_ {clip}$) wordt dan voor elk $n$-gram in de vertaalde zin opgeteld en gedeeld door de som van het totaal aantal $n$-grams in de gegenereerde zin. Deze score is afhankelijk van de waarde van $n$.

\begin{equation}
p_{modified}(s) =
\frac{\sum\limits_{ngram \in s} Count_{clip}(ngram)}{\sum\limits_{ngram' \in s} Count(ngram')}
\label{formule:ngramprecision}
\end{equation}
Vanuit deze formule volgt een score voor een volledig corpus van gegenereerde zinnen als volgt:
\begin{equation}
p_{n} =
\frac{\sum\limits_{C \in \{Candidates\} } \sum\limits_{ngram \in C} Count_{clip}(ngram)}{\sum\limits_{C' \in \{Candidates\} } \sum\limits_{ngram' \in C'} Count(ngram')}
\label{formule:corpus_modified}
\end{equation}
Hierin is $Candidates$ de verzameling van alle gegenereerde zinnen.

Op basis van deze scores voor $n=1$ tot en met $N$ is het mogelijk om een $N$-gram BLEU-score te bepalen. Dit gebeurt door het gemiddelde logaritme te nemen met uniforme gewichten $w_n$, wat overeenkomt met het geometrisch gemiddelde van de gewijzigde $n$-gram-precisies.
\begin{equation}
BLEU = exp(\sum\limits_{n=1}^N w_nlog(p_n))
\end{equation}
Deze score dwingt echter niet de juiste lengte van de zin af. Daarom bepaalt Papineni~\cite{Papineni2001} een extra multiplicatieve factor, namelijk de \emph{Brevity Penalty} ($BP$). Voor elke gegenereerde zin bepaalt het algoritme de referentiezin met de dichtstbijzijnde lengte. De lengte daarvan noemt de paper de \emph{beste match lengte}. Vervolgens telt het zowel de lengtes van de gegenereerde zinnen als de beste match lengte op tot respectievelijk $c$ en $q$. Formule~\eqref{BP} berekent de Brevity Penalty.
\begin{equation}BP=
 \begin{cases}
1 & \textrm{if}\quad c > q \\
e^{1-q/c} & \textrm{else}
\end{cases}
\label{BP}
\end{equation}

De uiteindelijke BLEU-N score is dan gelijk aan:
\begin{equation}
\text{BLEU-N} = BP\cdot exp(\sum\limits_{n=1}^N w_nlog(p_n))
\end{equation}
Hierbij is $w_n$ gelijk aan $\frac{1}{N}$ wanneer het uniforme geometrisch gemiddelde wordt genomen.

\subsubsection{Nadelen en gebruik}
De BLEU-score is de meest gebruikte evaluatiemethode in de literatuur. De meeste papers die afbeeldingsbeschrijvingen genereren, bespreken BLEU-1 tot BLEU-4 scores. De literatuur lijkt het echter niet eens over het gebruik van de Brevity Penalty. Sommige papers vermelden expliciet dat ze deze niet gebruiken, maar andere volgen de paper van Papineni~\cite{Papineni2001} volledig. Door dit verschil in evaluatie is de vergelijking van verschillende systemen niet altijd eerlijk. Wanneer de gegenereerde zinnen lang genoeg zijn komen de scores wel overeen.
In onze experimenten stellen we de Brevity Penalty steeds gelijk aan 1. Het gebruik van lengtenormalisatie bij beam search leidt ook tot een Brevity Penalty van 1, waardoor deze factor geen invloed heeft.

Naast deze problemen bestaan er verschillende implementaties met kleine onderlinge verschillen. De meeste van deze verschillen vinden hun oorsprong in het al dan niet toevoegen van normalisatie. Hierdoor zijn de concrete scores dus afhankelijk van de hoeveelheid gebruikte implementatie. In onze experimenten gebruiken we de \texttt{multi-BLEU.pl}\footnote{\url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}} code uit de Moses decoder~\cite{Koehn2006}, omdat Karpathy deze ook voorziet in zijn code.

Elliot et al.~\cite{Elliott2014} tonen aan dat BLEU-3 en BLEU-4 scores slechts een matige correlatie hebben met menselijke beoordeling. De correlatie van BLEU-1 en BLEU-2 is nog minder.
Hiervoor zijn meerdere verklaringen mogelijk. Zo kijkt BLEU enkel naar exacte woordovereenkomsten, maar houdt het geen rekening met semantisch gelijkaardige woorden. Het gebruik van een synoniem in plaats van het exacte woord geeft een lagere BLEU-score, terwijl dit bij menselijke evaluatie niet tot een slechtere score leidt. Daarnaast is de score onafhankelijk van de semantische informatie die een woord bevat. Concreet zorgt een woord als \texttt{a} voor een even grote toename van de score als een woord als \texttt{ski}. Ook werkwoorden die in de referentiezin als substantief voorkomen en semantische informatie bevatten, geven problemen. Hieronder volgen voorbeelden van situaties waarbij zinnen een slechte BLEU-score krijgen terwijl ze inhoudelijk toch hetzelfde weergeven.
\\

Referentie 1: \texttt{Two boys are on their bikes.}

Kandidaat 1: \texttt{Two boys are on their bicycles.}

Referentie 2: \texttt{A man is skiing down a hill.}

Kandidaat 2: \texttt{A man is going down a hill on his skis}
\\

\subsection{Meteor}
Meteor is een metriek die specifiek ontworpen is om de tekortkomingen van BLEU te verbeteren. Van de door Elliott onderzochte evaluatiemechanismen correleert Meteor het beste met menselijke evaluatie~\cite{Elliott2014}.

\subsubsection{Algoritme}
Meteor~\cite{Denkowski2007a} evalueert vertaalde zinnen door ze te aligneren met de referenties en van deze alignering per zin een score te berekenen. Bij het berekenen van de score wordt net zoals bij BLEU gekeken naar de precisie. Daarnaast heeft ook recall een invloed, in tegenstelling tot bij de BLEU score. De auteurs voorzien een implementatie in de vorm van een JAR-bestand, zodat er geen twijfel bestaat over het gebruik van een correct ge\"implementeerd algoritme.

Concreet probeert het algoritme twee zinnen te aligneren met behulp van vier \emph{matchers}. Als eerste is er een match wanneer twee woordvormen exact hetzelfde zijn. Als tweede is er een match wanneer woorden na \emph{stemming} met een Snowball Stemmer~\cite{porter2001snowball} gelijk zijn. Een derde matcher kijkt naar overeenkomsten in de WordNet synoniemenlijst van elk woord~\cite{Miller1990}. Als laatste vormen frases of woordsequenties een match wanneer ze in zogenoemde parafrasetabellen voorkomen. Een parafrasetabel bevat paren van frases en overeenkomstige parafrases. Parafrases zijn woordsequenties die dezelfde betekenis hebben als de overeenkomstige frase, maar anders geformuleerd zijn.
Elke matcher heeft een bepaald gewicht. De experimenten gebruiken de standaardgewichten van Meteor: 0,85, 0,2 , 0,6 en 0,75.

Uiteindelijk generaliseert het algoritme alle matches tot frase matches met een bepaalde begin- en eindpositie. E\'en van de doelen van de Meteor score is om zoveel mogelijk woorden af te dekken in de twee zinnen. Daarbij moet het aantal \textit{chunks} minimaal zijn. Denkowski et al. defini\"eren een chunk als aaneengesloten en identiek geordende matches tussen de twee zinnen~\cite{Denkowski2007a}. De uiteindelijke Meteor-score bestaat uit de F-score $F_{mean}$ vermenigvuldigd met een penalisatiefactor $Pen$ op basis van het aantal chunks.


\begin{equation}
Score = (1 - Pen)*F_{mean}
\end{equation} 
\begin{equation}
Pen = \gamma (\frac{ch}{m})^\lambda 
\end{equation}
\begin{equation}
F_{mean} = \frac{PR}{\kappa P + (1- \kappa)R}
\end{equation}
Hierbij zijn $P$ en $R$ respectievelijk de gewogen precisie en recall van de gealigneerde unigrams tussen kandidaat- en referentiezin. $m$ is het gemiddeld aantal gematchte woorden. $ch$ is het aantal chunks. $\kappa$, $\lambda$ en $\gamma$ zijn vooraf getrainde parameters.

Wanneer er meerdere referenties zijn in het corpus bepaalt het maximum van de individuele scores van elke referentie de score van de vertaling.

\subsubsection{Gebruik en nadelen}
Elliott en Keller toonden in 2014 aan dat van de bestudeerde evaluatiemethodes Meteor de hoogste correlatie heeft met menselijke beoordelingen voor afbeeldingsbeschrijvingen. Om deze reden rapporteren wij ook de resultaten met dit algoritme. In de literatuur blijven BLEU-scores echter de meest gerapporteerde resultaten.

Hoewel Meteor het meest performante algoritme is, vertoont het nog steeds slechts matige correlatie met menselijke beoordelingen. Verder onderzoek naar betere automatische evaluatie lijkt dus nuttig. Daarnaast vereist het tabellen en synoniemenlijsten die niet voor elke taal beschikbaar zijn. Voor het Engels is deze informatie wel beschikbaar. 


\section{Extra informatie uit de gegenereerde zinnen}
Naast de automatische algoritmes die aan elk model een duidelijke score geven, kunnen de gegenereerde zinnen nog tot extra statistieken leiden. Deze statistieken hebben niet altijd een rechtstreeks verband met de kwaliteit van de zinnen, maar geven wel informatie die nuttig kan zijn om het gebruikte model te analyseren en te verbeteren.

Een eerste vorm van informatie ligt in de verdeling van de lengtes van de zinnen. Ons systeem biedt de mogelijkheid om voor elke aanwezige lengte in de bestudeerde verzameling het aantal zinnen te bepalen. Hierdoor is het onder andere mogelijk om uitschieters vast te stellen. Ook maakt het de detectie van vermoedelijk foutieve zinsconstructies mogelijk. Het aantal zinnen van lengte twee, drie of bijvoorbeeld meer dan twintig geeft een goede indicatie van de neiging om inhoudsloze zinnen te genereren. Daarnaast berekent het systeem ook de gemiddelde lengte en vergelijkt het deze met de gemiddelde lengte van de referentiezinnen. Dit maakt duidelijk of het model een voorkeur heeft voor bijvoorbeeld korte zinnen.

De gebruikte woordenschat en bijhorende woordfrequenties bieden een tweede bron van informatie. Het aantal unieke woorden geeft een idee van hoe gevarieerd het woordgebruik van het model is. Als er weinig unieke woorden zijn, is de kans groter dat het model veel dezelfde uitdrukkingen en bij uitbreiding zinnen genereert. Daarnaast zal het niet in staat zijn om uitzonderlijke foto's correct te beschrijven. De frequentie van de gebruikte woorden geeft informatie over de voorkeur voor bepaalde woorden. De vergelijking van deze voorkeur met deze van de referentiezinnen leidt tot de ontdekking van bepaalde eigenschappen van het model.

Een derde optie kijkt hoeveel unieke zinnen het systeem genereert. De implementatie biedt de mogelijkheid om de gegenereerde zinnen voor de testverzameling te vergelijken met die in de trainingsverzameling. Dit geeft een beeld van de mate waarin het model nieuwe zinnen genereert of gekende zinnen teruggeeft. Daarnaast is er de mogelijkheid om het aantal volledig unieke zinnen te berekenen. Dit zijn zinnen die niet in de trainingsverzameling voorkomen en nog niet gegenereerd zijn. Zo is het mogelijk om een beeld te vormen van hoe creatief het systeem is in het genereren van zinnen. 

\section{Afbeelding-zin rangschikking}
Enkele oudere werken in de literatuur over automatische afbeeldingsbeschrijving gebruiken nog een andere vorm van evaluatie. Hodosh introduceerde deze vorm van evolueren voor dit type van problemen~\cite{Hodosh2013}. Hij defini\"eert twee types van evaluatie op basis van het opzoeken van een gezochte zin of afbeelding. Enerzijds met als startpunt een afbeelding, waarbij beschrijvende zinnen worden gezocht (sentence retrieval). Anderzijds zoekt hij afbeeldingen op basis van een zin (image retrieval). Voor afbeeldingen moet een systeem een rangschikking van de zinnen bij elke foto produceren. Vervolgens vormt de positie $r$ van de eerste correcte zin de basis van de score. De eenvoudige maar veelgebruikte metriek $recall @ k$ wordt gebruikt voor evaluatie. Hierbij vormt het percentage van de afbeeldingen waarbij de correcte zin bij de eerste $k$ zinnen zit de uiteindelijke score. Ook de mediaan van de gevonden posities ($med~r$)wordt dikwijls vermeld. Hetzelfde principe werkt ook in de omgekeerde richting. Hierbij gaat het model op basis van een zin naar een rangschikking van de afbeeldingen. Deze laatste manier van evalueren toont aan hoe het gebruikte model kan worden gebruikt om afbeeldingen te zoeken op basis van nieuwe queries. 

Volgens Vinyals et al.~\cite{Google} is de transformatie van generatie naar rangschikking echter geen gerechtvaardigde evaluatiemethode. Naarmate afbeeldingen en daarbij ook het woordenboek complexer worden, groeit het aantal mogelijke zinnen exponentieel. Hierdoor daalt de waarschijnlijkheid van de voorgedefin\"ieerde zinnen, tenzij het aantal van deze zinnen ook exponentieel stijgt. Dit is geen realistische veronderstelling en maakt de evaluatie computationeel niet haalbaar. Omwille van deze redenering verdwijnt $recall @ k$ in de meer recente papers en gebruiken wij deze evaluatiemethode ook niet. Opvallend is ook dat in de paper van Vinyals~\cite{Google} hogere scores op BLEU niet overeenkomen met hogere scores op afbeelding-zin rangschikking. Dit illustreert ook dat correlatie met menselijke evaluatie van de rangschikking niet is aangetoond, terwijl dit voor hogere BLEU-scores wel het geval is.

\section{Besluit}
Het domein van computervertaling biedt twee veelgebruikte automatische evaluatiemethodes: BLEU en Meteor. Deze algoritmes kunnen ook dienen voor het evalueren van afbeeldingsbeschrijvingen. Ze beschouwen het probleem als een vertaling van een afbeelding naar een beschrijving. Het blijkt echter dat deze methodes niet helemaal overeenkomen met menselijke perceptie. Het is ook mogelijk om op basis van de gegeneerde zinnen een aantal statistieken te berekenen die informatie geven over de originaliteit van de zinnen, de woordkeuze en de lengte van de zinnen. 

Met deze evaluatiemethodes is het mogelijk om experimenten uit te voeren die elk systeem op dezelfde manier evalueren. Dit maakt een vergelijking van de performantie mogelijk. Het volgende hoofdstuk biedt een overzicht van de verschillende experimenten die zijn uitgevoerd doorheen het onderzoek.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "masterproef"
%%% End:
