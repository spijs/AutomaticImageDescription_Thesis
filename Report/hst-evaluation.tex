\chapter{Evaluatie}
\label{hoofdstuk:evaluatie}
Dit hoofdstuk behandelt enkele gebruikte methodes om de ontwikkelde systemen te evalueren. Twee van deze methodes vinden hun oorsprong in de computervertaling. Ze zijn ontwikkeld om automatisch de kwaliteit van vertalingen te meten. Het doel van deze evaluatiealgoritmes is om zo goed als mogelijk menselijke evaluatiescores te benaderen. De laatste vorm van evaluatie kijkt naar statistieken van de gegenereerde zinnen.


\section{Automatische evaluatie}
Om verschillende systemen te kunnen vergelijken, moet er een manier zijn om de gegenereerde zinnen te evalueren. Elke zin moet aan twee belangrijke voorwaarden voldoen. Enerzijds moet de inhoud van de zin overeenkomen met de inhoud van de afbeelding. Anderzijds moet de zin een grammaticaal correcte structuur hebben en voldoende vloeiend zijn.

Ideaal gezien beoordelen meerdere mensen elke zin om tot een betrouwbare evaluatie van beide eigenschappen te komen. Deze beoordeling kan bijvoorbeeld door een score te geven op de kwaliteit van elke zin. Een andere optie is om proefpersonen te laten oordelen of een automatisch gegenereerde zin beter, gelijkaardig of slechter is dan de overeenkomstige referentiezin. Het nadeel van elk van deze methodes is de nood aan meerdere proefpersonen die voor elke gebruikte methode of instelling van de netwerken, alle duizend zinnen van de validatieverzameling of de testverzameling moet beoordelen. Het is duidelijk dat dit een kostelijke operatie is. Daarvoor bieden automatische evaluatiealgoritmes een oplossing. Het nadeel van deze methodes is dat ze niet perfect overeenkomen met de menselijke evaluatie.

In wat volgt bespreken we eerst twee algoritmes uit de computervertaling, namelijk BLEU\cite{Papineni2001} en Meteor\cite{Denkowski2007a}. Deze algoritmes zijn bruikbaar omdat de gegenereerde zinnen kunnen worden beschouwd als ``vertalingen'' van de afbeeldingen. Het is aangetoond dat de twee methodes correleren met menselijke evaluaties van gegenereerde zinnen. Van de gebruikte methodes heeft Meteor de hoogste correlatie\cite{Elliott2014}.
Geen van beide algoritmes is echter perfect voor het meten van de kwaliteit.
Om die reden bekijken we ook nog enkele statistieken van de gegenereerde zinnen om hieruit andere interessante fenomenen af te leiden.
Als laatste onderdeel volgt een bespreking van een methode die de laatste twee jaar in onbruik is geraakt binnen dit onderzoeksdomein. Deze methode is gebaseerd op een rangschikking van zinnen en afbeeldingen. Deze rangschikking leidt tot een numerieke score voor de bekomen resultaten.

\section{BLEU}
Kort gesteld berekent het BLEU-algoritme scores van computervertalingen op basis van overeenkomsten met de referentiezinnen. Deze overeenkomsten bestaan uit gemeenschappelijke woorden of opeenvolgingen van gemeenschappelijke woorden. Verschillende vormen van BLEU kunnen worden gebruikt afhankelijk van het aantal gebruikte woorden in een opeenvolging (n-grams) \todo{deze zin is vaag, ma ik weet ni wa ge bedoelt dus ik kan het ni verbeteren -> beter?}. Het gebruik van BLEU heeft echter wel enkele nadelen.

\subsection{Algoritme}
Om de n-gram BLEU-score van een zin te berekenen bepaalt het algoritme eerst de \textit{modified n-gram precision} of gewijzigde n-gram precisie. Dit is een vorm van precisie die rekening houdt met het aantal keer dat elk woord in de referentiezinnen voorkomt. Op deze manier krijgen zinnen als \texttt{the the the the the} een lage score omdat \texttt{the} nooit vijfmaal voorkomt in een referentiezin. 

De berekening van de gewijzigde n-gram precisie gebeurt door eerst het maximum te nemen van het aantal keer dat een specifieke woordsequentie voorkomt in elke referentiezin. Vervolgens telt het algoritme het aantal voorkomens ($Count(ngram)$) van deze sequentie in de gegenereerde zin $s$. Het minimum van deze twee getallen ($Count_ {clip}$) wordt dan voor elk woord in de vertaalde zin opgeteld en gedeeld door het totaal aantal woorden in de gegenereerde zin.

\begin{equation}
p_{modified}(s) =
\frac{\sum\limits_{ngram \in s} Count_{clip}(ngram)}{\sum\limits_{ngram' \in s} Count(ngram')}
\label{formule:ngramprecision}
\end{equation}
Vanuit deze formule volgt een score voor een volledige corpus van gegenereerde zinnen als volgt:
\begin{equation}
p_{n} =
\frac{\sum\limits_{C \in \{Candidates\} } \sum\limits_{ngram \in C} Count_{clip}(ngram)}{\sum\limits_{C' \in \{Candidates\} } \sum\limits_{ngram' \in C'} Count(ngram')}
\label{formule:corpus_modified}
\end{equation}
Hierin is $Candidates$ de verzameling van alle gegenereerde zinnen.

Vanuit deze scores voor $n=1$ tot en met $N$ is het mogelijk om een N-gram BLEU-score te bepalen. Dit gebeurt door het gemiddelde logaritme te nemen met uniforme gewichten $w_n$, wat overeenkomt met het geometrisch gemiddelde van de gewijzigde n-gram precisies.
\begin{equation}
BLEU = exp(\sum\limits_{n=1}^N w_nlog(p_n))
\end{equation}
Deze score dwingt echter niet de juiste lengte van de zin af. Om die reden bepaalt Papineni een extra multiplicatieve factor, namelijk de Brevity Penalty ($BP$). Voor elke gegenereerde zin bepaalt het algoritme de referentiezin met de dichtstbijzijnde lengte. De lengte daarvan noemt de paper de ``beste match lengte''. Vervolgens worden zowel de lengtes van de gegenereerde zinnen als de beste match lengte opgeteld tot respectievelijk $c$ en $r$. Berekenen van de Brevity Penalty kan dan met volgende formule:
\begin{equation}BP=
 \begin{cases}
1 & if c > r \\
e^{1-r/c} & else
\end{cases}
\end{equation}

De uiteindelijke BLEU-N score is dan gelijk aan:
\begin{equation}
BLEU = BP\cdot exp(\sum\limits_{n=1}^N w_nlog(p_n))
\end{equation}
Hierbij is $w_n$ gelijk aan $1/N$ wanneer het uniforme geometrisch gemiddelde wordt genomen.

\subsection{Nadelen en gebruik}
De BLEU-score is de meest gebruikte gebruikte evaluatiemethode in de literatuur. De meeste papers die afbeeldingsbeschrijvingen genereren bespreken BLEU-1 tot BLEU-4 scores. De literatuur lijkt het echter niet eens over het gebruik van de Brevity Penalty. Sommige papers vermelden expliciet dat ze het niet gebruiken, maar andere volgen de paper van Papinemi volledig. Door dit verschil in evaluatie is de vergelijking van verschillende systemen niet altijd eerlijk. Wanneer de gegenereerde zinnen lang genoeg zijn komen de scores wel overeen.
In onze experimenten stellen we de Brevity Penalty steeds gelijk aan 1.

Naast deze problemen zijn er verschillende implementaties met kleine onderlinge verschillen. De meeste van deze verschillen vinden in hun oorsprong in het al dan niet toevoegen van normalisatie. Hierdoor zijn de concrete scores dus afhankelijk van de gebruikte implementatie. In onze experimenten gebruiken we de \texttt{multi-BLEU.pl}\footnote{\url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}} code uit de Moses decoder\cite{Koehn2006} omdat Karpathy deze ook voorziet in zijn code.

Elliot et al.\cite{Elliott2014} tonen aan dat BLEU slechts een matige correlatie heeft met menselijke beoordeling.
Hiervoor zijn meerdere verklaringen mogelijk. Zo kijkt BLEU enkel naar exacte woordmatches maar houdt het geen rekening mee met semantisch gelijkaardige woorden. Het gebruik van een synoniem in plaats van hetzelfde woord bijvoorbeeld geeft geen hogere score, terwijl dit bij menselijke evaluaties wel zo is. Daarnaast krijgen woorden die weinig semantische waarde bevatten een evengrote score als woorden die veel meer informatie over de afbeelding geven. Concreet geeft dus een woord als \texttt{a} evenveel extra score als een woord als \texttt{ski}. Ook werkwoorden die semantische informatie bevatten die in de referentiezin als substantief voorkomen geven problemen. Hieronder volgen voorbeelden van situaties waarbij zinnen slecht scoren volgens BLEU, maar inhoudelijk toch hetzelfde weergeven.
\\

Reference 1: \texttt{Two boys are on their bikes.}

Candiate 1: \texttt{Two boys are on their bicycles.}

Reference 2: \texttt{A man is skiing down a hill.}

Candidate 2: \texttt{A man is going down a hill on his skis}
\\

Meteor is een evaluatiealgoritme met als doel het vermijden van dergelijke foute evaluaties.

\section{Meteor}
Elliott et al.\cite{Elliott2014} tonen aan dat zeker unigram BLEU (B1) slechts een zwakke correlatie heeft met menselijke evaluatie. Hogere orde BLEU-scores hebben slechts een matige correlatie. Meteor is een metriek die specifiek ontworpen is om de tekortkomingen van BLEU te verbeteren. Van de door Elliott onderzochte evaluatiemechanismen, correleert Meteor het beste met menselijke evaluatie.

\subsection{Algoritme}
Meteor\cite{Denkowski2007a} evalueert vertaalde zinnen door ze te aligneren met de referenties en hiervan per zin een score te berekenen. Bij het berekenen van de score wordt net zoals bij BLEU gekeken naar de precisie. Daarnaast heeft in contrast met BLEU ook de recall een invloed. De paper bevat een implementatie in de vorm van een JAR-bestand, zodat er geen twijfel bestaat over het gebruik van het algoritme.

Concreet probeert het algoritme twee zinnen te aligneren met behulp van vier matchers. Als eerste is er een match wanneer de woordvormen exact dezelfde zijn. Als tweede is er een match wanneer met een Snowball Stemmer\cite{porter2001snowball} ``gestemde'' woorden gelijk zijn. Een derde matcher kijkt naar overeenkomsten in de WordNet synoniemenlijst van elk woord\cite{Miller1990}. Als laatste vormen frases of woordsequenties een match wanneer ze in zogenoemde parafrasetabellen voorkomen. Een parafrasetabel bevat paren tussen frases en parafrases die ermee overeenkomen. Parafrases zijn woordsequenties die dezelfde betekenis hebben dan de overeenkomstige frase, maar op een andere manier geformuleerd zijn.
Elke matcher heeft een bepaald gewicht. De experimenten gebruiken de standaardgewichten 0.85, 0.2 ,0.6 en 0.75.

Uiteindelijk worden alle matches gegeneraliseerd tot frase matches met een bepaalde startpositie en eindlengte. Een van de doelen van de Meteor score is om zoveel mogelijk woorden af te dekken in de twee zinnen. Daarnaast moet het aantal \textit{chunks} minimaal zijn. Denkowski et al. defini\"eren een chunk als aaneengesloten en identiek geordende matches tussen de twee zinnen. De uiteindelijke Meteor score bestaat dan uit de F-score $F_{mean}$ vermenigvuldigd met een penalisatiefactor $Pen$ op basis van het aantal chunks.


\begin{equation}
Score = (1 - Pen)*F_{mean}
\end{equation} 
\begin{equation}
Pen = \gamma (\frac{ch}{m})^\beta 
\end{equation}
\begin{equation}
F_{mean} = \frac{PR}{\alpha P + (1- \alpha)R}
\end{equation}
Hierbij zijn $P$ en $R$ respectievelijk de gewogen precisie en recall van de gealigneerde unigrams ($m$) tussen kandidaat- en referentiezin. $m$ is het gemiddeld aantal gematchte woorden. $ch$ is het aantal chunks. $\alpha$,$\beta$ en $\gamma$ zijn vooraf getrainde parameters.

Wanneer er meerdere referenties zijn in het corpus, bepaalt het maximum van de individuele scores voor elke referentie de score van de vertaling.

\subsection{Gebruik en nadelen}
Zoals al aangehaald toonden Elliott en Keller in 2014 aan dat van de bestudeerde evaluatiemethodes Meteor de hoogste correlatie heeft met menselijke beoordelingen voor afbeeldingsbeschrijvingen. Om deze reden rapporteren wij ook de resultaten met dit algoritme. In de literatuur blijven BLEU-scores echter de meest gerapporteerde resultaten.

Ondanks dat het het meest performante algoritme is, vertoont Meteor nog steeds slechts matige correlatie met menselijke beoordelingen. Verder onderzoek naar betere automatische evaluatie kan dus nuttig zijn. Daarnaast vereist het tabellen en synoniemenlijsten die niet voor elke taal beschikbaar zijn. Voor het Engels is deze informatie echter wel beschikbaar. 


\section{Extra informatie uit de gegenereerde zinnen}
Naast de automatische algoritmes die aan elk model een duidelijke score geven, bevatten de gegenereerde zinnen van bijvoorbeeld de validatieverzameling nog andere interessante statistieken. Deze statistieken hebben niet altijd een rechtstreeks verband met de kwaliteit van de zinnen. Ze geven wel informatie die nuttig kan zijn om het gebruikte model te analyseren en te verbeteren.

Een eerste vorm van informatie ligt in de verdeling van de lengtes van de zinnen. Ons systeem biedt de mogelijkheid om voor elke aanwezige lengte in de bestudeerde verzameling het aantal zinnen te bepalen. Hierdoor is het onder andere mogelijk om uitschieters vast te stellen. Ook maakt het de detectie van vermoedelijk foutieve zinsconstructies mogelijk. Het aantal zinnen van lengte twee, drie of bijvoorbeeld meer dan twintig geeft een goede indicatie van de neiging om inhoudsloze zinnen te genereren. Daarnaast berekent het systeem ook de gemiddelde lengte en vergelijkt het deze met de gemiddelde lengte van de referentiezinnen. Dit maakt duidelijk of het model een voorkeur heeft voor bijvoorbeeld korte zinnen.

De gebruikte woordenschat en bijbehorende woordfrequenties bieden een tweede bron van informatie. Het aantal unieke woorden geeft een idee van hoe gevarieerd het woordgebruik is van het model. Als dit aantal aan de lage kant is, is de kans groter dat het model veel dezelfde uitdrukkingen en bij uitbreiding zinnen genereert. Daarnaast evalueren we ook het aantal voorkomens van de gebruikte woorden. Dit geeft dus informatie over de voorkeur voor bepaalde woorden. De vergelijking van deze voorkeur met deze van de referentiezinnen kan dan leiden tot het ontdekken van bepaalde eigenschappen van het model.

Een derde optie bestaat erin om te kijken hoeveel unieke zinnen het systeem genereert. Onze implementatie biedt de mogelijkheid om de gegenereerde zinnen voor de testverzameling te vergelijken met die in de trainingsverzameling. Dit geeft een beeld van in welke mate het model echt nieuwe zinnen genereert of gekende zinnen teruggeeft. Daarnaast is er de mogelijkheid om het aantal echt unieke zinnen te berekenen. Dit zijn zinnen die niet in de trainingsverzameling voorkomen of al gegenereerd zijn. Zo is het mogelijk om een beeld te vormen van hoe gevarieerd het systeem is in het genereren van zinnen. 

\section{Afbeelding-zin rangschikking}
Enkele oudere werken in de literatuur over automatische afbeeldingsbeschrijving gebruiken nog een andere vorm van evaluatie. Hodosh introduceerde deze vorm van evolueren voor dit type van problemen\cite{Hodosh2013}. Hij defini\"eert twee types van evaluatie op basis van het opzoeken van een gezochte zin of afbeelding. Enerzijds met als startpunt een afbeelding, waarbij beschrijvende zinnen worden gezocht (sentence retrieval). Anderzijds zoekt men afbeeldingen op basis van een zin (image retrieval). Voor foto's bijvoorbeeld moet een systeem dan een rangschikking van de zinnen bij elke foto produceren. Vervolgens vormt de positie $r$ van de eerste correcte zin de basis van de score. De eenvoudige maar veelgebruikte metriek $recall @ k$ wordt gebruikt voor evaluatie. Hierbij vormt het percentage van de afbeeldingen waarbij de correcte zin bij de eerste $k$ zinnen zit de uiteindelijke score. Ook de mediaan van de gevonden posities ($med\: r$)wordt dikwijls vermeld. Hetzelfde principe werkt ook in de omgekeerde richting. Hierbij gaat men van een zin via het model terug naar een rangschikking van afbeeldingen. Deze laatste manier van evalueren toont aan hoe het gebruikte model kan worden gebruikt om gekende afbeeldingen te zoeken op basis van nieuwe queries. 

Volgens Vinyals et al.\cite{Google} is de transformatie van generatie naar rangschikking echter geen gerechtvaardigde evaluatiemethode. Naarmate afbeeldingen en daarbij ook het woordenboek complexer worden, groeit het aantal mogelijke zinnen exponentieel. Hierdoor daalt de likelihood van de voorgedefin\"ieerde zinnen, tenzij het aantal van deze zinnen ook exponentieel stijgt. Dit is geen realistische veronderstelling en maakt de evaluatie computationeel onhaalbaar. Omwille van deze redenering verdwijnt $recall @ k$ in de meer recente papers en gebruiken wij deze evaluatiemethode ook niet. Tabel \ref{table:recall} geeft een voorbeeld van de afbeelding-zin rangschikking in beide richtingen. Dit zijn de resultaten zoals weergegeven in de paper van Vinyals. Opvallend is hier dat de gevonden scores zeker niet veel beter zijn dan de referentiescores, terwijl zijn model wel veel beter scoort op de automatische evaluatiemethodes (BLEU-N). Van deze laatste methodes is bovendien de correlatie met menselijke evaluatie wel aangetoond.


\begin{table}
	\centering
	\begin{small}
		\setlength{\tabcolsep}{3pt}
		\begin{tabular}{|c|ccc|ccc|}
			\hline
			\multirow{2}{*}{Approach} & \multicolumn{3}{c|}{Image Annotation} & \multicolumn{3}{c|}{Image Search} \\
			& R@1 & R@10 & Med $r$ &  R@1 & R@10 & Med $r$ \\
			\hline
			\hline
			DeFrag~\cite{Karpathy2014} & 16 & 55 & 8             &    10 & 45 & 13  \\
			m-RNN~\cite{Mao2014}         &  18 & 51 & 10               &  13 & 42 & 16\\
			MNLM~\cite{Kiros2014}        &  \textbf{23}   & \textbf{63} & \textbf{5}        &  \textbf{17} & \textbf{57} & \textbf{8}   \\
			\hline
			NIC~\cite{Google}                           &  17 & 56  & 7               &    \textbf{17} & \textbf{57} & \textbf{7} \\
			\hline
		\end{tabular}
	\end{small}
	\caption{Recall@k and mediaan van de rang op Flickr30k uit de NIC paper.\label{tab:recall@1030k}}
	\label{table:recall}
\end{table}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "masterproef"
%%% End:
