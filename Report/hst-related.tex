\chapter{Gerelateerd Werk}
\label{hoofdstuk:related}
Het automatisch genereren van beschrijvingen voor ongeziene afbeeldingen is een complex proces. Het combineert zowel computervisie (CV) als natuurlijke taalverwerking (NLP). Vele modellen zijn al voorgesteld die telkens elementen uit beide onderzoeksvelden combineren. Daarom volgt er eerst een opdeling van de gerelateerde literatuur op basis van de gebruikte technieken in deze twee domeinen. 

Het probleem van automatische afbeeldingsbeschrijving werd eerst niet beschouwd als generatieprobleem maar als opvraag- of retrievalprobleem. Hierbij zoekt een model naar de beste zin in een bestaande verzameling van zinnen op basis van de ingevoerde foto\cite{Hodosh2013}. Andere papers behandelen het probleem als een vertalingsprobleem naar analogie met automatisch vertalen. Hierbij gebruikt men een codeer-decodeersysteem waarbij de afbeelding als brontaal wordt beschouwd en het Engels als doeltaal.

Dit hoofdstuk bevat een overzicht van de gebruikte technologie\"en uit computervisie en NLP voor het genereren van beschrijvingen. Daarnaast bespreken we de verschillende manieren waarop deze technologi\"en in de literatuur ge\"integreerd worden in \'e\'en model.

\section{Representatie van afbeeldingen}
Alle huidige modellen gebruiken technieken uit computervisie om nuttige features af te leiden uit afbeeldingen. Features bevatten onder andere gedetecteerde acties, sc\`enes en objecten met hun attributen en relaties\cite{Bernardi}. Deze features vormen de basis voor een representatie van de afbeelding die als input dient voor het generatie- of retrieval-model.

Eerst volgt een bespreking van technieken die in het verleden zijn gebruikt. Daarna volgt een beschrijving van Convolutionele Neurale Netwerken (CNN), die in de meer recente literatuur veelvuldig voorkomen.


\subsection{Oorspronkelijke CV modellen}
De literatuur gebruikt meerdere technieken uit computervisie om nuttige features of eigenschappen af te leiden uit afbeeldingen. Features die in de vroegste papers over dit onderwerp voorkomen zijn bijvoorbeeld sc\`eneclassificaties, objectdetecties en attribuutclassificaties \cite{Farhadi2010,Patterson2014,Yang2011}. Bij sc\`eneclassificatie traint men modellen om voor elke afbeelding een bepaalde sc\`ene (bvb. restaurant, slaapkamer, keuken) af te leiden. Ook modellen die verschillende objecten detecteren en benoemen zijn hier van nut. Van zulke gevonden objecten kunnen classifiers ook nog nuttige attributen zoals kleur afleiden. Om zulke voorspellingen te doen, gebruiken verschillende oudere papers bestaande classifiers en detectors zoals bij Felzenswalb et al.\cite{Felzenszwalb2008} Im2Text\cite{Ordonez2011} en GIST\cite{Oliva2006}. Ee\'n of meerdere van deze features kan dan rechtstreeks de representatie vormen van een afbeelding. Daarnaast vormen deze features in enkele werken \cite{Farhadi2010,Li2011,Mitchell2012,Yang2011} enkel de input voor abstracte afbeeldingsrepresentaties in de vorm van tupels. Deze tupels bevatten dan objecten, acties tussen objecten, sc\`ene types en/of ruimtelijke relaties.

Een andere manier om afbeeldingen te representeren is het gebruik van Visual Dependency Representations (VDR) zoals voorgesteld door Elliott et al.\cite{Elliott2013}. Dit model werkt analoog aan een taalgebaseerde afhankelijkheidsgrammatica. Dit type grammatica stelt de syntactische structuur van een zin voor met woorden en binaire relaties, semantisch of syntactisch, tussen deze woorden\cite{Jurafsky:2009:SLP:1214993}. Figuur \ref{fig:dep_grammar} toont een voorbeeld van een zin geparset met een afhankelijkheidsgrammatica. De gelabelde pijlen stellen de verschillende relaties tussen de woorden voor. 

\begin{figure}[tb]
      \centering
      \includegraphics[width=\linewidth]{Images/dependencygrammar.jpg}
      \caption{Zin geparset met afhankelijkheidsgrammatica\cite{GasserNotes}}
      \label{fig:dep_grammar}
  \end{figure}  

VDR's gebruiken een afhankelijkheidsgraaf om de ruimtelijke relaties tussen de objecten in een foto voor te stellen. Elke relatie tussen twee objecten krijgt dan een ruimtelijk positie als label. Mogelijke relaties zijn bijvoorbeeld \texttt{op}, \texttt{boven}, \texttt{onder}, \texttt{naast}, ... Het leren van VDR's kan op basis van geannoteerde trainingsdata, automatisch met objectherkenning\cite{Elliott2015} of nog met andere informatie die in de abstracte sc\`ene zit\cite{Gilberto2015}.  

\subsection{CV met behulp van neurale netwerken}
Naast deze meer traditionele manieren om informatie uit afbeeldingen af te leiden, bieden neurale netwerken een alternatieve oplossing.
Voor de meeste taken binnen computervisie blijkt dat convolutionele neurale netwerken (CNN) beter presteren dan bovenstaande methodes. Deze CNN's zijn deep learning neurale netwerken met soms meer dan 15 verborgen lagen. CNN's hebben minder verbindingen en parameters dan overeenkomstige feedforward neurale netwerken terwijl ze niet veel slechter presteren\cite{Krizhevsky2012a}. 
Voor verschillende CV-taken werken de huidige state-of-the-art oplossingen op basis van CNN's. Dit is onder andere het geval voor gezichtsherkenning\cite{Zhou2015}, tekenherkenning\cite{Ciresan2012} en objectherkenning\cite{Szegedy2014}.

De gebruikte CNN's binnen het automatisch afbeeldingen beschrijven zijn netwerken die getraind zijn op ImageNet\cite{Russakovsky2014}. ImageNet is een dataset bestaande uit miljoenen afbeeldingen die gelabeld zijn binnen enkele duizenden categorie\"en. Het neuraal netwerk leert afbeeldingen correct te labelen. Vaak gebruikte CNN modellen zijn AlexNet\cite{Krizhevsky2012a} en het recentere VGGNet\cite{Arge2015}. Elke afbeelding wordt als input gegeven aan het netwerk om zo tot een vectorrepresentatie te komen. De meeste papers die afbeeldingen beschrijven gebruiken de gewichten van de voorlaatste laag van het netwerk als representatie van deze afbeelding\cite{Chen2014,Karpathy2015,Mao2014a,Google}. Aandacht-gebaseerde oplossingen\cite{Jin2015,Xu2015} gebruiken ook de output van lagere convolutionele lagen als extra informatie.

Regionale CNN's vormen een variatie op CNN die het ook mogelijk om een afbeelding op te delen in verschillende interessante regio's. Hiermee is het mogelijk om voor elke regio een representatie te maken\cite{Karpathy2015,Mitchell2015}. 

\section{Representatie van zinnen}
Het is niet eenvoudig om rechtstreeks met zinnen als een geheel te werken. Om die redenen zijn er verschillende manieren om ook een zin op een alternatieve wijze voor te stellen.
De meeste modellen vertrekken van een vectorrepresentatie voor individuele woorden. In de literatuur zijn er verschillende manieren om woordrepresentaties samen te voegen tot een representatie van een zin.

\subsection{Voorstellen van woorden}
 Een voorstelling van woorden als vector vergemakkelijkt enerzijds de verdere verwerking en kan anderzijds ook semantische informatie bevatten zoals bij bijvoorbeeld \texttt{word2vec}\cite{Mikolov2013}. 
 Een mogelijke tweede voorstelling is een one-hot codering waarin elk woord wordt voorgesteld door een vector met als grootte het aantal woorden in het vocabularium. Deze vector is volledig nul behalve op \'e\'en rij die overeenkomt met het woord. Het is mogelijk om deze representatie verder uit breiden met een gewichtsmatrix om zo ook de semantische informatie van de woorden te leren. Deze gewichtsmatrix kan willekeurig worden ge\"initialiseerd of eerst worden geleerd op bestaande corpora\cite{Lebret2013,Mao2014a,Google}. Daarna kunnen de gekende woorden en zinnen uit de dataset de gewichten nog verfijnen.  

 Een andere mogelijkheid is om bestaande word embeddings te gebruiken zoals \texttt{word2vec}. Dat algoritme mapt ook elk woord op een vector, maar heeft bovendien enkele mooie eigenschappen. Zo mapt het bijvoorbeeld semantisch gelijkaardige woorden op nabijgelegen posities in de vectorruimte\cite{Mikolov2013}. Deze voorgedefin\"ieerde vectoren hebben als nadeel dat niet voor elk woord uit de beschrijvingen een vector representatie beschikbaar is. Het is wel mogelijk om zelf deze vectoren te leren op basis van de gebruikte dataset.

 De meeste werken in de literatuur rapporteren dat one-hotcodering in combinatie met een te leren gewichtsmatrix betere resultaten oplevert. Daarnaast hebben sommige semantisch gelijkaardige woorden zoals kleuren gelijkaardige vectoren, terwijl deze in een afbeelding toch uitgesproken verschillend zijn\cite{Karpathy2015}.
 
\subsection{Voorstellen van zinnen}
 Verschillende mogelijkheden bestaan om de zinnen voor te stellen wanneer de woordvectoren gekend zijn. Een eerste mogelijkheid gebruikt een afhankelijkheidsparser en stelt de zinnen voor als een volledige afhankelijkheidsboom\cite{Socher2014}. Karpathy\cite{Karpathy2014} gebruikt ook een afhankelijkheidsparser, maar probeert hier een verzameling van triplets uit te halen. Dergelijke triplets bestaan uit twee entiteiten die verbonden zijn door een actie. 

 Een volgende mogelijkheid is om de woordvectoren op te telle\cite{Lebret2013}n. Hierdoor gaat informatie uit de woordvolgorde wel verloren. Le en Mikolov\cite{Le2014a} bieden een oplossing voor het verloren gaan van woordvolgorde. Zij trainen een model om representaties te maken van stukken tekst met variabele lengte. Hun algoritme kan dus zowel zinnen als paragrafen en volledige teksten voorstellen.

 Een vaak gebruikt taalmodel in de meer recente NLP-literatuur zijn Recurrente Neurale Netwerken (RNN)\cite{Mikolov2010}. Dit zijn neurale netwerken die goed overweg kunnen met sequenti\"ele data zoals taal. Kiros et al.\cite{Kiros2013} gebruiken  de verborgen lagen van een RNN zin samen met nog extra informatie over de zin zoals POS-tags als representatie. De meer recente modellen stellen een zin voor als de sequentie van de woordvectoren in die zin.
 
\section{Van afbeeldingsrepresentaties naar beschrijvingen}
Er zijn verschillende methodes om vanuit de representaties van de afbeelding en bijbehorende referentiezinnen een model te trainen dat in staat is om ongeziene afbeeldingen om te zetten tot correcte beschrijvingen. 
De meeste modellen trainen met als doel het verschil tussen de gegenereerde omschrijving en de trainingsafbeelding te minimaliseren.

\subsection{Dichtstbijzijnde afbeelding}
De meest eenvoudige aanpak zoekt naar de meest gelijkaardige afbeelding in de trainingsvezameling en geeft \'e\'en van zijn beschrijvingen terug als resultaat (Nearest Neighbour)\cite{Devlin2015a}. Een gelijkaardigheidsmetriek zoals de cosinusgelijkenis tussen de afbeeldingsrepresentaties evalueert de gelijkaardigheid van twee representaties.

E\'en uitbreiding op deze aanpak is het zoeken naar de verzameling van de meest gelijkaardige afbeeldingen in de trainingsverzameling. Vervolgens cre\"eert een model een rangorde op basis van extra visuele of tekstuele informatie. De referentiezin van de hoogst scorende afbeelding is dan het resultaat\cite{Devlin2015a,Hodosh2013,Ordonez2011,Oliva2006}. 
Deze modellen hebben als nadeel dat ze nooit resulteren in een zin die niet in de trainingsverzameling zit.

Een variatie\cite{Gupta2012,Kuznetsova2012} hierop gebruikt een afbeeldingsvoorstelling met gedetecteerde objecten. Vervolgens zoekt hun systeem naar de beschrijving van visueel gelijkaardige objecten in de vorm van zinsfragmenten (phrases). Met de verzamelde fragmenten wordt dan de meest waarschijnlijke nieuwe zin gegenereerd op basis van hun type. Deze types bestaan uit naamwoordsgroep(NP), werkwoordsgroep(VP) en PP(voorzetselgroep). De auteurs defin\"ieren meerdere beperkingen op de gegeneerde zinnen om het aantal mogelijkheden te verkleinen.

Naas het rechstreeks gebruiken van de beschrijvingen van de meest gelijkaardige afbeeldingen is het ook mogelijk om deze zinnen, samen met de afbeelding, als input aan een tweede model te geven. Zo beschouwen Mason et al.\cite{Mason2014} bijvoorbeeld het genereren van beschrijvingen als een samenvattingsprobleem en gebruikt hij de beschrijvingen van gelijkaardige afbeeldingen als extra input. Analoog verkrijgen Jia et al.\cite{Fernando2015} verbeteringen op bestaande modellen door het toevoegen van extra semantische informatie zoals beschrijvingen van gelijkaardige afbeeldingen op basis van Canonical Correlation Analysis (CCA).
 
\subsection{Multimodale modellen}
Enkele werken proberen een gemeenschappelijke ruimte tussen zinnen en afbeeldingen te leren zodat het mogelijk is om zowel de representatie van zinnen als afbeelding op dezelfde ruimte te projecteren. Dit laat toe om afbeeldingen en zinnen rechtstreeks te vergelijken met een afstandsmaat zoals bijvoorbeeld de cosinusgelijkenis. Dit is zeer nuttig voor onder andere het opvragen van afbeeldingen en zinnen (verder: image retrieval en sentence retrieval). Het leren van multimodale modellen kan onder andere met Canonical Correlation Analysis (CCA)\cite{Hodosh2013} en neurale netwerken\cite{Mao2014,Karpathy2014,Kiros2013}. 

\subsection{Sjabloongebaseerd}
Een volgende aanpak baseert zich op sjablonen om zinnen te genereren. Op basis van de gebruikte afbeeldingsvoorstelling vult een algoritme de overeenstemmende voorgedefinieerde sjabloon in\cite{Yang2011}. Hiervoor is het dikwijls nodig om bijkomende complexe modellen te trainen zoals bijvoorbeeld bij Elliott et al.\cite{Elliott2013}. Het nadeel van deze methode is dat de gegenereerde zinnen wel syntactisch correct zijn, maar dikwijls onnatuurlijk aanvoelen voor mensen. Om deze methode te verbeteren kunnen gegenereerde of vooraf gekende zinsfragmenten helpen bij het recombineren van fragmenten om nieuwe beschrijvingen te genereren\cite{Mitchell2012,Kuznetsova2012}. 

\subsection{Neurale netwerken}
De meest recente en best scorende modellen gebruiken neurale netwerken als taalmodel voor de generatie van beschrijvingen. Deze taalmodellen zijn in staat om compleet nieuwe en voor mensen vlotte zinnen te produceren. Vooral Recurrente Neurale Netwerken (RNN)\cite{Mikolov2010} winnen in de literatuur aan populariteit als taalmodel. RNN's zijn in staat om sequenti\"ele data te genereren op basis van een zekere input. LSTM's (Long Short Term Memory)\cite{SeppHochreiter1997} vormen een uitbreiding op de RNN's en houden informatie bij die ze gedurende een langere termijn kunnen bijhouden in een geheugencel. 

Beide modellen verwachten een sequentie van woordrepresentaties als input, maar kunnen ook uitgebreid worden met extra informatie als invoer. Er zijn een aantal verschillende mogelijkheden uitgeprobeerd in de literatuur. Ten eerste is er het cre\"eren van een multimodale ruimte tussen afbeeldingen en zinnen\cite{Kiros2014,Socher2014}. Een andere optie is het aanwenden van een ``aandachtsvector''\cite{Xu2015}. Het is ook mogelijk om een combinatie van verschillende technieken te gebruiken, zoals het leren van een ``sc\'enevector'' in combinatie met een aandachtsgebaseerd systeem\cite{Jin2015}. Jia et al. experimenteren met verschillende mogelijkheden om extra informatie toe te voegen, zoals CCA-projecties van afbeeldingen, of de afbeelding zelf\cite{Fernando2015}.

Een eerste verzameling van modellen met neurale netwerken volgen het codeer-decodeerprincipe uit de automatische vertaling.\cite{Kiros2014} De codeercomponent transformeert een afbeelding naar een nieuwe (multimodale) representatie. De decodeercomponent vertaalt vervolgens deze multimodale representatie naar een zin in natuurlijke taal. Door het multimodale karakter van deze modellen is het opvragen van afbeeldingen en zinnen ook mogelijk. Dit opvragen gebeurt door middel van projectie van een nieuwe afbeelding/zin in de multimodale ruimte. Een vergelijking van deze projectie met de zinnen of afbeeldingen uit de trainingsverzameling leidt tot een rangschikking van de trainingsvoorbeelden. Er bestaan zowel codeer-decodeer modellen met LSTM's\cite{Kiros2014} als met RNN's\cite{Karpathy2014,Mao2014a}.

Een tweede categorie gebruikt zowel de afbeeldingsrepresentatie als de sequentie van 
woordrepresenties als input bij het trainen van het netwerk. Op basis van een trainingsafbeelding probeert het netwerk het eerste woord uit de zin te voorspellen. Deze voorspelling dient dan, al dan niet samen met de afbeelding, als input voor de voorspelling van het volgende woord. Terugpropagatie van de fout op de gegenereerde woorden doorheen het netwerk zorgt voor de juiste wijzigingen aan de gewichten. Op deze manier leert het netwerk om op basis van een ongezien afbeelding de juiste sequentie van woorden te genereren. Ook hier bestaan er modellen met LSTM\cite{Donahue2015,Google} en RNN\cite{Karpathy2015}. \todo{meer papers toevoegen}

Het trainen van deze netwerken gebeurt met terugpropagatie doorheen het netwerk. Het is mogelijk om de fouten ook verder door te propageren naar de gewichtsvectoren van de woordrepresentaties of naar de gewichten van een CNN. Die methode optimaliseert op die manier alle waarden op de  dataset, maar kan computationeel kostelijk zijn.
Alle gebruikte neurale netwerken hebben een ``softmax'' als laatste laag. Deze laag zorgt ervoor dat het netwerk een kansverdeling genereert voor het volgende woord. Het meest waarschijnlijke woord, heeft dan de hoogste kans.
Het selecteren van het woord voor de generatie kan gebeuren door het samplen van deze verdeling, of door het gebruik van beam search, om zo de meest waarschijnlijke beschrijving te genereren.  Zowel het begin als het einde van de zin wordt gekenmerkt met een specifiek stopwoord.

\subsection{Statistische taalmodellen}
Naast de neurale netwerken behoren ook de statistische taalmodellen tot de beter scorende modellen. Deze modellen proberen op basis van entropie een taal zo goed mogelijk te beschrijven. Entropie is een maat voor informatie en dient verschillende doeleinden binnen het domein van NLP. 

Concreet is de entropie $H$ de hoeveelheid informatie voor die in een random variabele $X$ zit. $X$ is typisch de voorspelde variabele, dit kunnen letters zijn, woorden,\ldots Het domein van deze variabele is $\chi$. De entropie van $X$ is te berekenen met formule \eqref{entropy}, waarbij $p(x)$ de kans voorstelt dat $X$ waarde $x$ heeft\cite{Jurafsky:2009:SLP:1214993}.

\begin{equation}
     H(X) = -\sum_{x \in \chi}p(x)\log_2p(x)
     \label{entropy}
 \end{equation} 

Zo leren Mitchell et al.\cite{Mitchell2015} een lijst met waarschijnlijke woorden uit de afbeeldingsrepresentatie en combineren ze deze met een uitgebreid taalmodel. Dit taalmodel is geleerd op basis van de beschrijvingen in de trainingsdata. Het leren van dit model gebeurt op basis van een statistische benadering waarbij het algoritme de entropie maximaliseert. In een volgende stap van hun systeem zoeken ze de zinnen die het meest waarschijnlijk zijn, gegeven de woorden die voorkomen in de afbeelding. Vervolgens sorteren ze de gegeneerde zinnen op basis van een aantal additionele features. Dit model is net als de modellen met neurale netwerken in staat om nieuwe en vlotte zinnen te vormen. De prestatie is gelijkaardig aan die van de neurale netwerken.

Lebret\cite{Lebret2015} toont bovendien aan dat ook met een veel eenvoudiger taalmodel toch relatief goede resultaten kunnen worden bekomen. Dit model extraheert alle zinsfragmenten (phrases) uit de trainingsdata en leert daarmee een eenvoudig 3-gram taalmodel. Voor ongeziene afbeeldingen bepaalt het model de best overeenkomende zinsfragmenten en probeert hiermee de meest waarschijnlijke zinnen te maken. In tegenstelling tot alle voorgaande modellen gebeurt training van deze multimodale transformatie met negatieve sampling. Hierbij leert het model een juiste referentiezin te onderscheiden in een lijst die ook willekeurig gekozen zinnen bevat. Ook hier gebeurt er op het einde nog een hersortering van de gegenereerde zinnen op basis van de overeenstemming met de afbeelding.

\subsection{Aandachtsgebaseerde modellen}
State-of-the-art modellen uit het automatisch vertalen gebruiken mechanismes die aandachtsgebaseerd zijn. Concreet betekent dit dat het decodeeralgoritme zelf beslist welke stukken uit de zin meer aandacht vereisen.
In de setting van het beschrijven van afbeeldingen komt dit neer op een mechanisme dat aangeeft welke regio's in de afbeelding belangrijk zijn. In de decoder zorgt dit aandachtsmechanisme voor een extra contextvector als input voor een neuraal netwerk. 

Deze aandachtsvector kan zowel ``zacht'' als ``hard'' zijn. ``Harde'' aandacht is een stochastisch principe dat de aandachtslocatie beschouwt als een latente variabele. Tijdens elke stap samplet het algoritme \'e\'en locatie om de aandacht op te vestigen tijdens het genereren van het volgende woord. ``Zachte'' aandachtsmethodes werken deterministisch, waardoor er geen nood is om elke stap een sample-operatie uit te voeren. Een differentieerbare functie bepaalt de aandachtslocatie tijdens elke stap van het trainingsproces. Door de differentieerbaarheid is het netwerk dat de aandachtsvector berekent, eenvoudig te trainen met behulp van terugpropagatie.

Aandachtsmodellen geven bovendien een mooie visuele voorstelling van waar het model bepaalde woorden ziet (figuur \ref{fig:attention-example}). Binnen het automatisch beschrijven van afbeeldingen bereiken deze aandachtsgebaseerde modellen voorlopig de beste resultaten\cite{Jin2015,Xu2015}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\linewidth]{Images/good_Xu.pdf}
	\caption[Voorbeelden van aandacht op correcte regio.]{Voorbeelden van aandacht op correcte regio. (Wit in de afbeelding is de gefocuste regio, onderlijnde tekst is het overeenkomstige woord.)\cite{Xu2015}}
	\label{fig:attention-example}
\end{figure}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "masterproef"
%%% End: 